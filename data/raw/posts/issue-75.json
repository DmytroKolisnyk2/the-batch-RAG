{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-75","id":"60bfddae274d5b003b106301","uuid":"10d2494e-063c-4e46-8ecc-f0fab6a85bc4","title":"The Batch: Detecting Guns, Fighting Lead Poisoning, Adversarial Training for Language-and-Vision, Financial Reports for Robots","html":"<p><em>Dear friends,</em></p><p><em>Experience gained in building a model to solve one problem doesn’t always transfer to building models for other problems. How can you tell whether or not intuitions honed in one project are likely to generalize to another? I’ve found that two factors can make the difference: the size of the training set and whether the data is unstructured or structured.<br><br>For instance, I’ve heard blanket statements like, “you should always have at least 1,000 examples before tackling a problem.” This is good advice if you’re working on a pedestrian detector, where data is readily available and prior art shows that large datasets are important. But it’s bad advice if you’re building a model to diagnose rare medical conditions, where waiting for 1,000 examples might mean you’ll never get started.</em></p><p><em>Unstructured data includes text, images, and audio clips, which lend themselves to interpretation by humans. Structured data, on the other hand, includes things like transaction records or clickstream logs, which humans don’t process easily.</em></p><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/Screen20Shot202021-01-2020at2010.25.5520AM20copy.png\" class=\"kg-image\" alt=\"Table with information related to data and datasets\" loading=\"lazy\"></figure><p><em>This difference leads to very different strategies for training and deploying models:</em></p><ul><li><em><strong>Unstructured data:</strong> Because the examples are easy for humans to understand, you can recruit people to label them and benchmark trained models against human-level performance (HLP). If you need more examples, you might be able to collect them by capturing more text/images/audio or by using <a href=\"https://www.coursera.org/lecture/convolutional-neural-networks/data-augmentation-AYzbX?ref=dl-staging-website.ghost.io\" rel=\"noopener\">data augmentation</a> to distort existing examples. <a href=\"https://www.coursera.org/lecture/machine-learning-projects/carrying-out-error-analysis-GwViP?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Error analysis</a> can take advantage of human intuition.</em></li><li><em><strong>Structured data:</strong> This class of data is harder for humans to interpret, and thus harder for humans to label. Algorithms that learn from structured data often surpass HLP, making that measure a poor benchmark. It can also be hard to find additional examples. For instance, if the training dataset comprises records of your customers’ purchases, it’s hard to get data from additional customers beyond your current user base.</em></li></ul><p><em>Dataset size has implications as well:</em></p><ul><li><em><strong>Small dataset:</strong> If the dataset includes &lt;1,000 examples, you can examine every example manually, check if the labels are correct, and even add labels yourself. You’re likely to have only a handful of labelers, so it’s easy to hash out any disagreements together on a call. Every single example is a significant fraction of the dataset, so it’s worthwhile to fix every incorrect label.</em></li><li><em><strong>Large dataset:</strong> If the dataset is &gt;100,000 examples, it’s impractical for a single engineer to examine every one manually. The number of labelers involved is likely to be large, so it’s critical to define standards clearly, and it may be worthwhile to automate labeling. If a significant number of examples are mislabeled, it may be hard to fix them, and you may have to feed the noisy data to your algorithm and hope it can learn a robust model despite the noise.</em></li></ul><p><em>If you find yourself in need of advice while working on, say, a manufacturing visual inspection problem with 100 examples, the best person to ask would be someone who has worked on a manufacturing visual inspection problem with 100 examples. But if you can’t find such a person, consider looking for someone with expertise in the same dataset size/type quadrant as the problem you’re working on.</em></p><p><em>As you develop your career, you might also consider whether you want to stay in one quadrant and develop deep expertise there, or move across quadrants and develop more general skills.</em></p><p><em>Keep learning!</em></p><p><em>Andrew</em></p><p></p><h2 id=\"deeplearningai-exclusive\">DeepLearning.AI Exclusive</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/SWETHA120576x324.png\" class=\"kg-image\" alt=\"SWETHA1 576x324\" loading=\"lazy\"></figure><h2 id=\"working-ai-stoking-gpu-clusters\">Working AI: Stoking GPU Clusters</h2><p>As a senior deep learning engineer at Nvidia, Swetha Mandava helps make models run more efficiently on large-scale hardware. Learn about her onramp to AI and how she stays on track. <a href=\"https://www.deeplearning.ai/blog/pushing-the-state-of-the-art-with-swetha-mandava?ref=dl-staging-website.ghost.io\">Read more</a></p><hr><h2 id=\"news\">News</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/LEAD20576x324.gif\" class=\"kg-image\" alt=\"Series of images related to water lines replacement and poisoned water\" loading=\"lazy\"></figure><h2 id=\"ai-versus-lead-poisoning\">AI Versus Lead Poisoning</h2><p>An algorithm is helping cities locate pipes that could release highly toxic lead into drinking water.<br><br><strong>What’s new:</strong> <a href=\"https://www.blueconduit.com/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">BlueConduit</a>, a startup that focuses on water safety, is working with dozens of North American municipal governments to locate lead water lines so they can be replaced, <em><a href=\"https://www.wired.com/story/algorithm-helping-community-detect-lead-pipes/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Wired</a></em> reported.<br><br><strong>How it works:</strong> For each city, the company develops a custom <a href=\"https://arxiv.org/abs/1806.10692?ref=dl-staging-website.ghost.io\" rel=\"noopener\">model</a> that ranks the likelihood that any given property has lead pipes.</p><ul><li>The company starts by collecting comprehensive data on building locations, ages, market values, occupants, and other variables, BlueConduit executives told <em>The Batch</em>. It works with the local government to gather details from a representative set of properties, including known pipe materials and results of water tests if they’re available.</li><li>It trains a <a href=\"https://en.wikipedia.org/wiki/Gradient_boosting?ref=dl-staging-website.ghost.io\" rel=\"noopener\">gradient boosted tree</a> on the data, tuning the model to account for uncertainties in the dataset.</li><li>The model’s output is used to produce <a href=\"https://flintpipemap.org/map?ref=dl-staging-website.ghost.io\" rel=\"noopener\">maps</a> that officials can use to prioritize removal of potentially hazardous pipes and residents can use to request removal.</li></ul><p><strong>Behind the news:</strong> Founded by faculty at Georgia Tech and University of Michigan, BlueConduit developed its technology to help manage a wave of lead poisoning in Flint, Michigan, between 2014 and 2019. There it achieved <a href=\"https://www.theatlantic.com/technology/archive/2019/01/how-machine-learning-found-flints-lead-pipes/578692/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">70 percent</a> accuracy in classifying properties with lead pipes. Contaminated water in Flint exposed <a href=\"https://www.nrdc.org/stories/flint-water-crisis-everything-you-need-know?ref=dl-staging-website.ghost.io\" rel=\"noopener\">thousands</a> of people to dangerously high levels of lead.<br><br><strong>Why it matters:</strong> Lead exposure can impair development in children, and it’s linked to heart, kidney, and fertility problems in adults. Yet digging up older water lines that may use lead pipes can cost <a href=\"https://freshwaterfuture.org/uncategorized/city-of-toledo-receives-epa-grant-to-utilize-artificial-intelligence-to-identify-lead-water-lines/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">thousands</a> of dollars. Cities can save millions if they can focus on the most dangerous locations and avoid replacing pipes in houses that are already safe.<br><br><strong>We’re thinking:</strong> Flint stopped using BlueConduit’s system in 2018 <a href=\"https://www.theatlantic.com/technology/archive/2019/01/how-machine-learning-found-flints-lead-pipes/578692/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">partly because</a> some residents complained they were being passed over by the AI-driven replacement strategy — a sign of how little they trusted their local government and the unfamiliar technology. The city reinstated the system the following year under pressure from the state and legal actions, but the lesson remains: When you’re deploying a major AI system, establishing trust is as important as tuning parameters.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/VILLA20576x324.gif\" class=\"kg-image\" alt=\"Data related to adversarial learning\" loading=\"lazy\"></figure><h2 id=\"adversarial-helper\">Adversarial Helper</h2><p>Models that learn relationships between images and words are gaining a <a href=\"https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf?ref=dl-staging-website.ghost.io\" rel=\"noopener\">higher</a> <a href=\"https://arxiv.org/abs/2005.04790?ref=dl-staging-website.ghost.io\" rel=\"noopener\">profile</a>. New research shows that adversarial learning, usually a way to make models robust to deliberately misleading inputs, can boost vision-and-language performance.<br><br><strong>What’s new:</strong> Vision-and-language models based on transformer networks have shown strong performance on tasks such as answering questions about images. Zhe Gan of Microsoft and colleagues at Microsoft and the University of Maryland improved such models via <a href=\"https://arxiv.org/abs/2006.06195v2?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Vision-and-Language Large-scale Adversarial (VILLA)</a> training.<br><br><strong>Key insight:</strong> Vision-and-language models often are pretrained, for instance, to fill in blanks in image captions, and then fine-tuned for a specific task, such as answering questions about images. <a href=\"https://arxiv.org/abs/1909.11764?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Previous work</a> with language models showed that adversarial fine-tuning — that is, giving the model input that’s designed to fool it and training it not to be fooled — can increase accuracy. The team extended this idea to vision-and-language models in both pretraining and fine-tuning.<br><br><strong>How it works:</strong> The authors worked with <a href=\"https://arxiv.org/abs/1909.11740?ref=dl-staging-website.ghost.io\" rel=\"noopener\">UNITER</a>, which has achieved state-of-the-art performance on several vision-and-language tasks. UNITER embeds images and text separately. Then it feeds the embeddings into a <a href=\"https://arxiv.org/abs/1810.04805?ref=dl-staging-website.ghost.io\" rel=\"noopener\">BERT</a>-like model to create a multimodal embedding.</p><ul><li>The authors used a variation on <a href=\"https://arxiv.org/abs/1909.11764?ref=dl-staging-website.ghost.io\" rel=\"noopener\">FreeLB</a>, an adversarial training technique. FreeLB perturbs embeddings by learning a small vector that, when added to embeddings, is likely to fool the network, and then training the model to answer correctly regardless.</li><li>The authors perturbed both image and text embeddings, but not at the same time. The model’s objective was threefold: predict the correct answer using unperturbed embeddings, predict the correct answer using perturbed embeddings, and to keep those predictions and confidence in them close to one another.</li><li>They pretrained UNITER to perform masked language modeling (guessing which words are missing from a text passage, usually based on surrounding words, but in this case based on an accompanying image) and image-text matching (guessing whether a text and image are paired). Pretraining involved <a href=\"https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48?ref=dl-staging-website.ghost.io\" rel=\"noopener\">four</a> <a href=\"https://dl.acm.org/doi/abs/10.1007/s11263-016-0981-7?ref=dl-staging-website.ghost.io\" rel=\"noopener\">large</a> <a href=\"https://www.aclweb.org/anthology/P18-1238?ref=dl-staging-website.ghost.io\" rel=\"noopener\">image-and-caption</a> <a href=\"https://papers.nips.cc/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html?ref=dl-staging-website.ghost.io\" rel=\"noopener\">datasets</a>.</li><li>They fine-tuned and tested on several vision-and-language tasks. For instance, <a href=\"https://visualqa.org/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">visual question answering</a> required answering questions about images like, “what color are her eyes?” <a href=\"https://visualcommonsense.com/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Visual commonsense reasoning</a> required answering multiple-choice questions such as, “why is [person4] pointing at [person1]?” followed by “I think so because…”</li></ul><p><strong>Results:</strong> UNITER trained with VILLA outperformed a standard UNITER in six vision-and-language tasks. In visual question answering, UNITER with VILLA answered 73.67 percent correctly, while the plain model answered 72.91 percent correctly. In the two-stage visual commonsense reasoning task of answering a question and justifying the answer, UNITER with VILLA scored 59.75 percent, while its standard counterpart succeeded 57.76 percent of the time.<br><br><strong>Why it matters:</strong> We understand the world through several modalities, and that makes us smarter. For instance, to describe a tree, neither an image nor a biological description is sufficient, but together they have a revealing synergy. Current models still struggle to grasp the meaning of images and language individually, but they will always be missing something until they can draw connections between them.<br><br><strong>We’re thinking:</strong> Vision: check. Language: check. Now sound, aroma, touch . . .</p><hr><h2 id=\"a-message-from-deeplearningai\">A MESSAGE FROM <a href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer noopener\">DEEPLEARNING.AI</a></h2><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/Gif202-1080202-1.gif\" class=\"kg-image\" alt=\"Gif 2-1080 (2)-1\" loading=\"lazy\"></figure><p>All four courses of our <em>TensorFlow: Advanced Techniques Specialization</em> are now available on Coursera! <a href=\"https://www.coursera.org/specializations/tensorflow-advanced-techniques?ref=dl-staging-website.ghost.io\">Enroll now</a></p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/GUN.gif\" class=\"kg-image\" alt=\"Gun detecting system working and alerting the police\" loading=\"lazy\"></figure><h2 id=\"draw-a-gun-trigger-an-algorithm\">Draw a Gun, Trigger an Algorithm</h2><p>Computer vision is alerting authorities the moment someone draws a gun.<br><br><strong>What’s new:</strong> Several companies offer deep learning systems that enable surveillance cameras to spot firearms and quickly notify security guards or police, according to <a href=\"https://www.vice.com/en/article/7k94a9/gun-detection-ai-is-being-trained-with-homemade-active-shooter-videos?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Vice</a>.<br><br><strong>No people were harmed in the training of this model:</strong> Some developers of gun detection models have gone to great lengths to produce training data.</p><ul><li>Virginia-based Omnilert trained its <a href=\"https://www.prnewswire.com/news-releases/omnilert-launches-industrys-first-ai-powered-visual-gun-detection-solution-301161396.html?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Gun Detect</a> system using simulations from video game software, scenes from action movies, and thousands of hours of video depicting employees holding toy or real guns.</li><li>Alabama-headquartered <a href=\"https://arcarithm.com/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Arcarithm</a>, which makes systems for gun detection, produced training data by photographing guns in front of a green screen and compositing them into scenes such as offices. The company created 30,000 to 50,000 images of each of America’s 10 most popular rifles and handguns to train its <a href=\"https://www.archarithms.com/exigent.html?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Exigent-GR</a> software.</li><li>Other companies including <a href=\"https://actuate.ai/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Actuate</a>, <a href=\"https://defendry.com/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Defendry</a>, <a href=\"https://www.scylla.ai/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Scylla</a>, and <a href=\"https://zeroeyes.com/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">ZeroEyes</a> offer similar systems.</li></ul><p><strong>Behind the news:</strong> The use of computer vision in such offerings updates earlier systems based on sounds. For instance, <a href=\"https://www.shotspotter.com/results/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">ShotSpotter</a> is used by over 100 police departments in the U.S. The system picks up gunshot sounds from acoustic sensors placed around a community and uses machine learning to compare them with an audio database. When it recognizes a gunshot, it triangulates the location and alerts police.<br><br><strong>Why it matters: </strong><a href=\"https://www.gunviolencearchive.org/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Gun violence</a> is endemic across the U.S, including <a href=\"https://www.cbsnews.com/news/mass-shootings-2019-more-than-days-365/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">hundreds</a> of mass shootings. By warning police or security guards before a shooter opens up, AI-powered gun detection could save lives.<br><br><strong>We’re thinking:</strong> Like any machine learning system applied to the real world, gun detection algorithms aren’t perfect. <a href=\"https://www.vice.com/en/article/qjpkmx/fac-recognition-company-lied-to-school-district-about-its-racist-tech?ref=dl-staging-website.ghost.io\" rel=\"noopener\">One such system</a> used in New York state schools was found to mistake broom handles for guns. Such mistakes could be dangerous if they prompt police to enter possible crime scenes with their own weapons drawn and pulses pounding.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/CORPORATE.gif\" class=\"kg-image\" alt=\"Some findings from a study by the U.S. nonprofit National Bureau of Economic Research.\" loading=\"lazy\"></figure><h2 id=\"annual-report-robot-edition\">Annual Report, Robot Edition</h2><p>Corporations are tailoring their financial reports to be read by machines.<br><br><strong>What’s new:</strong> Automated systems download far more company financial reports than humans, according to a <a href=\"https://www.nber.org/papers/w27950?ref=dl-staging-website.ghost.io\" rel=\"noopener\">study</a> by the U.S. nonprofit National Bureau of Economic Research. Consequently, companies are filling those reports with data that looks good to computers.<br><br><strong>What they did:</strong> The study analyzed 50 years of quarterly and annual financial reports submitted by public companies to the U.S. Securities and Exchange Commission.</p><ul><li>Drawing on SEC download logs, the authors examined the IP address associated with each download to determine whether a person or a machine initiated it. They found that automated downloads grew from 360,862, or 39 percent of the total, in 2003 to around 165 million, or 78 percent, in 2016.</li><li>Companies that served large numbers of machines-initiated downloads were more likely to make their reports machine-readable by, say, adhering to ASCII standards, separating tables from text, and ensuring that documents contained all the information required to interpret them.</li><li>Moreover, these companies use language more likely to produce positive scores from sentiment-analysis models. For instance, they tend to avoid words associated with negative emotions, lawsuits, or uncertainty.</li></ul><p><strong>Behind the news:</strong> Computer systems increasingly drive the stock market. Last year, Deutsche Bank <a href=\"https://www.economist.com/briefing/2019/10/05/the-stockmarket-is-now-run-by-computers-algorithms-and-passive-managers?ref=dl-staging-website.ghost.io\" rel=\"noopener\">estimated</a> that automated systems made buying and selling decisions for 80 percent of equity trading and 90 percent of equity futures trading. Corporate financials are following suit.<br><br><strong>Why it matters:</strong> The study found that the more easily a computer can digest a company’s financial reports, the faster its stock is traded after a report has been published. This suggests that the market’s pace, already lightning-fast, is bound to accelerate.<br><br><strong>We’re thinking:</strong> Companies have every incentive to tweak their reports to impress their audience, whether readers consist of wetware or software. But there’s a slippery slope between painting a rosy picture and exaggerating in ways that border on fraud. Regulators, analysts, and AI practitioners alike have a responsibility to guard against market manipulation.</p>","comment_id":"60bfd76b274d5b003b10610e","feature_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202021-01-2020at2010.png","featured":false,"visibility":"public","created_at":"2021-06-08T13:47:39.000-07:00","updated_at":"2022-10-06T12:00:47.000-07:00","published_at":"2021-01-20T12:00:00.000-08:00","custom_excerpt":"Experience gained in building a model to solve one problem doesn’t always transfer to building models for other problems. How can you tell whether or not intuitions honed in one project are likely to generalize to another? ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"60bfddad274d5b003b1062c3","name":"issue-75","slug":"issue-75","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-75/"},{"id":"60cc1eda70a082003e13dd23","name":"Jan 20, 2021","slug":"jan-20-2021","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/jan-20-2021/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-75/","excerpt":"Experience gained in building a model to solve one problem doesn’t always transfer to building models for other problems. How can you tell whether or not intuitions honed in one project are likely to generalize to another? ","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Detecting Guns, Fighting Lead Poisoning, Adversarial Training","meta_description":"The Batch - AI News & Insights: AI Versus Lead Poisoning | Computer vision is alerting authorities the moment someone draws a gun | Annual Report, Robot Edition","email_subject":null,"frontmatter":null,"feature_image_alt":"Table with information related to data and datasets","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202021-01-2020at2010.png","dimensions":{"width":576,"height":324}},"banner":{"title":"Data Analytics Professional Certificate","databaseId":36316,"id":"cG9zdDozNjMxNg==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2025/03/Vertical-side-banner-ads-7.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3XWMC3m","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}