{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-86","id":"60bfddae274d5b003b1062f6","uuid":"a077dbec-ae04-44f4-8c95-8fbc7023af9e","title":"The Batch: Doctors Distrust AI, New Life For Old Songs, ImageNet Without Faces, Learning From Random Data","html":"<p><em>Dear friends,</em></p><p><em>Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest <a href=\"https://www.edelman.com/sites/g/files/aatuss191/files/2021-03/2021%20Edelman%20Trust%20Barometer%20Tech%20Sector%20Report_0.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">Edelman Trust Barometer</a> contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. last year, this year we plunged to ninth place. Trust in the tech industry fell to new lows in the majority of 27 countries surveyed.</em><br><br><em>Tech can be a huge force for moving the world forward, but many well meaning efforts will run into headwinds if we aren’t able to gain others’ trust. It’s more urgent than ever that we collectively act in a way that is genuinely deserving of the rest of society’s trust.</em><br><br><em>Trust is much harder to build than to destroy. One company that hypes AI can do more damage than 10 others that speak about it responsibly. One company that makes misleading statements can do more damage than 10 that speak honestly.</em></p><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1-.png\" class=\"kg-image\" alt=\"Chart with percent trust in tech sector vs. business\" loading=\"lazy\" width=\"576\" height=\"324\"></figure><p><em>How can we regain trust? Several steps are needed, but to my mind, chief among them are:</em></p><ul><li><em><strong>Straight talk.</strong> I think we’re all tired of hearing tech companies say they’re fighting for small businesses when they’re just fighting for their own bottom line. I realize that no company can address every issue under the sun, but when we speak about something, we owe it to the public to tell it like it is.</em><br></li><li><em><strong>Take responsibility.</strong> Tech’s influence on what people see and hear has a huge impact on their perception of reality. Our collective influence on automation has a huge impact on jobs. I hope that each organization will acknowledge the power it has and use it to benefit society.</em><br></li><li><em><strong>Engage and empathize.</strong> When someone who is honest and well meaning has a problem with what we do, our first step should be to try to understand their point of view, not to dismiss their concerns. Society has reasonable worries about tech’s concentration of power, fairness, and impact on jobs. Whether we agree or disagree in a certain instance, let's acknowledge the concern and see if we can address it honestly.</em></li></ul><p><em>Trying to fool the public and government officials doesn’t work. We often read in the news about politicians who know little about tech, and say things that reflect their lack of understanding. But let me tell you this: Every large government has at least a handful of people who are tech-savvy enough to see through the spin to the heart of an issue. Companies shouldn’t try to fool people and instead do the harder — but more effective — work of solving problems thoughtfully.</em></p><p><em>On the plus side, 62 percent of respondents to Edelman’s survey agreed that employees have the power to force corporations to change. CEO aren’t the only people responsible for what companies do. All employees have a responsibility to help build trustworthy businesses. Wherever you work, I hope you’ll support straight talk, taking responsibility, and engaging and empathizing.</em></p><p><em>Keep learning!</em></p><p><em>Andrew</em></p><p></p><h2 id=\"news\">News</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/BLUR%20TG.gif?upscale=true&amp;name=BLUR%20TG.gif\" class=\"kg-image\" alt=\"Blurred human faces in different pictures\" loading=\"lazy\"></figure><h2 id=\"de-facing-imagenet\">De-Facing ImageNet</h2><p>ImageNet now comes with privacy protection.</p><p><strong>What’s new:</strong> The team that manages the machine learning community’s go-to image dataset <a href=\"http://image-net.org/face-obfuscation/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">blurred</a> all the human faces pictured in it and tested how models trained on the modified images on a variety of image recognition tasks. The faces originally were included without consent.</p><p><strong>How it worked:</strong> The team used Amazon’s Rekognition platform to find faces in ImageNet’s nearly 1.5 million examples.</p><ul><li>Rekognition drew a bounding box around each of over 500,000 faces. (Some images contained more than one face.) Crowdsourced workers checked the model’s work and corrected errors where necessary. Then the team applied Gaussian blur to the area within bounding boxes.</li><li>The authors trained 24 image recognition architectures on the original ImageNet and copies of the same architectures on the blurred version, and compared their performance. The models trained on the blurred images were, on average, less accurate by under 1 percent. However, the decline was severe with respect to objects typically found close to a face, such as masks (-8.71 percent) and harmonicas (-8.93 percent).</li><li>They tested the blurred data’s effect on transfer learning by pretraining models using the unmodified and modified ImageNet and fine-tuning them for <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">object recognition</a>, <a href=\"https://vision.princeton.edu/projects/2010/SUN/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">scene recognition</a>, <a href=\"http://host.robots.ox.ac.uk/pascal/VOC/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">object detection</a>, and <a href=\"http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">facial attribute classification</a> (whether a person is smiling, wearing glasses, and the like). The models trained on blurred images performed roughly as well as those trained on unmodified ImageNet.</li><li>The face-blurred ImageNet will become the new official version, according to <em><a href=\"https://venturebeat.com/2021/03/16/imagenet-creators-find-blurring-faces-for-privacy-has-a-minimal-impact-on-accuracy/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">VentureBeat</a></em>.</li></ul><p><strong>Behind the news:</strong> This work is part of a wider movement toward protecting privacy in machine learning data. For instance, papers submitted to CVPR in recent years proposed models to automatically blur faces and license plates in <a href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Uittenbogaard_Privacy_Protection_in_Street-View_Panoramas_Using_Depth_and_Multi-View_Imagery_CVPR_2019_paper.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">Google Street View</a> as well as data for training <a href=\"https://arxiv.org/abs/1903.11027?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">autonomous vehicles</a>, and <a href=\"https://arxiv.org/abs/2007.05515?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">action recognition models</a>.</p><p><strong>Why it matters:</strong> Machine learning datasets need not violate privacy. We can develop datasets that both protect privacy and train good models.</p><p><strong>We’re thinking:</strong> Any loss of accuracy is painful, but a small loss is worthwhile to protect privacy. There’s more to life than optimizing test-set accuracy! We expect that most ImageNet-trained applications won’t suffer from the change, as they don’t involve objects that typically appear near to faces. Fine-tuning on a dataset obtained with permission might help for the rest.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/SEER%20(1).gif?upscale=true&amp;name=SEER%20(1).gif\" class=\"kg-image\" alt=\"Data related to SElf-supERvised (SEER), an image classifier pretrained on uncurated, unlabeled images\" loading=\"lazy\"></figure><h2 id=\"pretraining-on-uncurated-data\"><strong>Pretraining on Uncurated Data</strong></h2><p>It’s well established that pretraining a model on a large dataset improves performance on fine-tuned tasks. In sufficient quantity and paired with a big model, even data scraped from the internet at random can contribute to the performance boost.<br><br><strong>What’s new:</strong> Facebook researchers led by Priya Goyal built <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgBLHW3rbBF872GPwmW3FwJ6G1PMVpnW4S2pft3FVHzyN8DyDVNGlnMGW4PP4L97cFqslVlbwbL7yvFDfW2Fn8h-2WNsLhW6ctbWL1j8yZqW8QpsXs7LRPYmW85G0WR8BQkNVW6rNClY7zbRhhW6CPVz91LqySlW1hThV91rb9KJW7k540y64sJNFW3rSflk6RMsLgW5CGgfj3x07y7N3gCXmlJJsKgW39y9PH1FLmkGW4DBD6B4sZ86fMxX7v-9j6Y933B11?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">SElf-supERvised</a> (SEER), an image classifier pretrained on a huge number of uncurated, unlabeled images. It achieved better fine-tuned ImageNet accuracy than models pretrained on large datasets that were curated  to represent particular labels.<strong>Key insight:</strong> Large language models pretrained on billions of uncurated documents found in the wild, such as <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgX7zW6Z0FGx310VxbW64wx5c6QqqCSW8VnTRB6r3YWTVtYrhy79rFy7W5_dnck1wd87gW6T1jS86RVxH9W8h6sKq7Q6pYGW6Wyr1S85ZbyJN4Qd9rMZ55XFW22zwDy17bwCSW38zR7q1gf-k8W7Y11R815ZY5gW7Wf0_k235s-jW59Npct2H0r2RW22G-YK58TddKW2L5pfM5ZXQCtW2t0WHW90vCr_W4n_L4694STNzW5y7GBV4DvmdXW7d75V32kT_nC33wD1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">GPT-3</a>, have achieved state-of-the-art performance after fine-tuning on a smaller dataset. Computer vision should benefit likewise from such scale.<br><br><strong>How it works:</strong> The authors used a 1.3-billion parameter <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSyc3p_b1V1-WJV7CgW7QW3lCvwr2QsmvKW2YfKsS6nSlfsW5fm5GR5qGYpGW72kNsS3ckR5qW2Fb8hS5lZqCcW3kjy4w4xJJFSN2pCYmh-rDTdW5qM6hl6YgHXCW77GRq63cRsbyW6h2FZ1407Kb-W7TYHb39bVYNVW3YFj7z8Lgyy6N8Q-yxFBY35rW4X72HC2tDDLbVrfkrB9lqPDmW4ZXvXL6ggkZJW4LSDbv4905FRW4D0tcD5DCm8qW2ZwRvb3zD3W-W3TPcvc4jWdrnW3dhfL-5kfk6GW4WM94Q9hLHYNW4Kkj172kM4nPW83rZ7Y7DDj7mW5yXtlp740bj4W1d08jH4-G4PsW6RT1Jl2rsVcJW5bdGrw1y4w2qW4BhGgJ663lmRW3WJkpm7w2yJt3nQt1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">RegNet</a>, a convolutional neural network architecture similar to <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSxX3p_9LV1-WJV7CgPzcW5-pbKQ5KLn5FW7BQJmM3p0R5vW5PVLNL7l6mvZW80-pwZ6lzn22W4FHg9y94-0N3W3Q_hdT2lMKBxW7HZSkd2Lv-QxW409-2x3QQHh4W50Z0Yj5G2R-yW8bY5-Z6W_1r3W29HmdN3gDmcPW8T65R524LqY1W4s-N1g1cXGPtN7vNM-f-fCMPW45n7g94RtH3yVtnMND7cm6y6W8cZNVM8MXQ6zW1gXwjR4z8WfVW7j8YpF3sPczGW4TJXPh42dzNKW7GFzJQ3gDl_sW78hV-w3wtKy8VfnWv66BJHBkW551l9788GcYmN70DVr_jFGGkW5tvzV291FC7xW7PW7LR51k3SLW8t1WQs3KTZDm34CZ1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">ResNet</a>, pretrained on 1 billion images randomly scraped from Instagram.</p><ul><li>The pretraining procedure followed <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgSlHW986Zp749ZwVLVD7h8455CkrLW5vRhmG3tTxGRW1kWq0Q5gFRPWW4Cp4pj1JhRj1N22j0sMK5LbFW6mXXnt3-GSybVtsH9h2WhdGzW7yPFbd5zJz1VW6Z3TmR5y1LjDW2zDpxX9gzgYBN6qbqFqvdLZ_W7XcywY6CbS8ZW8zJp4Y5JppQ3W2hwMDM4X1fJ_N29H17y5ZN73W3h_q3L6V-T9NW6p_BJ95b6xL5W6VHfLd3ZmfwYW3SYk1y21FfJx3c0k1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">SwAV</a>, which was devised by several of the same researchers. SwAV receives representations — in this case, from the RegNet — and learns to group related images into a number of clusters by emphasizing similarities among them (similar to contrastive learning).</li><li>The authors fine-tuned the model on over 1 million images from <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSx13p_8SV1-WJV7CgMsZW2qVNfP5HkRHMW5jMBCL2Gn0m0W4F_YMd7zpj_mW8TclHy35ZVXxVMypKR7mngVrW6pQPFB6f-YyYV9QJ8W9flcm9W87vl2p744z6CW5Zc-7B85pn2sW5kKN7y1wShvbW3PvCDx7y88lwW1lV8lh2z3YTJW5JYw9W8Xh9rjW8_5N5S4kszVfW91fmQk7k5KnBW3r-xh-2GJsCYW2t7s4K1qjnqHW5t2jyj2Pz76kW12skZV6076_rW8p0hLz5BDcGKW4H_MX33LcSf4W9bM3zt4pDxb23lfz1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">ImageNet</a>.</li></ul><p><strong>Results:</strong> SEER achieved 84.2 percent top-1 accuracy on the ImageNet test set, 1.1 percent better than the best previous self-supervised, pretrained model (a ResNet of 795 million parameters pretrained on ImageNet using <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7Cg_2JN8MwkDY2fFxFMkq7m9Nf4jcW2sDmlh8VMxBDW8MxbN146LykpW5cC1cF7kCj3yW1Lp-BY2xHmqMW3kS4JZ2NrKPZW67WzW15_SQK2W7sr_PC5wzQ2hW7PXZkl5XBP2tW8LWNy49cXNNgW7tmvjc58j67dN7HlF0ckVCQZW7k5xkK61PgvFW4kvFHz44K_HKW3sPHJ74m9vMjW795y2Y40tGBtW5F-HDN14pJcJW7GtDVY1pWWJ4W8VDsbx4Xl-dl3gM61?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">SimCLRv2</a>). It was 4.3 percentage points better than a 91-million parameter <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgZQCW560Pvw3G0l03W4ZlnZF1wZZzDW95RlS16FBFBSW7dDGr_2SBlbxVbpZyC2j0vJqW99H_-W7j8L8LW7B19l55Njhj_VpXY_X5lVgXJT_LPn6xwrhkN5kJDkt8k_bjW3vHMC86K9rJBW42mwkk4xH90nVy8bN_28d7J0W8Wt3Wc54Mlk-W2K4_rq5CLkLPW1RG0FH2SpQWHW2YssrK5bZrZ7W5JW_Qk3QM6vZW1KSRFh5ChM_qF6nbHP6PL7s38Zy1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">ViT</a> pretrained on <a href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSyc3p_b1V1-WJV7CgVTLW7dh4-j6HxXWxW8pSyQF1ntC2bW3dwYs2461yKGW4PyDMh7xDgRwW8PklRP5dVSYGW2KHSYS4CjMd7W9jHh8w2Q3kZTW3mg5h53xtW6bW9lTPP-30RsXSW88nk1J6LJ-VXW4Vc-xF2BYw7zW7qsGJb1jDJ66W8R3xwq4C7LqtW6p90xm2DmcbYM7Y-Y6y-l6zVVtlCp6VcqgYMqYGM-dQ8F-W5brXdq4WccmzW5GW8QY3KrSY8W31Gmgv5rZvbDW76dBCk2rNTRwW5f6_xZ1T91C2W8SBdkd7WBfSvW4rDbZ21LmF6dW5LTRrT229NlwW580xk56_tmw_W8Nlxdg1d321jW1GRHvb59kG9bW8dLlhr4kmRMcN9g3FcWxlBgg32Zb1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985\" rel=\"noopener\">JFT-300M</a>, a curated dataset of 300 million images from Google Search. SEER also excelled at few-shot learning: Fine-tuned on 10 percent of ImageNet, it achieved 77.9 percent accuracy, 2.2 percentage points lower than a SimCLRv2 model pretrained on 100 percent of ImageNet and fine-tuned on 10 percent of ImageNet.<br><br><strong>Why it matters:</strong> Scraping the internet could be as productive in computer vision as it has been in language processing. Just keep in mind that training models on such data risks violating privacy and consent as well as absorbing the various biases — including objectionable social biases — inherent on the internet.<br><br><strong>We’re thinking:</strong> This paper suggests a tradeoff between the costs of building a curated dataset and training on a larger corpus plucked from the wild. If the cost of curation is high for your application, maybe you can cut it and spend more on training.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/INFLUENCE%20TG.gif?upscale=true&amp;name=INFLUENCE%20TG.gif\" class=\"kg-image\" alt=\"Data related to a diagnostic advice received from a machine learning model vs a human expert\" loading=\"lazy\"></figure><h2 id=\"would-your-doctor-take-ai%E2%80%99s-advice\">Would Your Doctor Take AI’s Advice?</h2><p>Some doctors don’t trust a second opinion when it comes from an AI system.</p><p><strong>What’s new:</strong> A team at MIT and Regensburg University <a href=\"https://www.nature.com/articles/s41746-021-00385-9?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">investigated</a> how physicians responded to diagnostic advice they received from a machine learning model versus a human expert.</p><p><strong>How it works:</strong> The authors recruited doctors to diagnose chest X-rays.</p><ul><li>The physicians fell into two groups: 138 radiologists highly experienced in reading X-rays and 127 internal or emergency medicine specialists with less experience in that task.</li><li>For each case, the doctors were given either accurate or inaccurate advice and told that it was generated by either a model or human expert.</li><li>The physicians rated the advice and offered their own diagnosis.</li></ul><p><strong>Results:</strong> The radiologists generally rated as lower-quality advice they believed was generated by AI. The others rated AI and human advice to be roughly of equal quality.</p><p>Both groups made more accurate diagnoses when given accurate advice, regardless of its source. However, 27 percent of radiologists and 41 percent of the less experienced offered an incorrect diagnosis when given inaccurate advice.</p><p><strong>Behind the news:</strong> AI-powered diagnostic tools are proliferating and becoming more widely accepted <a href=\"https://www.nature.com/articles/s41746-020-00324-0?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">in the U.S.</a> and elsewhere. These tools may <a href=\"https://www.aiin.healthcare/topics/diagnostics/machine-learning-risk-assessment-cardiovascular-disease?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">work about as well as traditional methods</a> at predicting clinical outcomes. Those that work well <a href=\"https://www.wired.com/story/ai-diagnose-illnesses-country-rich/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">may only do so on certain populations</a> due to biased training data.</p><p><strong>Why it matters:</strong> It’s not enough to develop AI systems in isolation. It’s important also to understand how humans use them. The best diagnostic algorithm in the world won’t help if people don’t heed its recommendations.</p><p><strong>We’re thinking:</strong> While some doctors are skeptical of AI, others may trust it too much, which also can lead to errors. Practitioners in a wide variety of fields will need to cultivate a balance between skepticism and trust in machine learning systems. We welcome help from the computer-human interface community in wrestling with these challenges.</p><hr><h2 id=\"a-message-from-deeplearningai\">A MESSAGE FROM <a href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\">DEEPLEARNING.AI</a></h2><figure class=\"kg-card kg-image-card\"><a href=\"https://www.coursera.org/specializations/deep-learning?ref=dl-staging-website.ghost.io\"><img src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20(1)-1.png?upscale=true&amp;name=The%20Batch%20(1)-1.png\" class=\"kg-image\" alt=\"The Batch (1)-1\" loading=\"lazy\"></a></figure><p>We've updated our Deep Learning Specialization with the latest advances. The new program covers TensorFlow 2 as well as advanced architectures like U-Net, MobileNet, and EfficientNet. Stay tuned for transformers! <a href=\"https://www.coursera.org/specializations/deep-learning?ref=dl-staging-website.ghost.io\">Enroll now</a></p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/upmix.gif?upscale=true&amp;name=upmix.gif\" class=\"kg-image\" alt=\"Recording unmixed\" loading=\"lazy\"></figure><h2 id=\"new-life-for-old-songs\">New Life for Old Songs</h2><p>Neural networks can tease apart the different sounds in musical recordings.</p><p><strong>What’s new:</strong> Companies and hobbyists are using deep learning to separate voices and instruments in commercial recordings, <a href=\"https://www.wired.com/story/upmixing-audio-recordings-artificial-intelligence/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"><em>Wired</em></a> reported. The process can improve the sound of old recordings and opens new possibilities for sampling, mash-ups, and other fresh uses.</p><p><strong>How it works:</strong> Finished recordings often combine voices, instruments, and other sounds recorded in a multitrack format into a smaller number of audio channels; say, one for mono or two for stereo. The mingling of signals limits how much the sonic balance can be changed afterward, but neural networks have learned to disentangle individual sounds — including noise and distortion — so they can be rebalanced or removed without access to the multitrack recordings.</p><ul><li><a href=\"https://www.theaudioresearchgroup.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">Audio Research Group</a> was founded by an audio technician at Abbey Road Studios, who developed a deep learning system to remix the Beatles’ 1964 hit, “She Loves You,” which was produced without multitracking.</li><li><a href=\"https://audionamix.com/2019/05/08/audionamix-showcases-advanced-audio-ai-at-vivatech-2/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">Audionamix</a> separates mono recordings into tracks for vocals, drums, bass guitar, and other sounds. The service has been used to manipulate old recordings for use in commercials and to purge television and film soundtracks of music, inadvertently playing in the background, that would be expensive to license.</li><li>French music streaming service Deezer offers <a href=\"https://github.com/deezer/spleeter?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">Spleeter</a>, an open-source system that unmixes recordings (pictured above). Users have scrubbed vocals to produce custom karaoke tracks, create <a href=\"https://twitter.com/waxpancake/status/1191519229029142528?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\">oddball mashups</a>, and cleanse their own recordings of unwanted noises.</li></ul><p><strong>Why it matters:</strong> Many worthwhile recordings are distorted or obscured by noises like an audience’s cheers or analog tape hiss, making the quality of the musicianship difficult to hear. Others could simply use a bit of buffing after decades of improvement in playback equipment. AI-powered unmixing can upgrade such recordings as well as inspire new uses for old sounds.</p><p><strong>We’re thinking:</strong> Endless remixes of our favorite Taylor Swift tracks? We like the sound of that!</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif?upscale=true&amp;name=ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif\" class=\"kg-image\" alt=\"Examples of image generators using GANsformer\" loading=\"lazy\"></figure><h2 id=\"attention-for-image-generation\">Attention for Image Generation</h2><p>Attention quantifies how each part of one input affects the various parts of another. Researchers added a step that reverses this comparison to produce more convincing images.<br><br><strong>What’s new:</strong> Drew A. Hudson at Stanford and C. Lawrence Zitnick at Facebook chalked up a new state of the art in generative modeling by integrating attention layers into a generative adversarial network (GAN). They call their system <a href=\"https://arxiv.org/abs/2103.01209?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">GANsformer</a>.<br><br><strong>Key insight:</strong> Typically, a GAN learns through competition between a generator that aims to produce realistic images and a discriminator that judges whether images are generated or real. <a href=\"https://arxiv.org/abs/1812.04948?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">StyleGAN</a> splits the generator into (a) a mapping network and (b) a synthesis network, and uses the output of the mapping network to control high-level properties (for example, pose and facial expression) of an image generated by the synthesis network. The output of the mapping layer can be viewed as a high-level representation of the scene, and the output of each layer of the synthesis network as a low-level representation. The authors devised a two-way version of attention, which they call duplex attention, to refine each representation based on the other.<br><br><strong>How it works:</strong> GANsformer is a modified StyleGAN. The authors trained it on four types of subject matter: faces in <a href=\"https://github.com/NVlabs/ffhq-dataset?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">FFHQ;</a> scenes composed of cubes, cylinders, and spheres in <a href=\"https://cs.stanford.edu/people/jcjohns/clevr/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">CLEVR</a>; pictures of bedrooms in <a href=\"https://github.com/fyu/lsun?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">LSUN</a>; and urban scenes in <a href=\"https://www.cityscapes-dataset.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">Cityscapes.</a></p><ul><li>Given a random vector, the mapping network produced an intermediate representation via a series of fully connected layers. Given a random vector, the synthesis network produced an image via alternating layers of convolution and duplex attention.</li><li>The authors fed the mapping network's intermediate representation to the synthesis network’s first duplex attention layer.</li><li>Duplex attention updated the synthesis network’s representation by calculating how each part of the image influenced the parts of the intermediate representation. Then it updated the intermediate representation by calculating how each of its parts influenced the parts of the image. In this way, the system refined the mapping network’s high-level view according to the synthesis network’s low-level details and vice versa.</li><li>The discriminator used duplex attention to iteratively hone the image representation along with a learned vector representing general scene characteristics. Like the synthesis network, it comprised alternating layers of convolution and duplex attention.</li></ul><p><strong>Results:</strong> GANsformer outperformed the previous state of the art on CLEVR, LSUN-Bedroom, and Cityscapes (comparing Fréchet Inception Distance based on representations produced by a pretrained <a href=\"https://arxiv.org/abs/1409.4842v1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">Inception</a> model). For example, on Cityscapes, GANsformer achieved 5.7589 FID compared to <a href=\"https://arxiv.org/abs/1912.04958?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">StyleGAN2</a>’s 8.3500 FID. GANsformer also learned more efficiently than a <a href=\"https://arxiv.org/abs/1406.2661?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">vanilla GAN</a>, StyleGAN, StyleGAN2, <a href=\"https://arxiv.org/abs/1810.10340?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">k-GAN</a>, and <a href=\"https://arxiv.org/abs/1805.08318?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\">SAGAN</a>. It required a third as many training iterations to achieve equal performance.<br><br><strong>Why it matters:</strong> Duplex attention helps to generate scenes that make sense in terms of both the big picture and the details. Moreover, it uses memory and compute efficiently: Consumption grows linearly as input size increases. (In transformer-style self-attention, which evaluates the importance of each part of an input with respect to other parts of the same input, memory and compute cost grows quadratically with input size.)<br><br><strong>We’re thinking:</strong> Transformers, which alternate attention and fully connected layers, perform better than other architectures in language processing. This work, which alternates attention and convolutional layers, may bring similar improvements to image processing.</p>","comment_id":"60be9659274d5b003b10601f","feature_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1--2.png","featured":false,"visibility":"public","created_at":"2021-06-07T14:57:45.000-07:00","updated_at":"2022-10-06T08:07:43.000-07:00","published_at":"2021-04-07T12:00:00.000-07:00","custom_excerpt":"Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest Edelman Trust Barometer contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"60bfddad274d5b003b1062b8","name":"issue-86","slug":"issue-86","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-86/"},{"id":"63176d4e1dce94004d638e14","name":"Apr 07, 2021","slug":"apr-07-2021","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/apr-07-2021/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-86/","excerpt":"Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest Edelman Trust Barometer contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. ","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Doctors Distrust AI, New Life For Old Songs, ImageNet...","meta_description":"The Batch - AI News & Insights: Would Your Doctor Take AI’s Advice? | ImageNet now comes with privacy protection | Neural networks can tease apart the different sounds in musical recordings","email_subject":null,"frontmatter":null,"feature_image_alt":"Chart with percent trust in tech sector vs. business","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1--2.png","dimensions":{"width":576,"height":324}},"banner":{"title":"Data Analytics Professional Certificate","databaseId":36316,"id":"cG9zdDozNjMxNg==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2025/03/Vertical-side-banner-ads-7.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3XWMC3m","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}