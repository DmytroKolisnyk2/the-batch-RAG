{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-287","id":"67a3e4c802b44f0001f4d881","uuid":"635cb7af-505a-4e72-a6b4-e11419459892","title":"o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, More-Responsive Voice Interactions","html":"<p>Dear friends,</p><p>A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.”</p><p>There aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job).</p><p>But for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow.</p><p>10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done.</p><p>I think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6-1.jpg\" class=\"kg-image\" alt=\"Comic-style illustration of a confident woman and man standing beside bold ‘10X’ text on a bright background.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/10x_1200px_6-1.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/10x_1200px_6-1.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6-1.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Similarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way.</p><p>A 2023 Harvard/BCG&nbsp;<a href=\"https://www.hbs.edu/faculty/Pages/item.aspx?num=64700&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">study</a>&nbsp;estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves.</p><p>Here in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.”</p><p>Keep learning!</p><p>Andrew</p><hr><h2 id=\"a-message-from-deeplearningai\">A MESSAGE FROM&nbsp;DEEPLEARNING.AI</h2><figure class=\"kg-card kg-image-card\"><a href=\"https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?ref=dl-staging-website.ghost.io\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--6-.png\" class=\"kg-image\" alt=\"Promo banner for &quot;How Transformer LLMs Work&quot;\" loading=\"lazy\" width=\"1680\" height=\"945\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1600w, https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1680w\" sizes=\"(min-width: 720px) 720px\"></a></figure><p>Learn in detail how transformer-based large language models work in this new course by the authors of&nbsp;<em>Hands-On Large Language Models</em>. Explore the architecture introduced in the paper “Attention Is All You Need,” and learn through intuitive explanations and code examples.&nbsp;<a href=\"https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\">Join in for free</a></p><h1 id=\"news\">News</h1><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/02/O3.gif\" class=\"kg-image\" alt=\"Bar chart animation showing accuracy improvements in AIME 2024 competition math models.\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2025/02/O3.gif 600w\"></figure><h1 id=\"reasoning-in-high-gear\"><strong>Reasoning in High Gear</strong></h1><p>OpenAI introduced a successor to its o1 models that’s faster, less expensive, and especially strong in coding, math, and science.</p><p><strong>What’s new:</strong>&nbsp;o3-mini is a large language model that offers selectable low, medium, and high levels of reasoning “effort.” These levels consume progressively higher numbers of reasoning tokens (specific numbers and methods are undisclosed), and thus greater time and cost, to generate a chain of thought. It’s&nbsp;<a href=\"https://openai.com/index/openai-o3-mini/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">available</a>&nbsp;to subscribers to ChatGPT Plus, Team, and Pro, as well as to higher-volume users of the API (tiers 3 through 5). Registered users can try it via the free ChatGPT service by selecting “reason” in the message composer or selecting o3-mini before regenerating a response.</p><p><strong>How it works:</strong>&nbsp;o3-mini’s training set emphasized structured problem-solving in science and technology fields, and fine-tuning used reinforcement learning on chain-of-thought (CoT) data. Like the o1 family, it charges for tokens that are processed during reasoning operations and hides them from the user. (Competing reasoning models DeepSeek-R1, Gemini 2.0 Flash Thinking, and QwQ-32B-Preview make these tokens available to users.) o3-mini has a maximum input of 200,000 tokens and a maximum output of 100,000 tokens. Its knowledge cutoff is October 2023.</p><ul><li>In OpenAI’s tests, o3-mini beat o1 and o1-mini on multiple benchmarks including math (AIME 2024), science (GPQA Diamond), and coding (Codeforces and LiveBench). It outperformed o1 by 1 to 4 percentage points when set at high or medium effort, and it outperformed o1-mini when set at low effort. It did significantly less well on tests of general knowledge, even with high effort. On MMLU (multiple-choice questions in many fields) and SimpleQA (questions about basic facts), o3-mini with high effort (which achieved 86.9 percent and 13.8 percent respectively) underperformed o1 (92.3 percent and 47 percent) and GPT-4o (88.7 percent and 39 percent).</li><li>Unlike o1-mini, o3-mini supports&nbsp;<a href=\"https://platform.openai.com/docs/guides/function-calling?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">function calling</a>,&nbsp;<a href=\"https://platform.openai.com/docs/guides/structured-outputs?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">structured outputs</a>&nbsp;(JSON format),&nbsp;<a href=\"https://platform.openai.com/docs/guides/text-generation?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP#building-prompts\" rel=\"noopener\">developer messages</a>&nbsp;(system prompts that specify the model’s context or persona separately from user input), and&nbsp;<a href=\"https://platform.openai.com/docs/api-reference/streaming?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">streaming</a>&nbsp;(delivering responses token-by-token in real time).</li><li>API access&nbsp;<a href=\"https://openai.com/api/pricing/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">costs</a>&nbsp;$1.10/$4.40 per million input/output tokens with a discounted rate of $0.55 per million cached input tokens. OpenAI’s&nbsp;<a href=\"https://platform.openai.com/docs/guides/batch?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Batch API</a>, which processes high-volume requests asynchronously, costs half as much. In comparison, access to o1 costs $15/$60 per million input/output tokens and o1-mini costs $3/$12 per million input/output tokens. (OpenAI recently removed API pricing for o1-mini and, in the ChatGPT model picker, replaced it with o3-mini, which suggests that o1-mini is being phased out.)</li><li>OpenAI&nbsp;<a href=\"https://platform.openai.com/docs/guides/rate-limits/usage-tiers?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP#tier-3-rate-limits\" rel=\"noopener\">limits</a>&nbsp;the number API calls users can make per minute and per day depending on how frequently they use the API and how much money they’ve spent. Rate limits range from 5,000/4 million requests/tokens per per minute (Tier 3) to 30,000/150 million requests/tokens per minute (Tier 5), with higher limits for batch requests.</li><li>o3-mini’s&nbsp;<a href=\"https://openai.com/index/o3-mini-system-card/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">system card</a>&nbsp;highlights safety measures taken during the model’s training. OpenAI notes that o3-mini’s improved coding ability puts it at a medium risk for autonomous misuse, the first OpenAI model to be so flagged.</li></ul><p><strong>What they’re saying:</strong>&nbsp;Users&nbsp;<a href=\"https://x.com/noahmacca/status/1885519380655857999?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">praised</a>&nbsp;o3-mini for its speed, reasoning, and coding abilities. They noted that it responds best to “chunkier” prompts with lots of context. However, due to its smaller size, it lacks extensive real-world knowledge and struggles to recall facts.</p><p><strong>Behind the news:</strong>&nbsp;Days after releasing o3-mini, OpenAI launched&nbsp;<a href=\"https://openai.com/index/introducing-deep-research/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">deep research</a>, a ChatGPT research agent based on o3. OpenAI had&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/deepseek-v3-is-the-new-best-open-model/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">announced</a>&nbsp;the o3 model family in December, positioning it as an evolution of its chain-of-thought approach. The release followed hard upon that of DeepSeek-R1, an open weights model that captivated the AI community with its high performance and low training cost, but OpenAI&nbsp;<a href=\"https://www.wired.com/story/openai-o3-mini-release/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">maintained</a>&nbsp;that the debut took place on its original schedule.</p><p><strong>Why it matters:</strong>&nbsp;o3-mini continues OpenAI’s leadership in language models and further refines the reasoning capabilities introduced with the o1 family. In focusing on coding, math, and science tasks, it takes advantage of the strengths of reasoning models and raises the bar for other model builders. In practical terms, it pushes AI toward applications in which it’s a reliable professional partner rather than a smart intern.</p><p><strong>We’re thinking:</strong>&nbsp;We’re glad that o3-mini is available to users of ChatGPT’s free tier as well as paid subscribers and API users. The more users become familiar with how to prompt reasoning models, the more value they’ll deliver.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/02/UITARS.png\" class=\"kg-image\" alt=\"Flowchart illustrating the automation of opening, editing, and saving a Word document using PyAutoGUI.\" loading=\"lazy\" width=\"2000\" height=\"1125\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/UITARS.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/UITARS.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/02/UITARS.png 1600w, https://dl-staging-website.ghost.io/content/images/size/w2400/2025/02/UITARS.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"training-for-computer-use\"><strong>Training for Computer Use</strong></h1><p>As Anthropic, Google, OpenAI, and others roll out agents that are capable of computer use, new work shows how underlying models can be trained to do this.</p><p><strong>What’s new:</strong>&nbsp;Yujian Qin and colleagues at ByteDance and Tsinghua University introduced&nbsp;<a href=\"https://arxiv.org/abs/2501.12326?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">UI-TARS</a>, a fine-tuned version of the vision-language model Qwen2-VL that uses lines of reasoning to decide which mouse clicks, keyboard presses, and other actions to take in desktop and mobile apps. The model’s weights are&nbsp;<a href=\"https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">licensed</a>&nbsp;freely for commercial and noncommercial uses via Apache 2.0. You can download them&nbsp;<a href=\"https://huggingface.co/bytedance-research/UI-TARS-72B-DPO?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">here</a>.</p><ul><li>The authors added&nbsp;<a href=\"https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">chains of thought</a>&nbsp;(CoTs) to their training set of screenshots and actions by prompting an unspecified vision-language model to explain the current action given previous screenshots, actions, and generated CoTs. Sometimes that process led to bad explanations, so they also generated multiple CoTs and actions for a given screenshot and selected the CoT that led to the correct action.</li><li>They fine-tuned UI-TARS to generate a CoT and action from an instruction (such as “Open the document and add the text ‘hello’”) plus the screenshots, CoTs, and actions so far.</li><li>They ran UI-TARS within a virtual PC, generating a large number of screenshots, CoTs, and actions so far. They filtered out erroneous CoTs and actions using rules (such as removing those that included redundant actions), scoring outputs automatically and removing those with low scores, and reviewing them manually. They fine-tuned the model on the remaining outputs and repeatedly generated, filtered, and fine-tuned.</li><li>They also fine-tuned the model on corrected examples of&nbsp;erroneous CoTs and actions. Human annotators corrected the CoT and action to (a) avoid the error and (b) fix the error after it occurred.</li><li>Finally, they fine-tuned the model using&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/human-feedback-without-reinforcement-learning/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Direct Preference Optimization</a>&nbsp;(DPO) to prefer generating the corrected examples over the erroneous examples from the previous step.</li><li>At inference, given a screenshot, an instruction, and potential actions (as is typical with open computer use models; the authors provide a handy list in a sample prompt), UI-TARS generated a CoT and an action to take. After taking that action (via&nbsp;<a href=\"https://pypi.org/project/PyAutoGUI/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">PyAutoGUI</a>, a Python module that controls computers), the model received a new screenshot and generated another chain of thought and action, and so on. At each step, the model produced a new chain of thought and action, taking into account the instruction and all CoTs, actions, and screenshots so far.</li></ul><p><strong>Behind the news:</strong>&nbsp;Adept&nbsp;<a href=\"https://techcrunch.com/2022/04/26/2304039/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">touted</a>&nbsp;computer use in early 2022, and&nbsp;<a href=\"https://arxiv.org/abs/2408.00203?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">OmniParser</a>&nbsp;<a href=\"https://arxiv.org/abs/2412.04454?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Aguvis</a>&nbsp;soon followed with practical implementations. In October 2024, Anthropic set off the current wave of model/app interaction with its&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/anthropic-empowers-claude-sonnet-3-5-to-operate-desktop-apps-but-cautions-remain/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">announcement</a>&nbsp;of computer use for Claude 3.5 Sonnet. OpenAI recently&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/openais-operator-automates-online-tasks-with-a-new-ai-agent/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">responded</a>&nbsp;with Operator, its own foray into using vision and language models to control computers.</p><p><strong>Results:</strong>&nbsp;UI-TARS matched or outperformed Claude 3.5 Sonnet with computer use, GPT-4o with various computer use frameworks, and the Aguvis framework with its native model on 11 benchmarks. On OSWorld, which asks models to perform tasks using a variety of real-world applications and operating systems, UI-TARS successfully completed 22.7 percent of the tasks in 15 steps, whereas Claude 3.5 Sonnet with computer use completed 14.9 percent, GPT-4o with Aguvis 17 percent, and Aguvis with its native model 10.3 percent.</p><p><strong>Why it matters:</strong>&nbsp;Training a model to take good actions enables it to perform well. Training it to correct its mistakes after making them enables it to recover from unexpected issues that may occur in the real world.</p><p><strong>We</strong>’<strong>re thinking:</strong>&nbsp;Since computer use can be simulated in a virtual machine, it’s possible to generate massive amounts of training data automatically. This is bound to spur rapid progress in computer use by large language models.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/02/FLASH2THINKING.png\" class=\"kg-image\" alt=\"Line charts showing performance improvements in math and science with 2.0 Flash Thinking models.\" loading=\"lazy\" width=\"1200\" height=\"640\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/FLASH2THINKING.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/FLASH2THINKING.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/02/FLASH2THINKING.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"gemini-thinks-faster\">Gemini Thinks Faster</h1><p>Google updated the December-vintage reasoning model Gemini 2.0 Flash Thinking and other Flash models, gaining ground on OpenAI o1 and DeepSeek-R1.</p><p><strong>What’s new:</strong>&nbsp;<a href=\"https://deepmind.google/technologies/gemini/flash-thinking?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Gemini 2.0 Flash Thinking Experimental 1-21</a>&nbsp;is a vision-language model (images and text in, text out) that’s trained to generate a structured reasoning process or chain of thought. The new version improves on its predecessor’s reasoning capability and extends its context window. It's free to access via&nbsp;<a href=\"https://ai.google.dev/gemini-api/docs/thinking?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">API</a>&nbsp;while it remains designated “experimental” and&nbsp;<a href=\"https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">available</a>&nbsp;to paid users of the Gemini app, along with&nbsp;<a href=\"https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Gemini 2.0 Flash</a>&nbsp;(fresh out of experimental mode) and the newly released&nbsp;<a href=\"https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-pro-exp-02-05&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Gemini 2.0 Pro Experimental</a>. The company also launched a preview of&nbsp;<a href=\"https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-lite-preview-02-05&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Gemini 2.0 Flash Lite</a>, a vision-language model (images and text in, text out) that outperforms Gemini 1.5 Flash at the same price.</p><p><strong>How it works:</strong>&nbsp;Gemini 2.0 Flash Thinking Experimental 1-21 is based on&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/google-introduces-gemini-2-0-flash-a-faster-more-capable-ai-model/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Gemini 2.0 Flash Experimental</a>&nbsp;(parameter count undisclosed). It processes up to 1 million tokens of input context, compared to its predecessor’s 32,000 and o1’s 128,000.</p><ul><li>Unlike o1, which hides its chain of thought, but like DeepSeek-R1 and Qwen QwQ, Gemini 2.0 Flash Thinking Experimental 1-21 includes its reasoning in its output.</li><li>On the graduate-level science exam GPQA-Diamond, it achieved 74.2 percent compared to the earlier version’s 58.6 percent, surpassing DeepSeek-R1 (71.5 percent) but behind o1 (77.3 percent).</li><li>On the advanced math benchmark AIME 2024, it achieved 73.3 percent compared to the previous version’s 35.5 percent, but it trails behind DeepSeek-R1 (79.8 percent) and o1 (74.4 percent).</li><li>On the visual and multimedia understanding test MMMU, it achieved 75.4 percent to outperform the previous version (70.7 percent) but fell short of o1 (78.2 percent).</li><li>Developers can integrate Python code execution via the&nbsp;<a href=\"https://ai.google.dev/gemini-api/docs/thinking?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">API</a>, with support for data analysis and visualization through&nbsp;<a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution?t&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">pre-installed libraries</a>.</li></ul><p><strong>Speed bumps:</strong>&nbsp;Large language models that are trained to generate a&nbsp;<a href=\"https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">chain of thought</a>&nbsp;(CoT) are boosting accuracy even as the additional processing increases inference costs and latency. Reliable measures of Gemini 2.0 Flash Thinking Experimental 1-21’s speed are not yet available, but its base model runs faster (168.8 tokens per second with 0.46 seconds of latency to the first token,&nbsp;<a href=\"https://artificialanalysis.ai/models/gemini-2-0-flash-experimental?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">according to</a>&nbsp;Artificial Analysis) than all models in its class except o1-mini (which outputs 200 tokens per second with 10.59 seconds of latency to the first token).</p><p><strong>Why it matters:</strong>&nbsp;The combination of CoT reasoning and long context — assuming the new model can take advantage of its 1 million-token context window, as measured by a benchmark such as&nbsp;<a href=\"https://arxiv.org/abs/2404.06654?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">RULER</a>&nbsp;— could open up valuable applications. Imagine a reasoning model that can take an entire codebase as input and analyze it without breaking it into smaller chunks.<br><strong>We’re thinking:</strong>&nbsp;Regardless of benchmark performance, this model topped the&nbsp;<a href=\"https://lmarena.ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Chatbot Arena</a>&nbsp;leaderboard at the time of writing. This suggests that users preferred it over o1 and DeepSeek-R1 — at least for common, everyday prompts.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/02/MOSHI.gif\" class=\"kg-image\" alt=\"Diagram illustrating Moshi’s use of an LLM to process user audio input, inner monologue, and output.\" loading=\"lazy\" width=\"600\" height=\"336\" srcset=\"https://dl-staging-website.ghost.io/content/images/2025/02/MOSHI.gif 600w\"></figure><h1 id=\"okay-but-please-don%E2%80%99t-stop-talking\">Okay, But Please Don’t Stop Talking</h1><p>Even cutting-edge, end-to-end, speech-to-speech systems like ChatGPT’s Advanced Voice Mode tend to get interrupted by interjections like “I see” and “uh-huh” that keep human conversations going. Researchers built an open alternative that’s designed to go with the flow of overlapping speech.</p><p><strong>What’s new:</strong>&nbsp;Alexandre Défossez, Laurent Mazaré, and colleagues at Kyutai, a nonprofit research lab in Paris, released&nbsp;<a href=\"https://kyutai.org/Moshi.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Moshi</a>, an end-to-end, speech-to-speech system that’s always listening and always responding. The&nbsp;<a href=\"https://huggingface.co/kyutai/moshiko-pytorch-bf16?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">weights</a>&nbsp;and&nbsp;<a href=\"https://github.com/kyutai-labs/moshi/tree/main?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">code</a>&nbsp;are free for noncommercial and commercial uses under&nbsp;<a href=\"https://choosealicense.com/licenses/cc-by-4.0/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">CC-BY 4.0</a>,&nbsp;<a href=\"https://github.com/square/moshi/blob/master/LICENSE.txt?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Apache 2.0</a>, and&nbsp;<a href=\"https://github.com/kyutai-labs/moshi/blob/main/LICENSE-MIT?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">MIT</a>&nbsp;licenses. You can try a web demo&nbsp;<a href=\"https://moshi.chat/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">here</a>.</p><p><strong>Key insight:</strong>&nbsp;Up to 20 percent of spoken conversation consists of&nbsp;<a href=\"https://www.isca-archive.org/interspeech_2006/cetin06_interspeech.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">overlapping speech</a>, including interjections like “okay” and “I see.”</p><ul><li>To respond appropriately despite such overlaps, a system must both listen and generate sound continuously — although much of what it will generate is silence.</li><li>To respond without delay, it must keep latency to a minimum. This goal requires an end-to-end design rather than a pipeline of stand-alone models to perform voice detection, speech-to-text, text processing, and text-to-speech in turn.</li></ul><p><strong>How it works:</strong>&nbsp;The authors combined an encoder-decoder called Mimi and an&nbsp;<a href=\"https://arxiv.org/abs/2203.01941?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">RQ-Transformer</a>, which is made up of the Helium transformer-based large language model (LLM) plus another transformer.</p><ul><li>Mimi’s encoder embedded spoken input using 8 audio tokens per timestep (80 milliseconds). The authors trained Mimi on 7 million hours of mostly English speech from undisclosed sources. The training involved two loss terms. (i) The first loss term encouraged Mimi, given one audio timestep, to produce audio that fooled a pretrained&nbsp;<a href=\"https://arxiv.org/pdf/2210.13438?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">MS-STFT discriminator</a>&nbsp;into thinking it was human speech. The second loss term distilled knowledge from a pretrained&nbsp;<a href=\"https://huggingface.co/microsoft/wavlm-large?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">WavLM</a>, an audio embedding model. It encouraged Mimi’s encoder, when Mimi and WavLM received the same audio timestep, to produce one audio token (of its 8 audio tokens per timestep) whose embedding was similar to the corresponding embedding produced by WavLM.</li><li>Given the audio tokens, the Helium LLM produced text tokens that were used internally to help the additional transformer predict the next audio token (the idea being that the LLM’s skill with words would inform which audio token to generate next). The authors trained Helium to predict the next text token in 2.1 trillion tokens of English text (12.5 percent from&nbsp;<a href=\"https://dumps.wikimedia.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Wikipedia</a>&nbsp;and&nbsp;<a href=\"https://archive.org/details/stackexchange?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Stack Exchange</a>, and the remaining 87.5 percent from&nbsp;<a href=\"https://commoncrawl.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Common Crawl</a>).</li><li>RQ-Transformer received many sets of 17 tokens per time step: 8 audio tokens encoded by Mimi from the audio input, 8 audio tokens from Moshi’s previously generated audio output, and 1 text token produced by Helium. RQ-Transformer learned to predict the next set of 17 tokens in 7 million hours of audio and transcribed text.</li><li>To train the system specifically on conversational interaction, the authors further trained it to predict the next token in 2,000 hours of&nbsp;<a href=\"https://catalog.ldc.upenn.edu/LDC2004S13?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">recorded phone conversations</a>&nbsp;between randomly paired participants.</li><li>At inference, given a user's speech, Mimi turned it into audio tokens. Given the audio tokens and RQ-Transformer’s previously generated audio and text tokens, RQ-Transformer generated new audio and text tokens. From the generated audio tokens, Mimi produced synthetic speech.</li></ul><p><strong>Results:</strong>&nbsp;In tests, Moshi proved fast and relatively accurate.</p><ul><li>Moshi (7 billion parameters) took around 200 milliseconds to respond to user input. In comparison, GPT-4o, which also produces speech output directly from speech input, took 232 milliseconds minimum (320 milliseconds average). Prior to GPT-4o, ChatGPT Voice Mode (a pipeline of speech-to-text, text-to-text, and text-to-speech models) took an average of 5.4 seconds.</li><li>Moshi achieved 26.6 percent accuracy on Web Questions, higher than the speech-to-text-to-speech models tested by the authors:&nbsp;<a href=\"https://arxiv.org/abs/2305.15255?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">Spectron</a>&nbsp;(1 billion parameters) achieved 6.1 percent accuracy and&nbsp;<a href=\"https://arxiv.org/abs/2305.11000?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\">SpeechGPT</a>&nbsp;(7 billion parameters) achieved 6.5 percent accuracy. The authors didn’t provide comparable results for GPT-4o or ChatGPT Voice.</li></ul><p><strong>Why it matters:</strong>&nbsp;While a turn-based approach may suffice for text input, voice-to-voice interactions benefit from a system that processes both input and output quickly and continuously. Previous systems process input and output separately, making users wait. Moshi delivers seamless interactivity.</p><p><strong>We’re thinking:</strong>&nbsp;Generating silence is golden!</p>","comment_id":"67a3e4c802b44f0001f4d881","feature_image":"https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6.jpg","featured":false,"visibility":"public","created_at":"2025-02-05T14:23:04.000-08:00","updated_at":"2025-02-05T14:43:07.000-08:00","published_at":"2025-02-05T14:40:00.000-08:00","custom_excerpt":"The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"67a3e94602b44f0001f4d8bd","name":"Feb 05, 2025","slug":"feb-05-2025","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/feb-05-2025/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-287/","excerpt":"The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer.","reading_time":14,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more...","meta_description":"The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer.","email_subject":null,"frontmatter":null,"feature_image_alt":"Comic-style illustration of a confident woman and man standing beside bold ‘10X’ text on a bright background.","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6.jpg","dimensions":{"width":1200,"height":676}},"banner":{"title":"Mathematics for Machine learning and data science specialization","databaseId":29052,"id":"cG9zdDoyOTA1Mg==","featuredImage":{"node":{"altText":"Mathematics for Machine learning and data science specialization. Enroll now to the course","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/3.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://www.deeplearning.ai/courses/mathematics-for-machine-learning-and-data-science-specialization/","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}