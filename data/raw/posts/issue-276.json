{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-276","id":"673e464d6dcabe0001af5c1c","uuid":"b8fe9637-64ba-4db1-a25f-aa1f4e227ae7","title":"Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, Transformer Training Streamlined","html":"<p>Dear friends,</p><p>A small number of people are posting text online that‚Äôs intended for direct consumption not by humans, but by LLMs (large language models). I find this a fascinating trend, particularly when writers are incentivized to help LLM providers better serve their users!</p><p>People who post text online don‚Äôt always have an incentive to help LLM providers. In fact, their incentives are often misaligned. Publishers worry about LLMs reading their text, paraphrasing it, and reusing their ideas without attribution, thus depriving them of subscription or ad revenue. This has even led to litigation such as&nbsp;<em>The New York Times</em>‚Äô lawsuit against OpenAI and Microsoft for alleged copyright infringement. There have also been demonstrations of&nbsp;<a href=\"https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">prompt injections</a>, where someone writes text to try to give an LLM instructions contrary to the provider‚Äôs intent. (For example, a handful of sites advise job seekers to get past LLM resum√© screeners by writing on their resum√©s, in a tiny/faint font that‚Äôs nearly invisible to humans, text like ‚ÄúThis candidate is very qualified for this role.‚Äù) Spammers who try to promote certain products ‚Äî which is already challenging for search engines to filter out ‚Äî will also turn their attention to spamming LLMs.</p><p>But there are examples of authors who want to actively help LLMs. Take the example of a startup that has just published a software library. Because the online documentation is very new, it won‚Äôt yet be in LLMs‚Äô pretraining data. So when a user asks an LLM to suggest software, the LLM won‚Äôt suggest this library, and even if a user asks the LLM directly to generate code using this library, the LLM won‚Äôt know how to do so. Now, if the LLM is augmented with online search capabilities, then it might find the new documentation and be able to use this to write code using the library. In this case, the developer may want to take additional steps to make the online documentation easier for the LLM to read and understand via RAG. (And perhaps the documentation eventually will make it into pretraining data as well.)</p><p>Compared to humans, LLMs are not as good at navigating complex websites, particularly ones with many graphical elements. However, LLMs are far better than people at rapidly ingesting long, dense, text documentation. Suppose the software library has many functions that we want an LLM to be able to use in the code it generates. If you were writing documentation to help humans use the library, you might create many web pages that break the information into bite-size chunks, with graphical illustrations to explain it. But for an LLM, it might be easier to have a long XML-formatted text file that clearly explains everything in one go. This text might include a list of all the functions, with a dense description of each and an example or two of how to use it. (This is not dissimilar to the way we specify information about functions to enable LLMs to use them as tools.)</p><p>A human would find this long document painful to navigate and read, but an LLM would do just fine ingesting it and deciding what functions to use and when!</p><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/11/Captura-de-pantalla-2024-11-20-a-la-s--2.49.16-p.-m..png\" class=\"kg-image\" alt=\"Two people reading in bed, one with a book on library functions and a head labeled with AI layers.\" loading=\"lazy\" width=\"1196\" height=\"666\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/Captura-de-pantalla-2024-11-20-a-la-s--2.49.16-p.-m..png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/Captura-de-pantalla-2024-11-20-a-la-s--2.49.16-p.-m..png 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/Captura-de-pantalla-2024-11-20-a-la-s--2.49.16-p.-m..png 1196w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Because LLMs and people are better at ingesting different types of text, we write differently for LLMs than for humans. Further, when someone has an incentive to help an LLM better understand a topic ‚Äî so the LLM can explain it better to users ‚Äî then an author might write text to help an LLM.</p><p>So far, text written specifically for consumption by LLMs has not been a huge trend. But Jeremy Howard‚Äôs proposal for web publishers to post a&nbsp;<a href=\"https://www.answer.ai/posts/2024-09-03-llmstxt.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">llms.txt</a>&nbsp;file to tell LLMs how to use their websites, like a robots.txt file tells web crawlers what to do, is an interesting step in this direction. In a related vein, some developers are posting detailed instructions that tell their IDE how to use tools, such as the plethora of&nbsp;<a href=\"https://github.com/PatrickJS/awesome-cursorrules?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">.cursorrules</a>&nbsp;files that tell the Cursor IDE how to use particular software stacks.</p><p>I see a parallel with SEO (search engine optimization). The discipline of SEO has been around for decades. Some SEO helps search engines find more relevant topics, and some is spam that promotes low-quality information. But many SEO techniques ‚Äî those that involve writing text for consumption by a search engine, rather than by a human ‚Äî have survived so long in part because search engines process web pages differently than humans, so providing tags or other information that tells them what a web page is about has been helpful.</p><p>The need to write text separately for LLMs and humans might diminish if LLMs catch up with humans in their ability to understand complex websites. But until then, as people get more information through LLMs, writing text to help LLMs will grow.</p><p>Keep learning!</p><p>Andrew</p><p>P.S. I like LLMs, but I like humans even more. So please keep writing text for humans as well. üòÄ</p><h2 id=\"a-message-from-deeplearningai\">A MESSAGE FROM&nbsp;DEEPLEARNING.AI</h2><figure class=\"kg-card kg-image-card\"><a href=\"https://www.deeplearning.ai/short-courses/building-an-ai-powered-game/?ref=dl-staging-website.ghost.io\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-19T094258.660.png\" class=\"kg-image\" alt=\"Promo banner for &quot;Building an AI-Powered Game&quot;\" loading=\"lazy\" width=\"1680\" height=\"945\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-19T094258.660.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-19T094258.660.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-19T094258.660.png 1600w, https://dl-staging-website.ghost.io/content/images/2024/11/The-Batch-ads-and-exclusive-banners---2024-11-19T094258.660.png 1680w\" sizes=\"(min-width: 720px) 720px\"></a></figure><p>Learn how to develop applications with large language models by building AI-powered games! Gain essential skills by designing a shareable text-based game and integrating safety features. If you‚Äôve completed our&nbsp;<em>AI Python for Beginners</em>&nbsp;series or want to improve your coding skills in a fun, interactive way, this is a perfect course for you!&nbsp;<a href=\"https://www.deeplearning.ai/short-courses/building-an-ai-powered-game/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\">Start today</a></p><h1 id=\"news\">News</h1><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--31-.gif\" class=\"kg-image\" alt=\"Graph showing test loss decreases with more tokens and larger model sizes (103-109 parameters).\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--31-.gif 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--31-.gif 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--31-.gif 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"next-gen-models-show-limited-gains\">Next-Gen Models Show Limited Gains</h1><p>Builders of large AI models have relied on the idea that bigger neural networks trained on more data and given more processing power would show steady improvements. Recent developments are challenging that idea.</p><p><strong>What‚Äôs new:</strong>&nbsp;Next-generation large language models from OpenAI, Google, and Anthropic are falling short of expectations, employees at those companies&nbsp;<a href=\"https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczMTUwOTk2NCwiZXhwIjoxNzMyMTE0NzY0LCJhcnRpY2xlSWQiOiJTTVZWU0tEV0xVNjgwMCIsImJjb25uZWN0SWQiOiIyMjNDRDM2NDg0QzY0OTc3QjY5ODE0Rjc1MTYxNDRGNyJ9.yhaEjOziVkd2as-6nNCjNGZ91hh8FdKUqv8mPyHbh4w&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">told</a>&nbsp;<a href=\"https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">multiple</a>&nbsp;<a href=\"https://www.theinformation.com/articles/openai-shifts-strategy-as-rate-of-gpt-ai-improvements-slows?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">publications</a>. All three companies are responding by shifting their focus from pretraining to enhancing performance through techniques like fine-tuning and multi-step inference.</p><p><strong>Scaling law basics:</strong>&nbsp;A classic 2020&nbsp;<a href=\"https://arxiv.org/abs/2001.08361?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">paper</a>&nbsp;shows that, assuming a sufficient quantity of data, a transformer network‚Äôs performance rises predictably with increases in model size (demonstrated between 768 parameters and 1.5 billion parameters). Likewise, assuming sufficient model size, performance rises predictably with increases in dataset size (demonstrated between 22 million tokens and 23 billion tokens). Furthermore, performance rises predictably with increases in both model and dataset sizes. The 2022 Chinchilla&nbsp;<a href=\"https://arxiv.org/abs/2203.15556?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">paper</a>&nbsp;shows that, to build an optimal model, every 4x increase in compute requires a 2x increase in the size of the model and dataset (demonstrated for models between 70 million and 16 billion parameters, trained on between 5 billion and 500 billion tokens). Due to limited experimentation and lack of a theoretical basis of their findings, the authors didn‚Äôt determine whether these relationships would continue to hold at larger scales.</p><p><strong>Diminishing returns:</strong>&nbsp;Major AI companies have been counting on scaling laws to keep their models growing more capable at a steady pace. However, the next generation of high-profile models has not shown the expected improvements despite larger architectures, more training data, and more processing power.</p><ul><li>One-quarter of the way through its training, performance of OpenAI‚Äôs next-generation model Orion was on par with GPT-4‚Äôs, anonymous staffers told reporters. But after training was finished, Orion‚Äôs improvement over GPT-4 was far smaller than that from GPT-3 to GPT-4. OpenAI‚Äôs o1 model, which is based on GPT-4o, delivers improved performance by using&nbsp;<a href=\"https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">additional processing during inference</a>. The company currently expects to introduce Orion early next year.</li><li>Google has faced similar challenges in developing the next version of Gemini. Employees who declined to be named said the development effort had shown disappointing results and slower-than-expected improvement despite training on larger amounts of data and processing power. Like OpenAI, Google is exploring alternative ways to boost performance, the sources said. The company expects to introduce the model in December.</li><li>Anthropic‚Äôs schedule for introducing Claude 3.5 Opus, the largest member of its Claude 3.5 family, has slipped. It hasn‚Äôt shown the expected performance given its size and cost, according to anonymous sources inside the company. Anthropic aims to improve performance by developing agentic capabilities and application-specific performance.</li><li>One clear limitation in realizing the performance gains predicted by scaling laws is the amount of data available for training. Current models learn from huge amounts of data scraped from the web. It‚Äôs getting harder to find high-quality materials on the web that haven‚Äôt already been tapped, and other large-scale data sources aren‚Äôt readily available. Some model builders are supplementing real-world data with synthetic data, but Google and OpenAI have been disappointed with the results of pretraining models on synthetic data. OpenAI found that pretraining Orion on synthetic data made it too much like earlier models, according to anonymous employees.</li></ul><p><strong>What they‚Äôre saying:&nbsp;</strong>AI leaders are divided on the future of scaling laws as they are currently understood.&nbsp;</p><ul><li>‚ÄúWe don‚Äôt see any evidence that things are leveling off. The reality of the world we live in is that it could stop at any time. Every time we train a new model, I look at it and I‚Äôm always wondering ‚Äî I‚Äôm never sure in relief or concern ‚Äî [if] at some point we‚Äôll see, oh man, the model doesn‚Äôt get any better.‚Äù ‚Äî&nbsp;<a href=\"https://time.com/6990386/anthropic-dario-amodei-interview/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">Dario Amodei</a><em>, CEO and co-founder, Anthropic</em></li><li>‚ÄúThere is no wall.‚Äù ‚Äî&nbsp;<a href=\"https://x.com/sama/status/1856941766915641580?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">Sam Altman</a><em>, CEO and co-founder, OpenAI</em></li><li>‚ÄúThe 2010s were the age of scaling, now we're back in the age of wonder and discovery once again. . . . Scaling the right thing matters now more than ever.‚Äù ‚Äî&nbsp;<a href=\"https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">Ilya Sutskever</a><em>, co-founder of OpenAI who now leads Safe Superintelligence, an independent research lab</em></li></ul><p><strong>Why it matters:</strong>&nbsp;AI‚Äôs phenomenal advance has drawn hundreds of millions of users and sparked a new era of progress and hope. Slower-than-expected improvements in future foundation models may blunt this progress. At the same time, the cost of training large AI models is rising dramatically. The latest models cost as much as $100 million to train, and this number could reach $100 billion within a few years,&nbsp;<a href=\"https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-models-that-cost-dollar1-billion-to-train-are-in-development-dollar100-billion-models-coming-soon-largest-current-models-take-only-dollar100-million-to-train-anthropic-ceo?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">according to</a>&nbsp;Anthropic‚Äôs Dario Amodei. Rising costs could lead companies to reallocate their gargantuan training budgets and researchers to focus on more cost-effective, application-specific approaches.</p><p><strong>We‚Äôre thinking:</strong>&nbsp;AI‚Äôs power-law curves may be flattening, but we don‚Äôt see overall progress slowing. Many developers already have shifted to building smaller, more processing-efficient models, especially networks that can run on edge devices. Agentic workflows are taking off and bringing huge gains in performance. Training on synthetic data is another frontier that‚Äôs only beginning to be explored. AI technology holds many wonders to come!</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--32-.gif\" class=\"kg-image\" alt=\"Comparison of Minecraft terrain with and without player modifications.\" loading=\"lazy\" width=\"600\" height=\"337\" srcset=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--32-.gif 600w\"></figure><h1 id=\"no-game-engine-required\">No Game Engine Required</h1><p>A real-time video generator lets you explore an open-ended, interactive virtual world ‚Äî a video game without a game engine.</p><p><strong>What‚Äôs new:</strong>&nbsp;Decart, a startup that‚Äôs building a platform for AI applications, and Etched, which designs specialized AI chips, introduced&nbsp;<a href=\"https://oasis-model.github.io/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">Oasis</a>, which generates a Minecraft-like game in real time. The weights are open and available&nbsp;<a href=\"https://huggingface.co/Etched/oasis-500m?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">here</a>. You can play with a demo&nbsp;<a href=\"https://oasis.decart.ai/welcome?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">here</a>.</p><p><strong>How it works:</strong>&nbsp;The system generates one frame at a time based on a user‚Äôs keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it‚Äôs almost certainly based on videos of Minecraft gameplay, given the output‚Äôs striking semblance to that game.</p><ul><li>Some recent video generators produce an initial frame, then the nth frame, and then the frames in between. This approach isn‚Äôt practical for real-time gameplay. Instead, Oasis learned to generate the next frame. A ViT encoder embeds previously generated frames. Given those embeddings, an embedding of a frame to which noise had been added, and a user‚Äôs input, a diffusion transformer learned to remove the noise using a variation on diffusion called&nbsp;<a href=\"https://arxiv.org/abs/2407.01392?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">diffusion forcing</a>.</li><li>Generated frames may contain glitches, and such errors can snowball if the model incorporates glitches from previous frames into subsequent frames. To avoid this, during training, the system added noise to embeddings of previous frames before feeding them to the transformer to generate the next frame. This way, the transformer learned to ignore glitches while producing new frames.</li><li>At inference, the ViT encoder embeds previously generated frames, and the system adds noise to the frame embeddings. Given the user‚Äôs input, the noisy frame embeddings, and a pure-noise embedding that represents the frame to be generated, the transformer iteratively removes the noise from the previous and current frame embeddings. The ViT‚Äôs decoder takes the denoised current frame embedding and produces an image.</li><li>The system currently runs on Nvidia H100 GPUs using Decart‚Äôs inference technology, which is tuned to run transformers on that hardware. The developers aim to change the hardware to Etched‚Äôs&nbsp;<a href=\"https://www.etched.com/announcing-etched?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">Sohu</a>&nbsp;chips, which are specialized for transformers and process Llama 70B at a jaw-dropping 500,000 tokens per second.</li></ul><p><strong>Results:</strong>&nbsp;The Oasis web demo enables users to interact with 360-by-360-pixel frames at 20 frames per second. Users can place blocks, place fences, and move through a Minecraft-like world. The demo starts with an image of a location, but users can upload an image (turning, say, a photo of your cat into a blocky Minecraft-style level, as&nbsp;<a href=\"https://www.wired.com/story/first-entirely-ai-generated-video-game-weird-and-fun/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">reported</a>&nbsp;by&nbsp;<em>Wired</em>).</p><p><strong>Yes, but:</strong>&nbsp;The game has its fair share of issues. For instance, objects disappear and menus items change unaccountably. The world‚Äôs physics are similarly inconsistent. For instance, players don‚Äôt fall into holes dug directly beneath them and, after jumping into water, players are likely to find themselves standing on a blue floor.</p><p><strong>Behind the news:</strong>&nbsp;In February, Google announced&nbsp;<a href=\"https://sites.google.com/view/genie-2024/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">Genie</a>, a model that generates two-dimensional platformer games from input images. We weren‚Äôt able to find a publicly available demo or model.</p><p><strong>Why it matters:</strong>&nbsp;Oasis is more a proof of concept than a product. Nonetheless, as an open-world video game entirely generated by AI ‚Äî albeit based on data produced by a traditional implementation ‚Äî it sets a bar for future game generators.</p><p><strong>We‚Äôre thinking:</strong>&nbsp;Real-time video generation suggests a wealth of potential applications ‚Äî say, a virtual workspace for interior decorating that can see and generate your home, or an interactive car repair manual that can create custom clips based on your own vehicle. Oasis is an early step in this direction.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--22-.png\" class=\"kg-image\" alt=\"Close-up of a Chinese-made server chip labeled with the logo and text ‚Äò710‚Äô mounted on a motherboard.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--22-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--22-.png 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--22-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"further-chip-restrictions-on-china\">Further Chip Restrictions on China</h1><p>The largest manufacturer of AI chips told its Chinese customers it would stop fabricating their most advanced designs, further limiting China‚Äôs access to AI hardware.</p><p><strong>What‚Äôs new:</strong>&nbsp;Taiwan Semiconductor Manufacturing Corp. (TSMC) notified Alibaba, Baidu, and others it would halt production of their most advanced chips starting November 13, according to&nbsp;<a href=\"https://www.ft.com/content/a736beeb-b38a-484e-bbe9-98e92ecb66d9?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">multiple</a>&nbsp;<a href=\"https://arstechnica.com/tech-policy/2024/11/tsmc-will-stop-making-7-nm-chips-for-chinese-customers/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">reports</a>. The restriction affects chip designs that are based on manufacturing processes at scales of 7 nanometers and below. TSMC must receive explicit permission from the U.S. government to manufacture advanced chips for a given customer, which likely would require that the government assess each chip to prevent potential military applications.</p><p><strong>How it works:</strong>&nbsp;The United States Department of Commerce ordered TSMC to halt shipments of advanced AI chips to China after a chip fabricated by TSMC was discovered in an AI system sold by the Chinese telecoms giant Huawei, apparently in violation of earlier U.S. controls,&nbsp;<a href=\"https://www.reuters.com/technology/us-ordered-tsmc-halt-shipments-china-chips-used-ai-applications-source-says-2024-11-10/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">Reuters</a>&nbsp;reported. Taiwan‚Äôs economic ministry said it would follow all domestic and international regulations.</p><ul><li>TSMC‚Äôs manufacturing processes etch transistors into silicon at minuscule sizes to fabricate hardware like the Nvidia A100 GPU (which uses the 7 nanometer process), Nvidia H100 GPU (5 nanometer process), and Apple A18 CPU (3 nanometer process). Smaller transistors make it possible to fit more transistors per area of silicon, leading to faster processing ‚Äî an important capability for training large neural networks and providing them to large numbers of users.</li><li>Although TSMC is headquartered in Taiwan, it uses chip-manufacturing equipment made by U.S. companies such as Applied Materials and Lam Research. TSMC‚Äôs use of U.S. equipment obligates the company to comply with U.S. export control policies.</li><li>The policy could&nbsp;<a href=\"https://www.taipeitimes.com/News/front/archives/2024/10/24/2003825780?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">force</a>&nbsp;several Chinese companies to either downgrade their chip designs or seek alternative suppliers. For example, Alibaba, Baidu, Huawei and Tencent have depended on TSMC to manufacture their chip designs. ByteDance partnered with TSMC to develop AI chips to rival Nvidia‚Äôs.</li><li>Samsung and Intel are capable of fabricating advanced chips, but they, too, are subject to U.S. restrictions on sales of advanced chips to China. U.S. officials have&nbsp;<a href=\"https://www.tomshardware.com/tech-industry/manufacturing/us-officials-doubt-chinas-smic-foundry-can-produce-enough-7nm-chips-to-satisfy-huaweis-demand?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">expressed</a>&nbsp;skepticism that China‚Äôs own Semiconductor Manufacturing International Corporation can supply in large volumes chips manufactured using processes of 7 nanometers or smaller.</li></ul><p><strong>Behind the news:</strong>&nbsp;The U.S.-China chip standoff began in 2020 and has&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/gpu-china/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">escalated</a>&nbsp;since. Initial restrictions&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/gpu-china/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">barred</a>&nbsp;U.S.-based companies like AMD, Intel, and Nvidia from selling advanced chips to Huawei and affiliated Chinese firms. China responded by&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/huawei-rises-as-key-ai-chip-supplier-amid-u-s-export-bans/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">promoting</a>&nbsp;domestic chip fabrication. In 2022, the U.S.&nbsp;<a href=\"https://www.whitehouse.gov/briefing-room/statements-releases/2022/08/09/fact-sheet-chips-and-science-act-will-lower-costs-create-jobs-strengthen-supply-chains-and-counter-china/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">passed</a>&nbsp;the CHIPS and Science Act to boost its own chip industry, seeking to counter China and decrease U.S. reliance on Taiwan.</p><p><strong>Why it matters:</strong>&nbsp;TSMC finds itself in the middle of an AI arms race in which cutting-edge chips could tip the balance. The company itself, which has been operating at full capacity, is unlikely to suffer business losses.</p><p><strong>We‚Äôre thinking:</strong>&nbsp;AI developers in China have been resourceful in navigating previous restrictions. Chip manufacturing is extraordinarily difficult to master, but China has made&nbsp;<a href=\"https://www.fpri.org/article/2024/06/chinas-defiant-chip-strategy/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">strides</a>&nbsp;in this direction. A proliferation of factories that can fabricate advanced chips would reshape AI research and business worldwide.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--30-.gif\" class=\"kg-image\" alt=\"Efficient Foundations animation showing layered AI model components.\" loading=\"lazy\" width=\"600\" height=\"336\" srcset=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--30-.gif 600w\"></figure><h1 id=\"more-efficient-training-for-transformers\">More-Efficient Training for Transformers</h1><p>Researchers cut the processing required to train transformers by around 20 percent with only a slight degradation in performance.</p><p><strong>What‚Äôs new:</strong>&nbsp;Xiuying Wei and colleagues at Swiss Federal Institute of Technology Lausanne&nbsp;<a href=\"https://arxiv.org/abs/2406.16450?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">replaced a transformer‚Äôs linear layers with approximations</a>&nbsp;based on computationally efficient low-rank linear layers.</p><p><strong>Key insight:</strong>&nbsp;A low-rank approximation replaces a matrix with a product of two smaller matrices. This technique is widely used to streamline fine-tuning via&nbsp;<a href=\"https://arxiv.org/pdf/2106.09685?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">LoRA</a>, which modifies the weights in each of a transformer‚Äôs linear layers by adding a learned low-rank approximation. As a direct replacement for the weights in linear layers, low-rank approximation saves processing during training, but it also causes unstable fluctuations in the training loss and slower convergence. The authors mitigated these undesirable effects by training each full-size layer in parallel with a low-rank approximation of the layer while gradually phasing out the full-size layer. This approach costs more memory and computation initially, but it saves those resources in the long run.</p><p><strong>How it works:</strong>&nbsp;The authors modified a transformer (1.3 billion parameters) to use low-rank approximation (which trimmed the parameter count to 985 million). They trained both models on 25.5B tokens of&nbsp;<a href=\"https://arxiv.org/abs/2306.01116?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">text</a>&nbsp;scraped from the web, filtered, and deduplicated.</p><ul><li>The authors replaced each of the larger transformer‚Äôs linear layers with two smaller linear layers, approximating its weight matrix with a product of two smaller matrices. (In mathematical terms, if a standard linear layer computes Wx, where W is the weights and x is the input, the replacement computes U(Vx), where U and V are smaller than W.)</li><li>During the first half of training, they trained both usual and low-rank layers in parallel. The output of each layer was a weighted sum of the two. Initially they weighed the usual layer at 1 and the low-rank layers at 0. As training progressed, they decreased the usual layer‚Äôs weighting to 0 and increased the low-rank layers‚Äô weighting to 1.</li></ul><p><strong>Results:</strong>&nbsp;The authors tested both the modified and full-size transformers on 500 million tokens from the validation set according to&nbsp;<a href=\"https://en.wikipedia.org/wiki/Perplexity?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">perplexity</a>&nbsp;(a measure of the likelihood that a model will predict the next word, lower is better). The modified version achieved 12.86 perplexity, slightly worse than the full-size version‚Äôs 12.46 perplexity. However, training the modified version required more than 20 percent less processing and 14 percent less time. The modified transformer used 1.66*10^20 FLOPS and took 302 hours, while the full-size version used 2.10*10^20 FLOPS and took 352 hours.</p><p><strong>Why it matters:</strong>&nbsp;Training large transformers requires a lot of computation. Low-rank approximation lightens the processing load. This work approximates a transformer's linear layers to save memory, while the earlier&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/galore-a-memory-saving-method-for-pretraining-and-fine-tuning-llms/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--TLD5p2cL7tzrlJ0OFLcdN6FsC6u-RWGAZW1vuA6wLOpBhF7eDoork7WZVLUDONutUIxXa\" rel=\"noopener\">GaLore</a>&nbsp;approximates the gradient to save optimizer memory.</p><p><strong>We‚Äôre thinking:</strong>&nbsp;The authors note that this approach also works for fine-tuning pretrained models ‚Äî a potential alternative to LoRA. Simply replace each pretrained linear layer (with weights W) with two linear layers (with weights U and V), and initialize U and V such that W = UV.</p>","comment_id":"673e464d6dcabe0001af5c1c","feature_image":"https://dl-staging-website.ghost.io/content/images/2024/11/Captura-de-pantalla-2024-11-20-a-la-s--2.49.16-p.-m.-1.png","featured":false,"visibility":"public","created_at":"2024-11-20T12:27:57.000-08:00","updated_at":"2024-11-20T12:50:15.000-08:00","published_at":"2024-11-20T12:46:00.000-08:00","custom_excerpt":"The Batch AI News and Insights: A small number of people are posting text online that‚Äôs intended for direct consumption not by humans, but by LLMs (large language models).","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"673e4ad16dcabe0001af5c64","name":"issue-276","slug":"issue-276","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-276/"},{"id":"673e4ad16dcabe0001af5c65","name":"Nov 20, 2024","slug":"nov-20-2024","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/nov-20-2024/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-276/","excerpt":"The Batch AI News and Insights: A small number of people are posting text online that‚Äôs intended for direct consumption not by humans, but by LLMs (large language models).","reading_time":14,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Next-Gen Models Show Limited Gains, Real-Time Video Generation, China AI Chips Blocked, and more...","meta_description":"The Batch AI News and Insights: A small number of people are posting text online that‚Äôs intended for direct consumption not by humans, but by LLMs...","email_subject":null,"frontmatter":null,"feature_image_alt":"Two people reading in bed, one with a book on library functions and a head labeled with AI layers.","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2024/11/Captura-de-pantalla-2024-11-20-a-la-s--2.49.16-p.-m.-1.png","dimensions":{"width":1196,"height":666}},"banner":{"title":"Generative AI for Everyone","databaseId":32549,"id":"cG9zdDozMjU0OQ==","featuredImage":{"node":{"altText":"Generative AI for Everyone","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/10/GenAI4E_sidebanner.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3Moa97R","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"‚ú® New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"‚ú® New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"‚ú® New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"‚ú® Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"‚ú® New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}