{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-294","id":"67e55e48abd8cc0001702b77","uuid":"07dc8dde-c335-4fb5-9103-d0228c5fd9ad","title":"Compact Vision-Language with Open Weights, Faster Learning, Diffusion in Few Steps, LLMs Aid Tutors","html":"\n<!--kg-card-begin: html-->\n<div id=\"elevenlabs-audionative-widget\" data-height=\"90\" data-width=\"100%\" data-frameborder=\"no\" data-scrolling=\"no\" data-publicuserid=\"e20b5cfed36900db239c005920538f20ce435963e95a0a4106d34bdd6bf0e46d\" data-playerurl=\"https://elevenlabs.io/player/index.html\" >Loading the <a href=\"https://elevenlabs.io/text-to-speech?ref=dl-staging-website.ghost.io\" target=\"_blank\" rel=\"noopener\">Elevenlabs Text to Speech</a> AudioNative Player...</div>\n<!--kg-card-end: html-->\n<p>Dear friends,</p><p>Fine-tuning small language models has been gaining traction over the past half year. I’d like to share my sense of when to use this technique, and also when not to, based on what I’m seeing in multiple companies.</p><p>First, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writing&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/from-prompts-to-mega-prompts/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">mega prompts</a>), few-shot prompting, or simple agentic workflows.</p><p>Why shouldn’t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task.</p><p>Having said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I’ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully:</p><p><strong>Improving accuracy of critical applications.</strong>&nbsp;Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims.</p><p><strong>Learning a particular &nbsp;style of communication.</strong>&nbsp;As I explain in “<a href=\"https://www.deeplearning.ai/courses/generative-ai-for-everyone/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Generative AI for Everyone</a>,” my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.jpg\" class=\"kg-image\" alt=\"Cartoon of a man playing violin saying “I’m fine-tuning!” while a woman at her desk covers her ears, replying “Did you try prompting?”\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--56-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--56-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56-.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>Reducing latency or cost during scale-ups.&nbsp;</strong>I’ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task.</p><p>At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which &nbsp;I think RAG would work better.</p><p>Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal.</p><p>It is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated.</p><p>In conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it’s a critical piece of a small minority of them.</p><p>Keep learning!</p><p>Andrew</p><hr><h2 id=\"a-message-from-deeplearningai\">A MESSAGE FROM&nbsp;DEEPLEARNING.AI</h2><figure class=\"kg-card kg-image-card\"><a href=\"https://www.deeplearning.ai/short-courses/vibe-coding-101-with-replit/?ref=dl-staging-website.ghost.io\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png\" class=\"kg-image\" alt=\"Promo banner for: &quot;Vibe Coding 101 with Replit&quot;\" loading=\"lazy\" width=\"2000\" height=\"1043\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png 1600w, https://dl-staging-website.ghost.io/content/images/size/w2400/2025/03/V3_DeepLearning_Replit_Banner_2070x1080-01.png 2400w\" sizes=\"(min-width: 720px) 720px\"></a></figure><p>In “Vibe Coding 101 with Replit,” you’ll learn to plan, prompt, and debug alongside a coding agent. Build, host, and share two real web apps in Replit’s cloud environment while developing effective development skills like writing product requirements, structuring tasks, and refining AI-generated code.&nbsp;<a href=\"https://www.deeplearning.ai/short-courses/vibe-coding-101-with-replit/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\">Start today</a></p><h1 id=\"news\">News</h1><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--67-.png\" class=\"kg-image\" alt=\"Comparison table of Gemini and Gemma models across benchmarks like MMLU, MATH, and GPQA with radar charts.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--67-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--67-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--67-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"vision-language-compact-and-open\">Vision-Language, Compact and Open</h1><p>Google updated its open-weights family of large language models to include versions that handle image and video inputs.</p><p><strong>What’s new:</strong>&nbsp;Google released its&nbsp;<a href=\"https://developers.googleblog.com/en/introducing-gemma3/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Gemma 3</a>&nbsp;multilingual large language models with parameter counts of 1 billion, 4 billion, 12 billion, and 27 billion. While the smallest processes text only, the other three are vision-language models that are small enough to run on a consumer hardware.</p><ul><li><strong>Input/output:</strong>&nbsp;Gemma 3 1B: text-in (up to 32,000 tokens), text out (up to 8,192 tokens). Gemma 3 4B, 7B, 27B: text, images/video in (up to 128,000 tokens), text out (up to 8,192 tokens). Gemma 3 27B&nbsp;<a href=\"https://artificialanalysis.ai/models/gemma-3-27b/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">outputs</a>&nbsp;24.61 tokens per /second, 0.68 seconds to first token.</li><li><strong>Knowledge cutoff:</strong>&nbsp;March 2024</li><li><strong>Architecture:</strong>&nbsp;Gemma 3 1B: Transformer. Gemma 3 4B, 12B, 27B: Transformer, SigLIP&nbsp; vision encoder.</li><li><strong>Features:</strong>&nbsp;140 languages, function calling, structured output.</li><li><strong>Training data:</strong>&nbsp;Gemma 3 1B: 2 trillion tokens of web text, code, and mathematics. Gemma 3 4B, 12B, 27B: between 4 trillion and 14 trillion tokens of text and images.</li><li><strong>Availability/price:</strong>&nbsp;Weights free to download from&nbsp;<a href=\"https://huggingface.co/blog/gemma3?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Hugging Face</a>&nbsp;and Kaggle under a&nbsp;<a href=\"https://ai.google.dev/gemma/terms?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">license</a>&nbsp;that allows noncommercial and commercial uses with some restrictions. Available free via Google’s AI Studio.</li></ul><p><strong>How it works:</strong>&nbsp;Gemma 3&nbsp;<a href=\"https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">rearchitects</a>&nbsp;and refines earlier Gemma models for higher performance at lower parameter counts.</p><ul><li>To save memory, Gemma 3 interleaves five local attention layers for every global attention layer. Global attention layers attend to the entire input, while local attention layers attend to 1,024 tokens.</li><li>The models were fine-tuned to encourage their outputs to match those of an unspecified larger teacher model.</li><li>Gemma 3 learned via reinforcement learning in three ways. (i) The models were aligned with human preferences via&nbsp;<a href=\"https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">reinforcement learning from human feedback</a>&nbsp;(RLHF). (ii) They were fine-tuned to solve math problems via reinforcement learning, much like&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/deepseek-r1-an-affordable-rival-to-openais-o1/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">DeepSeek-R1</a>. (iii) They were trained to generate better code via&nbsp;<a href=\"https://arxiv.org/abs/2410.02089?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">reinforcement learning from execution feedback (RLEF)</a>. Specifically, over several rounds of output, RLEF tested generated code on a subset of tests, then prompted the model to fix any bugs. RLEF rewarded the models if their final output passed all tests.</li></ul><p><strong>Performance:</strong>&nbsp;Gemma 3 models outperform Gemma 2 models of equal or larger size by several measures, and all sizes show a strong ability to solve mathematics word problems as measured by&nbsp;<a href=\"https://paperswithcode.com/sota/math-word-problem-solving-on-math?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">MATH</a>.&nbsp;</p><ul><li>In Google’s tests, Gemma 3 1B performs roughly comparably to Gemma 2 2B, outperforming the larger model on LiveCodeBench (1.9 percent to 1.2 percent) and MATH (48.0 percent to 27.2 percent).&nbsp;</li><li>Gemma 3 4B achieves roughly comparable performance to Gemma 2 9B, Llama 3.1 8B, and Qwen2.5-7B. It’s slightly behind Microsoft Phi-4 Mini (also 4 billion parameters), except on MATH, according to that company’s tests.</li><li>Gemma 3 12B improves on Gemma 2 27B and compares to Gemini 1.5 Flash (in TIGER-Lab’s tests) and Anthropic Claude 3.5 Haiku (in that developer’s tests). It outperforms the larger, proprietary models on MATH.&nbsp;</li><li>Gemma 3 27B consistently outperforms the Gemma 2 model of the same size&nbsp;and performs comparably to Gemini 1.5 Pro on&nbsp;<a href=\"https://arxiv.org/abs/2406.01574?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">MMLU-Pro</a>&nbsp;(high-level language comprehension) 67.5 percent to 56.9 percent, on&nbsp;<a href=\"https://arxiv.org/abs/2403.07974?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">LiveCodeBench</a>&nbsp;(coding) 29.7 percent to 20.4 percent, on&nbsp;<a href=\"https://arxiv.org/abs/2311.12022?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">GPQA Diamond</a>&nbsp;(graduate-level domain knowledge) 42.4 percent to 34.3 percent, and on MATH 89.0 percent to 55.6 percent.</li><li>Moreover, Gemma 3 27B achieves 1,338 ELO in&nbsp;<a href=\"https://lmarena.ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Chatbot Arena</a>, a top-ten score that puts it ahead of OpenAI o1 and behind only DeepSeek-R1 among models with open weights.</li></ul><p><strong>Hot on Gemma 3’s heels:</strong>&nbsp;Shortly after Gemma 3 became available, Mistral released&nbsp;<a href=\"https://mistral.ai/news/mistral-small-3-1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Small 3.1</a>&nbsp;(24 billion parameters), a vision-language model with open weights, under a more permissive Apache 2.0 license.&nbsp;</p><ul><li>Mistral Small 3.1 is similarly multilingual and offers a 128,000 token context window.</li><li>It slightly outperforms Gemma 3 27B on MMLU, MMLU-Pro, MMMU, and other selected benchmarks.</li><li>It also outperforms Gemma 3 27B and other models in its size range on long-context tests. (However, Gemma 3 27B performs better in the Chatbot Arena test of human preference.)&nbsp;</li></ul><p><strong>Why it matters:</strong>&nbsp;Gemma 3 takes advantage of a variety of techniques to raise the bar for vision-language performance in relatively small models. Knowledge distillation, multiple rounds of reinforcement learning, and fine-tuning on many languages are a powerful combination.</p><p><strong>We’re thinking:</strong>&nbsp;A vision-language model small enough to run on a smartphone feels increasingly close!</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--68-.png\" class=\"kg-image\" alt=\"Diagram comparing diffusion, flow matching, and shortcut models for image generation with fewer steps.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--68-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--68-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--68-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"better-images-in-fewer-steps\">Better Images in Fewer Steps</h1><p>Diffusion models usually take many noise-removal steps to produce an image, which takes time at inference. There are ways to reduce the number of steps, but the resulting systems are less effective. Researchers devised a streamlined approach that doesn’t sacrifice output quality.</p><p><strong>What’s new:</strong>&nbsp;Kevin Frans and colleagues at UC Berkeley introduced&nbsp;<a href=\"https://arxiv.org/abs/2410.12557?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">shortcut models</a>&nbsp;that learn to take larger noise-removal steps and thus require fewer steps to generate an image.</p><p><strong>Key insight:</strong>&nbsp;At inference, a scheduler like&nbsp;<a href=\"https://arxiv.org/abs/2206.00364?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Euler</a>&nbsp;can enable a model to take larger steps than those it learned during training, but this approach yields&nbsp;<a href=\"https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">worse performance</a>. Alternatively distillation, in which a student model learns to remove the same amount of noise as a teacher model when it takes several steps, offers improved performance at the cost of more cumbersome development. Training the model directly to take bigger steps — that are equivalent to multiple smaller steps — enables it to maintain high performance while taking fewer steps.</p><p><strong>How it works:</strong>&nbsp;The authors trained&nbsp;<a href=\"https://arxiv.org/abs/2212.09748?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">DiT-B</a>, a diffusion transformer, to generate images like those in CelebA-HQ (celebrity faces) and ImageNet-256 (various subjects, size 256x256).</p><ul><li>The loss function included terms for flow matching and self-consistency. The flow matching term encouraged the model to learn to remove noise. The self-consistency term encouraged the model to learn how to minimize the discrepancy between the noise removed by a single big step and two smaller steps.</li><li>Initially the model learned to combine two small steps into one step 2x as large. Combining two larger steps resulted in step sizes of 4x, 8x, and so on, up to 128x.</li><li>At inference, the user told the model how many small steps to take, and the model computed the single-step size necessary to accomplish that.</li></ul><p><strong>Results:</strong>&nbsp;The authors compared their model using 1, 4, or 128 steps to alternatives that were trained via various methods including many variants of distillation. They measured the results using&nbsp;<a href=\"https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Fréchet inception distance</a>&nbsp;(FID), which assesses how closely generated images resemble real-world images (lower is better).</p><ul><li>On both CelebA-HQ and ImageNet-256, their model, when it took four steps, achieved the best performance. For example, on CelebA-HQ, using four steps, the shortcut model achieved 13.8 FID, while the next-best model,&nbsp;<a href=\"https://arxiv.org/abs/2209.03003?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Reflow</a>&nbsp;(another variant of distillation), achieved 18.4 FID.</li><li>When it took one step, it achieved the second-best result, behind&nbsp;<a href=\"https://arxiv.org/abs/2202.00512?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">progressive distillation</a>, which trained a series of student models to remove the same amount of noise as a teacher model does when it takes multiple steps.</li></ul><p><strong>Why it matters:&nbsp;</strong>Generating images by diffusion is typically costly, and previous approaches to cutting the cost have compromised either performance or incurred additional development expense or both. This method achieves high performance at relatively low cost.</p><p><strong>We’re thinking:</strong>&nbsp;As diffusion models continue to become cheaper and faster, we expect to see applications blossom!</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--69-.png\" class=\"kg-image\" alt=\"AI tutoring system interface showing real-time context integration, privacy, and expert-like feedback generation.\" loading=\"lazy\" width=\"1200\" height=\"674\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--69-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--69-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--69-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"llm-support-for-tutors\">LLM Support for Tutors</h1><p>Students benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors’ effectiveness in real time.</p><p><strong>What’s new:</strong>&nbsp;Rose Wang and colleagues at Stanford built&nbsp;<a href=\"https://arxiv.org/abs/2410.03017?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Tutor CoPilot</a>, a tool for remote, online tutors that uses GPT-4 to generate hints, explanations, questions, and other helpful responses to students.</p><p><strong>Key insight:</strong>&nbsp;When a student makes an error, according to previous&nbsp;<a href=\"https://aclanthology.org/2024.naacl-long.120.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">work</a>&nbsp;by some of the same authors, effective teachers choose a strategy for addressing the mistake. The authors identified 11 strategies, such as ask a question, explain a concept, provide a hint, or encourage the student. Moreover, they found that an LLM that executed a strategy chosen by an expert teacher performed significantly better than an LLM that was prompted with a strategy chosen at random or no specific strategy. Letting inexperienced tutors choose a strategy while an LLM generates a response helps them learn how to execute the strategy. Students, in turn, benefit from responses that mimic those of an experienced teacher.</p><p><strong>How it works:</strong>&nbsp;The authors outfitted a remote tutoring application with GPT-4.</p><ul><li>The application included a tutor-student chat window, a problem display, and a whiteboard. The authors added a button that enabled the tutor to turn Tutor CoPilot on or off.&nbsp;</li><li>When a tutor engaged Tutor CoPilot, the system prompted GPT-4 to behave as an experienced elementary math teacher and provided context in the form of the 10 most recent messages, the current lesson topic, and a default strategy from the list. GPT-4 responded with guidance. (To preserve the tutor’s and student’s privacy, the system redacted their names using the open source library&nbsp;<a href=\"https://arxiv.org/abs/2402.05111?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Edu-ConvoKit</a>.)</li><li>The system prompted GPT-4 three times, each time changing the strategy, and presented the tutor with three potential responses.</li><li>The tutor could re-generate or edit GPT-4’s responses, or select a strategy and generate a new response before adding it to the chat window.</li></ul><p><strong>Results:&nbsp;</strong>The authors partnered with a virtual tutoring company and a school district in the United States for a two-month study of 874 tutors and 1,787 students between grades 3 and 8. They divided the participants into two groups. In one group, tutors conducted sessions with students as usual. In the other, tutors had access to Tutor CoPilot. The authors measured success by the percentage of students who passed a test at the end of a lesson.&nbsp;</p><ul><li>In the group that didn’t use Tutor CoPilot, 62 percent of students passed the test.</li><li>In the group with TutorCopilot, 66 percent passed.&nbsp;</li><li>The effect was most pronounced among the one-third of tutors who had the lowest ratings (9 percent higher) and least experience (7 percent higher). &nbsp;</li><li>The API cost was approximately $3.31 per tutor, or roughly $20 per tutor per year.</li></ul><p><strong>Yes, but:</strong>&nbsp;The authors found statistically significant improvements as measured by test results per lesson, but not in end-of-year exam results. The study’s two-month duration may account for the lack of evidence for longer-term effects.</p><p><strong>Why it matters:</strong>&nbsp;LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM’s general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination.</p><p><strong>We’re thinking:</strong>&nbsp;Although it relies on sophisticated technology, the authors’ approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.gif\" class=\"kg-image\" alt=\"Visual model aligning diffusion embeddings with DINOv2 encoders using REPA and DiT/SiT blocks.\" loading=\"lazy\" width=\"600\" height=\"336\" srcset=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.gif 600w\"></figure><h1 id=\"faster-learning-for-diffusion-models\">Faster Learning for Diffusion Models</h1><p>Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2.</p><p><strong>What’s new:</strong>&nbsp;Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposed&nbsp;<a href=\"https://arxiv.org/abs/2410.06940?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Representation Alignment</a>&nbsp;(REPA), a loss term for transformer-based diffusion.</p><p><strong>Key insight:</strong>&nbsp;Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn’t need to learn how to embed an image from scratch.</p><p><strong>How it works:</strong>&nbsp;The authors modified&nbsp;<a href=\"https://arxiv.org/pdf/2212.09748?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">DiT-XL/2</a>&nbsp;and&nbsp;<a href=\"https://arxiv.org/abs/2401.08740?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">SiT-XL/2</a>&nbsp;transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrained&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/multitask-vision-transformer/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">DINOv2</a>.</p><ul><li>The authors used&nbsp;<a href=\"https://arxiv.org/abs/2112.10752?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Stable Diffusion VAE’s</a>&nbsp;pretrained encoder to embed an image.</li><li>Given the embedding with noise added, the diffusion model learned to remove the noise according to the usual loss term.</li><li>It also learned according to the REPA loss. Specifically, it learned to maximize the cosine similarity between a specially processed version of its eighth-layer embedding and the embedding produced by a pretrained DINOv2. To process its eighth-layer embedding for the REPA loss, the diffusion model fed the embedding to a vanilla neural network.</li><li>At inference, given pure noise, the model removed it over several steps to produce an image embedding. Stable Diffusion VAE’s decoder converted the embedding into an image.</li></ul><p><strong>Results:</strong>&nbsp;The modified DiT-XL/2 learned significantly faster than the unmodified version.</p><ul><li>In 400,000 training steps, the modified model reached 12.3&nbsp;<a href=\"https://papers.nips.cc/paper_files/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9y2OPZmDPM8PgEFYb6bhb5cNMCCKhkdX7Wl9uZmgAeyu2O0jQbxa3F9xaorJPK6_obk1L7\" rel=\"noopener\">Fréchet inception distance</a>&nbsp;(FID) (which measures similarity between generated and non-generated images, lower is better), while the unmodified version reached 19.5 FID.</li><li>The models continued to learn at different speeds as training continued. The modified DiT-XL/2&nbsp; took 850,000 training steps to reach 9.6 FID, while the unmodified version took 7 million steps to reach the same number.</li><li>Experiments with modified and unmodified versions of SiT-XL/2 yielded similar results.</li><li>Trained to convergence, the modified models outperformed the unmodified versions. For instance, the modified&nbsp; SiT-XL/2 achieved 5.9 FID (after 4 million training steps), while the unmodified version achieved 8.3 FID (after 7 million training steps).</li></ul><p><strong>Why it matters:</strong>&nbsp;Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other’s embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings.</p><p><strong>We’re thinking:</strong>&nbsp;It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.</p>","comment_id":"67e55e48abd8cc0001702b77","feature_image":"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56--1.jpg","featured":false,"visibility":"public","created_at":"2025-03-27T07:18:48.000-07:00","updated_at":"2025-03-27T07:38:12.000-07:00","published_at":"2025-03-26T07:32:00.000-07:00","custom_excerpt":"The Batch AI News and Insights: Fine-tuning small language models has been gaining traction over the past half year.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"67e561c8abd8cc0001702bbe","name":"Mar 26, 2025","slug":"mar-26-2025","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/mar-26-2025/"},{"id":"67e561c8abd8cc0001702bbf","name":"issue-294","slug":"issue-294","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-294/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-294/","excerpt":"The Batch AI News and Insights: Fine-tuning small language models has been gaining traction over the past half year.","reading_time":14,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Compact Vision-Language with Open Weights, Faster Learning, Diffusion in Few Steps, LLMs Aid Tutors","meta_description":"The Batch AI News and Insights: Fine-tuning small language models has been gaining traction over the past half year.","email_subject":null,"frontmatter":null,"feature_image_alt":"Cartoon of a man playing violin saying “I’m fine-tuning!” while a woman at her desk covers her ears, replying “Did you try prompting?”","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--56--1.jpg","dimensions":{"width":1200,"height":676}},"banner":{"title":"AI is the new electricity","databaseId":29050,"id":"cG9zdDoyOTA1MA==","featuredImage":{"node":{"altText":"AI is the new electricity. Are you ready to flip the switch? Download your free copy of the ebook.","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/2.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://www.deeplearning.ai/resources/#ebooks","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}