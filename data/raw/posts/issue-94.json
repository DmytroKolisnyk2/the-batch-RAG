{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-94","id":"60bfddae274d5b003b1062f2","uuid":"2b0e4b26-7b0e-4e7e-8604-2a0afbbbd48c","title":"The Batch: Autonomous Weapons Used in Combat, Tesla Doubles Down on Computer Vision, Transformers Decipher Proteins","html":"<p><em>Dear friends,</em></p><p><em>In school, most questions have only one right answer. But elsewhere, decisions often come down to a difficult choice among imperfect options. I’d like to share with you some approaches that have helped me make such decisions.</em></p><p><em>When I was deciding where to set up a satellite office outside the U.S., there were many options. My team and I started by listing important criteria such as supply of talent, availability of local partners, safety and rule of law, availability of visas, and cost. Then we evaluated different options against these criteria and built a matrix with cities along one axis and our criteria along the other. That clarified which country would make a great choice.</em></p><p><em>When I feel stuck, I find it helpful to write out my thoughts:</em></p><ul><li><em>What options am I choosing among?</em></li><li><em>What criteria are driving the choice?</em></li><li><em>How does each option rate with respect to the criteria?</em></li><li><em>if I need more information, how can I get it?</em></li></ul><figure class=\"kg-card kg-image-card kg-width-full\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-06-01-at-5.57.29-PM-copy--1-.png\" class=\"kg-image\" alt=\"Photograph of a two-way road in the woods\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-06-01-at-5.57.29-PM-copy--1-.png 600w\"></figure><p><em>Documenting decisions in this way also builds a foundation for further choices. For example, over the years, I’ve collected training data for many different kinds of problems. When I need to select among tactics for acquiring data, having been through the process many times, I know that some of the most important criteria are (i) the time needed, (ii) the number of examples, (iii) accuracy of the labels, (iv) how representative the input distribution is, and (v) cost.</em></p><p><em>If I’m making a decision as part of a team, I check with teammates at each step to make sure we’re accurately capturing the top options, criteria, and so on. (The comments feature in Google Docs is a great way to facilitate open debate within a team.) This helps me avoid losing track of some criteria and acting based on an incomplete set; for example, picking the satellite office’s location based only on the availability of talent. It also helps align everyone on the final decision.</em></p><p><em>As you may know, I wound up setting up a satellite office in Colombia because of the availability of talent and a supportive ecosystem of partners. The team there has become a key part of many projects. Lately I’ve worried about their wellbeing amid Covid-19 and widespread unrest. But in hindsight, setting up in Colombia was one of my best decisions, and I remain as committed as ever to supporting my friends there.</em></p><p><em>Keep learning!</em><br><em><br>Andrew</em></p><p></p><h2 id=\"news\">News</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/Kargu-Redo-2.gif\" class=\"kg-image\" alt=\"A group of drones flying over a field\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/06/Kargu-Redo-2.gif 600w\"></figure><h2 id=\"deadly-drones-act-alone\">Deadly Drones Act Alone</h2><p>Autonomous weapons are often viewed as an alarming potential consequence of advances in AI — but they may already have been used in combat.</p><p><strong>What’s new:</strong> Libyan forces unleashed armed drones capable of choosing their own targets against a breakaway rebel faction last year, said a recent United Nations (UN) <a href=\"https://undocs.org/S/2021/229?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">report</a>. The document, a letter from the organization’s Panel of Experts on Libya to the president of the Security Council, does not specify whether the drones targeted, attacked, or killed anyone. It was brought to light by <a href=\"https://www.newscientist.com/article/2278852-drones-may-have-attacked-humans-fully-autonomously-for-the-first-time/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\"><em>New Scientist</em></a>.</p><p><strong>Killer robots:</strong> In March of 2020, amid Libya’s ongoing civil war, the UN-supported Government of National Accord allegedly attacked retreating rebel forces using <a href=\"https://www.stm.com.tr/en/kargu-autonomous-tactical-multi-rotor-attack-uav?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">Kargu-2</a> quadcopters manufactured by Turkish company STM.</p><ul><li>The fliers are equipped with object-detection and face-recognition algorithms to find and strike targets without explicit human direction.</li><li>Upon acquiring a target, the drone flies directly at it and detonates a small warhead just before impact.</li><li>STM <a href=\"https://www.newscientist.com/article/2217171-autonomous-killer-drones-set-to-be-used-by-turkey-in-syria/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">claims</a> that its systems can distinguish soldiers from civilians.</li><li>The Turkish military bought at least 500 such units for use in its border conflict with Syria. STM is negotiating sales to three other nations, according to <a href=\"https://www.forbes.com/sites/davidhambling/2020/06/17/turkish-military-to-receive-500-swarming-kamikaze-drones/?sh=1e718075251a&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\"><em>Forbes</em></a>.</li></ul><p><strong>Behind the news:</strong> Many nations use machine learning in their armed forces, usually to bolster existing systems, typically with a human in the loop.</p><ul><li>In the most recent battle between Israel and Palestinians in Gaza, the Israeli Defense Force <a href=\"https://www.jpost.com/arab-israeli-conflict/gaza-news/guardian-of-the-walls-the-first-ai-war-669371?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">deployed</a> machine learning systems that analyzed streams of incoming intelligence. The analysis helped its air force identify targets and warn ground troops about incoming attacks.</li><li>The U.S. Army is <a href=\"https://www.thedefensepost.com/2021/01/26/ssci-ai-enabled-drone/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">testing</a> a drone that uses computer vision to identify targets up to a kilometer away and determine whether they’re armed.</li><li>The European Union has <a href=\"https://www.sciencemag.org/news/2020/12/europe-hopes-new-rd-fund-will-boost-meager-defense-capabilities-and-create?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">funded</a> several AI-powered military projects including <a href=\"https://ec.europa.eu/commission/presscorner/detail/en/fs_20_1094?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">explosive device detection</a> and <a href=\"https://ec.europa.eu/commission/presscorner/detail/en/fs_20_1095?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">small unmanned ground vehicles</a> that follow foot soldiers through rough terrain.</li></ul><p><strong>Why it matters:</strong> Observers have long warned that deploying lethal autonomous weapons  on the battlefield could ignite an arms race of deadly machines that decide for themselves who to kill. Assuming the UN report is accurate, the skirmish in Libya appears to have set a precedent.</p><p><strong>We’re thinking:</strong> Considering the problems that have emerged in using today’s AI for critical processes like deploying police, sentencing convicts, and making loans, it’s clear that the technology simply should not be used to make life-and-death decisions. We urge all nations and the UN to develop <a href=\"https://www.hrw.org/report/2020/08/10/stopping-killer-robots/country-positions-banning-fully-autonomous-weapons-and?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek#\">rules</a> to ensure that the world never sees a real AI war.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/ezgif.com-gif-maker---2021-05-04T162716.343-2.gif\" class=\"kg-image\" alt=\"Architecture of vision-language tasks\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/06/ezgif.com-gif-maker---2021-05-04T162716.343-2.gif 600w\"></figure><h2 id=\"one-model-for-vision-language\">One Model for Vision-Language</h2><p>Researchers have proposed task-agnostic architectures for <a href=\"https://arxiv.org/abs/2103.00020?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">image classification tasks</a> and <a href=\"https://arxiv.org/abs/1910.10683?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">language tasks</a>. New work proposes a single architecture for vision-language tasks.<br><br><strong>What’s new:</strong> Led by Tanmay Gupta, researchers at the Allen Institute for AI and University of Illinois at Urbana-Champaign designed a general-purpose vision architecture and built a system, <a href=\"https://arxiv.org/abs/2104.00743?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">GPV-I</a>, that can perform visual question answering, image captioning, object localization, and image classification.<br><br><strong>Key insight:</strong> Model architectures usually are designed for specific tasks, which implies certain types of output. To classify ImageNet, for instance, you need 1,000 outputs, one for each class. But text can describe both tasks and outputs. Take classification: the task “Describe this image” leads to the output, “this image is a dog.” By generating a representation of text that describes a task, a model can learn to perform a variety of tasks and output text that completes it without task-specific alterations in its architecture.<br><br><strong>How it works:</strong> Given a text description of a task — say, “describe the image” — and an image, GPV-I generates separate representations of the text and image, determines their relative importance to one another, and outputs a relevant text response and a copy of the image with bounding boxes. The authors trained it on <a href=\"https://cocodataset.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">COCO</a> image captioning, <a href=\"https://visualqa.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">VQA</a> question answering, and <a href=\"https://arxiv.org/abs/1608.00272v3?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">RefCOCO+</a> object localization datasets.</p><ul><li>The system uses <a href=\"https://arxiv.org/abs/1810.04805?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">BERT</a> to produce a representation of the task. It extracts an initial image representation using a <a href=\"https://arxiv.org/abs/1512.03385?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">ResNet-50</a> and passes it to a transformer borrowed from <a href=\"https://arxiv.org/abs/2005.12872?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">DETR</a>. The transformer splits the representation into a grid, each cell of which contains a representation for the corresponding location in the image.</li><li>A so-called cross-modal module accepts the representations of the image (one for each grid cell) and text (that is, the task) and produces new ones that reflect their relationship. It uses <a href=\"https://arxiv.org/abs/1908.02265?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">co-attention</a> between transformer layers to compare image and text representations and a sigmoid layer to compute the relevance of the image representations to the task. Then it weights each image representation by its relevance.</li><li>An image decoder uses the DETR representations to generate a bounding box for each object detected and the relevance scores to select which boxes to draw over the image. The text decoder (a transformer) uses the BERT representations and weighted representations to generate text output.</li></ul><p><strong>Results:</strong> The researchers evaluated GPV-I on COCO classification, COCO captioning, and VQA question answering. They compared its performance with models trained for those tasks. On classification, GPV-I achieved accuracy of 83.6 percent, while a ResNet-50 achieved 83.3 percent. On captioning, GPV-I achieved 1.023 <a href=\"https://arxiv.org/abs/1411.5726?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">CIDEr-D</a> — a measure of the similarity of generated and ground-truth captions, higher is better — compared to a <a href=\"https://arxiv.org/abs/1909.11059?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">VLP</a>’s 0.961 CIDEr-D. On question answering, GPV-I achieved 62.5 percent accuracy compared to ViLBERT’s score of 60.1 percent, based on the output’s similarity to a human answer.<br><br><strong>Why it matters:</strong> A single architecture that can learn several tasks should be able to share concepts between tasks. For example, a model trained both to detect iguanas in images and to answer questions about other topics might be able to describe what these creatures look like even if they weren’t represented in the question-answering portion of the training data.<br><br><strong>We’re thinking:</strong> Visual classification, image captioning, and visual questioning answering are a start. We look forward to seeing how this approach performs on more varied tasks.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/radiology-redo-2.gif\" class=\"kg-image\" alt=\"X-rays and charts about AI use in radiology\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/06/radiology-redo-2.gif 600w\"></figure><h2 id=\"radiologists-eye-ai\">Radiologists Eye AI</h2><p>AI lately has achieved dazzling success interpreting X-rays and other medical imagery in the lab. Now it’s catching on in the clinic.</p><p><strong>What’s new:</strong> Roughly one-third of U.S. radiologists use AI in some form in their work, according to a <a href=\"https://www.jacr.org/article/S1546-1440(21)00293-3/fulltext?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">survey</a> by the American College of Radiology. One caveat: Many who responded positively may use older — and <a href=\"https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2443369?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">questionable</a> — computer-aided detection, a technique for diagnosing breast cancer that dates to the 1980s, rather than newer methods.</p><p><strong>What they found:</strong> The organization queried its membership via email and received 1,861 responses.</p><ul><li>Of respondents who said they use AI, just over half use it to interpret images, and another 11 percent for image enhancement. The most common applications were breast (45.7 percent), thoracic (36.2 percent), and neurological (30.1 percent) imaging.</li><li>12 percent of AI users said they use the technology to manage work lists, 11 percent to manage operations.</li><li>Nearly 10 percent of AI users built their own algorithms rather than buying from outside vendors.</li><li>94 percent of AI users said their systems perform inconsistently. Around 6 percent said they always work, and 2 percent said they never work.</li><li>More than two thirds of respondents said they don’t use AI, and 80 percent of those said they see no benefit in it. Many believe that the technology is too expensive to implement, would hamper productivity, or wouldn’t be reimbursed.</li></ul><p><strong>Behind the news:</strong> AI’s role in medical imaging is still taking shape, as <a href=\"https://pubs.rsna.org/doi/full/10.1148/ryai.2019190058?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">detailed</a> by Stanford radiology professor Curtis Langlotz in the journal <em>Radiology: Artificial Intelligence</em>. In 2016, a prominent oncologist <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5070532/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">wrote</a> in the <em>New England Journal of Medicine</em>, “machine learning will displace much of the work of radiologists.” Two years later, <a href=\"https://hbr.org/2018/03/ai-will-change-radiology-but-it-wont-replace-radiologists?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\"><em>Harvard Business Review</em></a> published a doctor-penned essay headlined, “AI Will Change Radiology, but It Won’t Replace Radiologists.” <em>Radiology Business</em> recently asked, “Will AI replace radiologists?” and <a href=\"https://www.radiologybusiness.com/topics/artificial-intelligence/wait-will-ai-replace-radiologists-after-all?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">concluded</a>, “Yes. No. Maybe. It depends.”</p><p><strong>Why it matters:</strong> AI’s recent progress in medical imaging is impressive. Although the reported 30 percent penetration rate probably includes approaches that have been uses for decades, radiologists are on their way to realizing the technology’s promise.</p><p><strong>We’re thinking:</strong> One-third down, two-thirds to go! Machine learning engineers can use such findings to understand what radiologists need and develop better systems for them.</p><hr><h3 id=\"a-message-from-deeplearningai\">A MESSAGE FROM <a href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\">DEEPLEARNING.AI</a></h3><figure class=\"kg-card kg-image-card\"><a href=\"https://www.coursera.org/specializations/practical-data-science?ref=dl-staging-website.ghost.io\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/Specialization-Name--1-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2021/06/Specialization-Name--1-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2021/06/Specialization-Name--1-.png 1000w, https://dl-staging-website.ghost.io/content/images/2021/06/Specialization-Name--1-.png 1200w\" sizes=\"(min-width: 720px) 720px\"></a></figure><p>We’re proud to launch <em>Practical Data Science</em>, in partnership with Amazon Web Services (AWS)! This new specialization will help you develop the practical skills to deploy data science projects effectively and overcome machine learning challenges using Amazon SageMaker. <a href=\"https://www.coursera.org/specializations/practical-data-science?ref=dl-staging-website.ghost.io\">Enroll now</a></p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/tesla-redo_orange-background-2.gif\" class=\"kg-image\" alt=\"Animation showing Tesla car's vision system\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/06/tesla-redo_orange-background-2.gif 600w\"></figure><h2 id=\"tesla-all-in-for-computer-vision\">Tesla All-In For Computer Vision</h2><p>Tesla is abandoning radar in favor of a self-driving system that relies entirely on cameras.</p><p><strong>What’s new:</strong> The electric car maker <a href=\"https://www.tesla.com/support/transitioning-tesla-vision?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">announced</a> it will no longer include radar sensors on Model 3 sedans and Model Y compact SUVs sold in North America. Tesla is the only major manufacturer of autonomous vehicles to bet solely on computer vision. Most others rely on a combination of lidar, radar, and cameras.</p><p><strong>How it works:</strong> Tesla has dropped radar only in the U.S. and only in its two most popular models. It aims to gather data and refine the technology before making the change in Model S, Model X, and vehicles sold outside the U.S.</p><ul><li>The eight-camera system called Tesla Vision will provide sensory input for Autopilot driver-assistance features such as lane controls as well as the Full Self-Driving upgrade, which automatically parks and summons vehicles, slows for stop signals, and automates highway driving. Such features will be “limited or inactive” during the transition.</li><li>The move comes on the heels of earlier statements that touted cameras. “When radar and vision disagree, which one do you believe?” Musk said in a <a href=\"https://twitter.com/elonmusk/status/1380796939151704071?s=20&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">tweet</a> on April 10. “Vision has much more precision, so better to double down on vision than do sensor fusion.”</li><li>CEO Elon Musk <a href=\"https://electrek.co/2021/04/29/elon-musk-tesla-next-full-self-driving-beta-update-will-blow-your-mind/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">predicted</a> that Tesla Vision would help the company’s vehicles achieve full autonomy by the end of 2021. (Musk has a <a href=\"https://www.roadandtrack.com/news/a35350331/checking-in-on-all-the-promises-elon-musk-and-tesla-have-made/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">history</a> of declaring ambitious goals his company has failed to meet.)</li></ul><p><strong>Behind the news:</strong> Some people in the self-driving car industry favor using relatively expensive lidar and radar sensors in addition to low-cost cameras because they provide more information and thus greater safety. Camera-only advocates counter that humans can drive safely perceiving only images, so we should build AI that does the same. Most companies working on autonomous vehicles have chosen the more expensive route  as the fastest way to reach full autonomy safely. Once they get there, the thinking goes, they can attend to bringing the cost down.</p><p><strong>Why it matters:</strong> If Tesla’s bet on cameras pays off, it could have an outsize influence on future self-driving technology.</p><p><strong>We’re thinking:</strong> While it’s great to see ambitious plans to commercialize computer vision, Tesla’s initiative will require tests on public streets. That means countless drivers will be the company’s unwitting test subjects — a situation that, as ever, demands strong oversight by road-safety authorities.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/PROTEIN-1-2.gif\" class=\"kg-image\" alt=\"Protein structures\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/06/PROTEIN-1-2.gif 600w\"></figure><h2 id=\"what-ai-knows-about-proteins\">What AI Knows About Proteins</h2><p>Transformer models trained on sequences of amino acids that form proteins have had success <a href=\"https://arxiv.org/abs/1906.08230?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">classifying</a> and <a href=\"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">generating</a> viable sequences. New research shows that they also capture information about protein structure.</p><p><strong>What’s new:</strong> Transformers can encode the grammar of amino acids in a sequence the same way they do the grammar of words in a language. Jesse Vig and colleagues at Salesforce Research and University of Illinois at Urbana-Champaign developed <a href=\"https://openreview.net/forum?id=YWtLZvLmud7&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">methods</a> to interpret such models that reveal biologically relevant properties.</p><p><strong>Key insight:</strong>  When amino acids bind to one another, the sequence folds into a shape that determines the resulting protein’s biological functions. In a transformer trained on such sequences, a high self-attention value between two amino acids can indicate that they play a significant role in the protein’s structure. For instance, the protein’s folds may bring them into contact.</p><p><strong><strong>How it works: </strong></strong>The authors studied a <a href=\"https://arxiv.org/abs/1810.04805?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">BERT</a> pretrained on a <a href=\"https://academic.oup.com/nar/article/47/D1/D427/5144153?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">database of amino acid sequences</a> to predict masked amino acids based on others in the sequence. Given a sequence, they studied the self-attention values in each layer of the model.</p><ul><li>For each sequence in the dataset, the authors filtered out self-attention values below a threshold to find amino acid pairs with strong relationships. Consulting information in the database, they tallied the number of relationships associated with a given property of the protein’s shape (for example, pairs of amino acids in contact).</li><li>Some properties depended on only one amino acid in a pair. For example, an amino acid may be part of the protein site that binds to molecules such as drugs. (The authors counted such relationships if the second amino acid had the property in question.)</li></ul><p><strong>Results:</strong> The authors compared their model’s findings with those reported in other <a href=\"https://papers.nips.cc/paper/2019/hash/37f65c068b7723cd7809ee2d31d7861c-Abstract.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">protein</a> <a href=\"https://papers.nips.cc/paper/2019/hash/37f65c068b7723cd7809ee2d31d7861c-Abstract.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\">databases</a>. The deeper layers of the model showed an increasing proportion of related pairs in which the amino acids actually were in contact, up to 44.7 percent, while the proportion of all amino acids in contact was 1.3 percent. The chance that the second amino acid in a related pair was part of a binding site didn’t rise steadily across layers, but it reached 48.2 percent, compared to a 4.8 percent chance that any amino acid was part of a binding site.</p><p><strong>Why it matters:</strong> A transformer model trained only to predict missing amino acids in a sequence learned important things about how amino acids form a larger structure. Interpreting self-attention values reveals not only how a model works but also how nature works.</p><p><strong>We’re thinking:</strong> Such tools might provide insight into the structure of viral proteins, helping biologists discover ways to fight viruses including SARS-CoV-2 more effectively.</p><hr><h3 id=\"a-message-from-deeplearningai-1\">A MESSAGE FROM <a href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\">DEEPLEARNING.AI</a></h3><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/06/The-Batch-Image--3--1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2021/06/The-Batch-Image--3--1.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2021/06/The-Batch-Image--3--1.png 1000w, https://dl-staging-website.ghost.io/content/images/2021/06/The-Batch-Image--3--1.png 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p><a href=\"https://aixrealworld.eventbrite.com/?aff=batch&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">Join us</a> for a live virtual event on June 9, 2021! Experts from <a href=\"https://omdena.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8rPiUKAGj2EQ7PgIpZWy_iTw2M4VPGg3DXbElMRU95WB-xeSv0l5NgxduJtvNu5L0kVbek\" rel=\"noopener\">Omdena</a> will walk through two AI case studies: “Real Life: Understanding the Causes and Effects of Student Debt through Machine Learning” and “AI for Energy: Transitioning Toward a Sustainable Energy System.”</p>","comment_id":"60b8702b41c1d6004ba45230","feature_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-06-01-at-5.57.29-PM-copy--1--2.png","featured":false,"visibility":"public","created_at":"2021-06-02T23:01:15.000-07:00","updated_at":"2022-10-06T07:28:29.000-07:00","published_at":"2021-06-02T12:00:00.000-07:00","custom_excerpt":"In school, most questions have only one right answer. But elsewhere, decisions often come down to a difficult choice among imperfect options. I’d like to share with you some approaches that have helped me make such decisions.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"60bfddad274d5b003b1062af","name":"issue-94","slug":"issue-94","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-94/"},{"id":"63125964a679c0004d519c3d","name":"Jun 02, 2021","slug":"jun-02-2021","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/jun-02-2021/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-94/","excerpt":"In school, most questions have only one right answer. But elsewhere, decisions often come down to a difficult choice among imperfect options. I’d like to share with you some approaches that have helped me make such decisions.","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Autonomous Weapons Used in Combat, Tesla Doubles Down on Computer","meta_description":"The Batch - AI News & Insights: Deadly Drones Act Alone | One Model for Vision-Language | Radiologists Eye AI | Tesla All-In For Computer Vision...","email_subject":null,"frontmatter":null,"feature_image_alt":"Photograph of a two-way road in the woods","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-06-01-at-5.57.29-PM-copy--1--2.png","dimensions":{"width":600,"height":338}},"banner":{"title":"Generative AI for Everyone","databaseId":32549,"id":"cG9zdDozMjU0OQ==","featuredImage":{"node":{"altText":"Generative AI for Everyone","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/10/GenAI4E_sidebanner.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3Moa97R","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}