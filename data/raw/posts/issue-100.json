{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-100","id":"60ef358d28328d003b2ea203","uuid":"de8ae427-bdb5-4a88-a8b0-593e302c6dd6","title":"The Batch: Walking the Robot Dog, Mistaking German for English, Making Art With an Image Classifier, Zero-Shot Object Detection","html":"<p><em>Dear friends,</em></p><p><em>I’ve been following with excitement the recent progress in space launches. Earlier this week, Richard Branson and his Virgin Galactic team <a href=\"https://arstechnica.com/science/2021/07/heres-why-richard-bransons-flight-matters-and-yes-it-really-matters/?ref=dl-staging-website.ghost.io\" rel=\"noopener\">flew</a> a rocket plane 53 miles up, earning him astronaut wings. Next week, Jeff Bezos’ Blue Origin is expected to attempt a similar feat and achieve an even greater altitude. (I once also sat in a Blue Origin passenger capsule; see the picture below. I remained firmly on planet Earth.)</em><br><br><em>The first <a href=\"https://en.wikipedia.org/wiki/Space_Race?ref=dl-staging-website.ghost.io\" rel=\"noopener\">space race</a> was between the U.S. and the Soviet Union, a competition between rival superpowers with dramatically different visions for civilization. Some pundits have panned the current space race as a contest between billionaires, but I’m glad that Bezos, Branson, and Elon Musk are pushing the boundaries of commercial flight.</em><br><br><em>I’ve found space exploration exhilarating since I was a child. My father had a passion for astronomy. We spent many hours on the rooftop of our apartment complex in Singapore — often staying up way past the bedtime designated by my mother 😅 — peering through my dad’s telescope at the planets in our solar system. I remember peering at <a href=\"https://en.wikipedia.org/wiki/Alpha_Centauri?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Alpha Centauri</a> (the closest star system to ours) and wondering if I would visit someday.</em></p><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/07/Screen-Shot-2021-07-13-at-4.22.27-PM-copy.png\" class=\"kg-image\" alt=\"Andrew Ng sitting in the Blue Origin passenger capsule\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/07/Screen-Shot-2021-07-13-at-4.22.27-PM-copy.png 600w\"></figure><p><em>Space exploration has been criticized as a waste of resources, given the problems we have here at home. Of course, we need to work on problems such as the still-rampaging Covid-19, climate change, poverty, and injustice. I believe society will be best off if we pursue multiple meaningful projects simultaneously.</em></p><p><em>As we push further into space, AI will play an increasing role. Our robots will need to be increasingly autonomous because, even though radio waves travel at the speed of light, there won’t be sufficient time to wait for guidance from human operators on Earth. (Mars averages 13 light minutes from Earth, and the more distant Neptune about 250 light minutes.) I was excited when ROS, the open-source Robot Operating System framework launched by Morgan Quigley out of my Stanford group, started <a href=\"https://www.ros.org/news/2014/09/ros-running-on-iss.html?ref=dl-staging-website.ghost.io\" rel=\"noopener\">running</a> in the International Space Station. And we still have much work ahead!</em><br><br><em>Private entities are at the center of this week’s space boom, but I would love to see public entities play a bigger role. NASA’s <a href=\"https://en.wikipedia.org/wiki/NASA_spinoff_technologies?ref=dl-staging-website.ghost.io\" rel=\"noopener\">innovations</a> have been widely shared. I’m excited about the <a href=\"https://en.wikipedia.org/wiki/Perseverance_(rover)?ref=dl-staging-website.ghost.io\">Perseverance</a> rover and <a href=\"https://en.wikipedia.org/wiki/Ingenuity_(helicopter)?ref=dl-staging-website.ghost.io\" rel=\"noopener\">Ingenuity</a> helicopter now roaming Mars (over 1 million times farther than Branson has yet to travel). So let’s make sure to strongly support public space exploration as well. Further advances will come even faster with their help.</em></p><p><em>Keep learning! 🚀</em><br><br><em>Andrew</em></p><p></p><h2 id=\"news\">News</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-07-14T100209.763.gif\" class=\"kg-image\" alt=\"A four-legged robot walking over difficult and changing terrain\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-07-14T100209.763.gif 600w\"></figure><h2 id=\"walking-the-dog\">Walking the Dog</h2><p>A reinforcement learning system enabled a four-legged robot to amble over unfamiliar, rapidly changing terrain.</p><p><strong>What’s new:</strong> Researchers at UC Berkeley, Facebook, and Carnegie Mellon developed <a href=\"https://ashish-kmr.github.io/rma-legged-robots?ref=dl-staging-website.ghost.io\">Rapid Motor Adaptation</a> (RMA). The system enabled a <a href=\"https://www.unitree.com/products/a1/?ref=dl-staging-website.ghost.io\">Unitree Robotics A1</a> to negotiate changing conditions and unexpected obstacles nearly in real time. The machine traversed muddy trails, bushy backcountry, and an oil-slicked plastic sheet without falling.</p><p><strong>How it works:</strong> The system includes two algorithms, both of which are trained in simulation. The reinforcement learning component learns to control locomotion basics, while the adaptation module learns to generate a representation of the environment.</p><ul><li>In deployment, the two algorithms run asynchronously on a single edge device. They analyze the previous 0.5 seconds of data from limbs and joints and adjust the gait accordingly.</li><li>In tests, the robot maneuvered through conditions that it hadn’t encountered in simulations, such as a squishy foam mattress, over piles of rubble, and rough-hewn staircases. It repeated many of the tests carrying loads of varying weight.</li><li>The machine achieved 70 percent or better success in each scenario. When it fell, the mishap typically was due to a sudden drop while descending stairs or debris that blocked more than one leg.</li></ul><p><strong>Behind the news:</strong> Video clips of robots from <a href=\"https://www.youtube.com/watch?v=7atZfX85nd4&ref=dl-staging-website.ghost.io\" rel=\"noopener\">Boston Dynamics</a> and <a href=\"https://www.youtube.com/watch?v=PqLZP8TANlg&ref=dl-staging-website.ghost.io\" rel=\"noopener\">others</a> have become viral sensations in recent years. They may be mouth-watering, but the bots involved often are programmed for specific motions or scenarios and can’t adapt to novel conditions.</p><p><strong>Why it matters:</strong> RMA is among the first robotic walking systems that don’t need to be trained for every variety of terrain they're likely to encounter.</p><p><strong>We’re thinking:</strong> For many applications where navigating flat ground is sufficient, wheeled locomotion is much simpler and more reliable. But legs still carry the day when navigating rough terrain — not to discount their uncanny anthropomorphic appeal. They’re likely to be important for tasks like fighting fires, traversing disaster zones, and navigating the toy-strewn obstacle course that is Andrew’s daughter's playroom.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/07/interview.gif\" class=\"kg-image\" alt=\"Screen captures of a job interviews automation system\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/07/interview.gif 600w\"></figure><h2 id=\"danke-for-the-interview\">Danke for the Interview</h2><p>An independent test found flaws in AI systems designed to evaluate job applicants.</p><p><strong>What’s new:</strong> MyInterview and Curious Thing, which automate job interviews, gave a candidate who spoke only in German high marks on English proficiency, according to <a href=\"https://www.technologyreview.com/2021/07/07/1027916/we-tested-ai-interview-tools/?ref=dl-staging-website.ghost.io\"><em>MIT Technology Review</em></a><em>.</em></p><p><strong>The test:</strong> Reporters created a fake job posting for an office administrator/researcher on both companies’ platforms. They used the tools provided to select questions for applicants to answer and define their ideal candidate. Then one of them applied for the position, completing interviews by reading aloud from a <em>Wikipedia</em> article written in German.</p><ul><li><a href=\"https://explore.myinterview.com/myinterviewintelligence?ref=dl-staging-website.ghost.io\">MyInterview</a> typically conducts a video interview and analyzes a candidate’s verbal and body language, then grades their suitability for a given job. MyInterview interpreted the German-speaking reporter’s responses as nonsensical English (“So humidity is desk a beat-up. Sociology, does it iron?”) but graded her as a 73 percent match for the job. A MyInterview spokesperson said the algorithm inferred personality traits from the interviewee’s voice rather the content of her answers.</li><li><a href=\"https://www.curiousthing.io/blog/product/how-curious-thing-measures-behaviour-traits?ref=dl-staging-website.ghost.io\">Curious Thing</a> analyzes phone interview responses. Its algorithm gave the reporter 6 out of 9 points for English-language competency after she responded exclusively in German. The company’s cofounder said the bogus application was an “extremely valuable data point.”</li></ul><p><strong>Behind the news:</strong> A <a href=\"https://www.shrm.org/resourcesandtools/hr-topics/global-hr/pages/employers-embrace-artificial-intelligence-for-hr.aspx?ref=dl-staging-website.ghost.io\">2019 survey</a> found that 40 percent of companies worldwide use AI to help screen job candidates, but outside investigators have found such systems lacking.</p><ul><li>In February, Bavarian Public Broadcasting <a href=\"https://web.br.de/interaktiv/ki-bewerbung/en/?ref=dl-staging-website.ghost.io\">showed</a> that accessories like glasses and headscarves and backgrounds including objects like pictures and bookcases dramatically changed a German video-interview platform’s automated assessments.</li><li>In 2018, <a href=\"https://www.technologyreview.com/2021/06/23/1026825/linkedin-ai-bias-ziprecruiter-monster-artificial-intelligence/?ref=dl-staging-website.ghost.io\">LinkedIn discovered</a> that a candidate recommendation algorithm preferred male applicants. The company replaced it with a new system intended to counteract that bias.</li><li>A recent <a href=\"https://arxiv.org/abs/2106.12403?ref=dl-staging-website.ghost.io\">study</a> from NYU, CUNY, and Twitter proposed a matrix for rating automated hiring systems to counteract the prevalence of algorithms that rely on dubious features like voice intonation and subtle facial expressions.</li></ul><p><strong>Why it matters:</strong> Matching prospective employers and employees is a nuanced process, and any attempt to automate it requires the utmost rigor. Applicants subject to a flawed algorithm could be barred from jobs they’re eminently qualified for, while prospective employers who rely on it could miss ideal candidates.</p><p><strong>We’re thinking:</strong> An AI system that gives high marks to someone who replies to an English-language interview in German — confidently rendering incorrect predictions in response to data that’s dramatically different its training set — is not equipped to handle <a href=\"https://www.coursera.org/learn/introduction-to-machine-learning-in-production/lecture/k9iID/key-challenges?ref=dl-staging-website.ghost.io\" rel=\"noopener\">data drift.</a> Such concepts are not purely academic. They have a huge impact on such systems — and on critical decisions like who gets a job.</p><hr><h3 id=\"a-message-from-deeplearningai\">A MESSAGE FROM <a href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\">DEEPLEARNING.AI</a></h3><figure class=\"kg-card kg-image-card\"><a href=\"https://www.coursera.org/learn/advanced-machine-learning-concepts?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20210519-pds-1-3-thebatch-email-pds-launch\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/07/Course-Name-3-2.webp\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1050\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2021/07/Course-Name-3-2.webp 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2021/07/Course-Name-3-2.webp 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2021/07/Course-Name-3-2.webp 1600w, https://dl-staging-website.ghost.io/content/images/size/w2400/2021/07/Course-Name-3-2.webp 2400w\" sizes=\"(min-width: 720px) 720px\"></a></figure><p>“Optimize ML Models and Deploy Human-in-the-Loop Pipelines,” Course 3 in our new <em>Practical Data Science Specialization</em>, is set to launch on July 21, 2021! Harness human intelligence to tune accuracy, compare performance, and generate new training data. <a href=\"https://www.coursera.org/learn/advanced-machine-learning-concepts?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20210519-pds-1-3-thebatch-email-pds-launch\" rel=\"noopener\">Pre-enroll now</a></p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-06-01T145617.637.gif\" class=\"kg-image\" alt=\"Image showing how object detectors work \" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-06-01T145617.637.gif 600w\"></figure><h2 id=\"i-know-it-when-i-see-it\">I Know It When I See It</h2><p>Object detectors typically detect only items that were labeled in their training data. A new method liberates them to locate and recognize a much wider variety of objects.</p><p><strong>What’s new:</strong> Xiuye Gu and colleagues at Google Research developed <a href=\"https://arxiv.org/abs/2104.13921?ref=dl-staging-website.ghost.io\">Vision and Language Knowledge Distillation</a> (ViLD) to build a zero-shot object detector — that is, one that can handle classes on which it didn’t train. ViLD takes advantage of representations generated by the pretrained zero-shot classifier <a href=\"https://arxiv.org/abs/2103.00020?ref=dl-staging-website.ghost.io\">CLIP</a>.</p><p><strong>Key Insight:</strong> In knowledge distillation, one model learns to mimic another model’s output. Similarly, one model can learn to mimic another’s representations. An object detector’s representations (which encode several regions and classifications per image) can conform to a classifier’s (which encode one classification per image) by cropping the images that contain multiple objects into separate regions for the classifier. Then the object detector can learn to reproduce the classifier’s representation of each region.</p><p><strong>How it works:</strong> To understand ViLD, it helps to know a bit about CLIP. CLIP matches images and text using a <a href=\"https://arxiv.org/abs/2010.11929?ref=dl-staging-website.ghost.io\">vision transformer</a> and a text transformer pretrained on 400 million image-text pairs. At inference, users give it a text list of the classes they want to recognize. Fed an image, it returns the most likely class in the list. To that system, the authors added a <a href=\"https://arxiv.org/abs/1703.06870?ref=dl-staging-website.ghost.io\">Mask R-CNN</a> object detector trained on the most common classes in <a href=\"https://arxiv.org/abs/1908.03195?ref=dl-staging-website.ghost.io\">Large Vocabulary Instance Segmentation</a> (LVIS), a dataset that contains images of objects that have been segmented and labeled. They reserved the other LVIS classes for the test set.</p><ul><li>Given a list of LVIS classes, CLIP’s text transformer generated a list of class representations.</li><li>Given an image, Mask R-CNN generated object representations. In parallel, CLIP’s vision transformer generated corresponding cropped-region representations.</li><li>For each Mask R-CNN object representation, the authors found the closest LVIS class representation. They measured similarity using cosine similarity, a measure of the angle between two vectors, and applied a softmax to predict the object’s class.</li><li>They trained the Mask R-CNN using two loss terms. The first minimized the difference between CLIP’s and Mask R-CNN’s representations. The second encouraged the Mask R-CNN’s predicted class of a region to match the known label.</li><li>At inference, they fed the remaining LVIS classes to CLIP and added the text transformer’s representations to the earlier list. Presented with a new object class, the Mask R-CNN generated a representation, and the authors found the closest LVIS class representation in the list.</li></ul><p><strong>Results:</strong> The authors pitted their system against a Mask R-CNN trained on all LVIS classes in a supervised manner. They compared average precision, a measure of how many objects were correctly identified in their correct location (higher is better). The author’s system achieved 16.1 average precision on novel categories, while the supervised model’s achieved 12.3 average precision.</p><p><strong>Why it matters</strong>: Large, diverse training datasets for object detection are difficult and expensive to obtain. ViLD offers a way to overcome this bottleneck.</p><p><strong>We’re thinking:</strong> Physicists who want to classify a Bose-Einstein condensate need absolute-zero-shot object detection.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2021/07/art3.gif\" class=\"kg-image\" alt=\"Series of AI generated imagery\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2021/07/art3.gif 600w\"></figure><h2 id=\"clip-art\">CLIP Art</h2><p>Creative engineers are combining deep learning systems to produce a groundswell of generated imagery.</p><p><strong>What’s new:</strong> Researchers, hackers, and artists are producing new works by pairing <a href=\"https://github.com/openai/CLIP?ref=dl-staging-website.ghost.io\" rel=\"noopener\">CLIP</a>, a pretrained image classifier, with a generative adversarial network (GAN). UC Berkeley researcher Charlie Snell captured the ferment in a <a href=\"https://ml.berkeley.edu/blog/posts/clip-art/?ref=dl-staging-website.ghost.io\">blog post</a>.</p><p><strong>How it works:</strong> Users typically give CLIP a text list of the classes they want to recognize; given an image, it returns the most likely class in the list. Digital artists, on the other hand, feed CLIP a verbal description of an image they want to produce and use its ability to match text with images to guide a GAN.</p><ul><li>The community has developed a set of Google Collab Notebooks that link CLIP with various GANs. A user types a phrase, sets some parameters, and chooses which GAN to use for image generation.</li><li>Once the GAN has generated an image, CLIP <a href=\"https://www.youtube.com/watch?v=_TIQtr8gNJQ&ref=dl-staging-website.ghost.io\">scores</a> it based on how closely it matches the original phrase. The Collab code then adjusts the GAN’s hyperparameters iteratively, so its output earns a higher score from CLIP. It repeats the cycle of generation and adjustment until CLIP’s score exceeds a threshold set by the user.</li><li>Different GANs yield images with different visual characteristics. For instance, <a href=\"https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing&ref=dl-staging-website.ghost.io\">pairing CLIP with BigGAN</a> produces output that tends to look like an impressionist painting. <a href=\"https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT?fbclid=IwAR30ZqxIJG0-2wDukRydFA3jU5OpLHrlC_Sg1iRXqmoTkEhaJtHdRi6H7AI&ref=dl-staging-website.ghost.io#scrollTo=g7EDme5RYCrt\">Pairing CLIP with VQ-GAN</a> produces more abstract images with a cubist look.</li><li>Adding to the prompt a phrase like “rendered in Unreal Engine,” referring to a popular video game renderer, can drastically improve the quality of the generated output.</li></ul><p><strong>Behind the news:</strong> Open AI has its own image generator, <a href=\"https://github.com/openai/DALL-E?ref=dl-staging-website.ghost.io\" rel=\"noopener\">DALL·E</a>. Reportedly its output is less abstract and fanciful.</p><p><strong>Why it matters:</strong> CLIP was built to classify, not co-create, while GANs were developed to produce variations on familiar images. The boomlet in generated art shows how the creative impulse can unlock potential that engineers may not have imagined.</p><p><strong>We’re thinking:</strong> It’s great to see human artists collaborating with neural networks. It’s even better to see neural networks collaborating with one another!<br></p>","comment_id":"60ef358d28328d003b2ea203","feature_image":"https://dl-staging-website.ghost.io/content/images/2021/07/Screen-Shot-2021-07-13-at-4.22.27-PM-copy-1.png","featured":false,"visibility":"public","created_at":"2021-07-14T12:05:49.000-07:00","updated_at":"2022-10-06T07:14:35.000-07:00","published_at":"2021-07-14T12:00:00.000-07:00","custom_excerpt":"I’ve been following with excitement the recent progress in space launches. Earlier this week, Richard Branson and his Virgin Galactic team flew a rocket plane 53 miles up, earning him astronaut wings. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"60ef358d28328d003b2ea205","name":"issue-100","slug":"issue-100","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-100/"},{"id":"60ef358d28328d003b2ea204","name":"Jul 14, 2021","slug":"jul-14-2021","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/jul-14-2021/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-100/","excerpt":"I’ve been following with excitement the recent progress in space launches. Earlier this week, Richard Branson and his Virgin Galactic team flew a rocket plane 53 miles up, earning him astronaut wings. ","reading_time":9,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Walking the Robot Dog, Mistaking German for English, Making...","meta_description":"The Batch - AI News & Insights: Reinforcement learning system enabled a four-legged robot to amble over unfamiliar, rapidly changing terrain | flaws...","email_subject":null,"frontmatter":null,"feature_image_alt":"Andrew Ng sitting in the Blue Origin passenger capsule","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/07/Screen-Shot-2021-07-13-at-4.22.27-PM-copy-1.png","dimensions":{"width":600,"height":338}},"banner":{"title":"AI is the new electricity","databaseId":29050,"id":"cG9zdDoyOTA1MA==","featuredImage":{"node":{"altText":"AI is the new electricity. Are you ready to flip the switch? Download your free copy of the ebook.","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/2.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://www.deeplearning.ai/resources/#ebooks","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}