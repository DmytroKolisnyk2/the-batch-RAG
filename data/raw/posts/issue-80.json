{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-80","id":"60bfddae274d5b003b1062fc","uuid":"196ce8c5-9f67-4c0b-a45e-fba5a8074eec","title":"The Batch: Face Datasets Under Fire, Baking With AI, Human Disabilities Baffle Algorithms, Ginormous Transformers","html":"<p><em>Dear friends,</em></p><p><em>AI-enabled automation is often portrayed as a binary on-or-off: A process is either automated or not. But in practice, automation is a spectrum, and AI teams have to choose where on this spectrum to operate. It’s important to weigh the social impact of our work, and we must ameliorate automation’s impact on jobs. In addition to this important consideration, the best choice often depends on the application and what AI can and cannot do.</em><br><br><em>Take the problem of diagnosing medical patients from X-rays. The deployment options include:</em></p><ul><li><em><strong>Human only:</strong> No AI involved.</em></li><li><em><strong>Shadow mode:</strong> A human doctor reads an X-ray and decides on a diagnosis, but an AI system shadows the doctor with its own attempt. The system’s output doesn’t create value for doctors or patients directly, but it is saved for analysis to help a machine learning team evaluate the AI’s performance before dialing it up to the next level of automation.</em></li><li><em><strong>AI assistance:</strong> A human doctor is responsible for the diagnosis, but the AI system may supply suggestions. For example, it can highlight areas of an X-ray for the doctor to focus on.</em></li><li><em><strong>Partial automation:</strong> An AI system looks at an X-ray image and, if it has high confidence in its decision, renders a diagnosis. In cases where it’s not confident, it asks a human to make the decision.</em></li><li><em><strong>Full automation:</strong> AI makes the diagnosis.</em></li></ul><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1-1.png\" class=\"kg-image\" alt=\"Process of diagnosing a medical patient slide\" loading=\"lazy\"></figure><p><em>These options can apply to medical diagnosis, visual inspection, autonomous navigation, media content moderation, and many other tasks. In many cases, I’ve found that picking the right one is critical for a successful deployment, and that using either too much or too little automation can have a significant negative impact.</em><br><br><em>When you’re choosing a point along the automation spectrum, it’s worth considering what degree of automation is possible given the AI system’s accuracy, availability of humans to assist with the task, and desired rate of decision making (for example, human-in-the-loop options won’t work if you need to select an ad to place on a webpage within 100 milliseconds). Today’s algorithms are good enough only for certain points on the spectrum in a given application. As an AI team gains experience and collects data, it might gradually move to higher levels of automation within ethical and legal boundaries.</em><br><br><em>Some people say that we should focus on IA (intelligence augmentation) rather than AI — that AI should be used to help humans perform tasks rather than automate those tasks. I believe we should try to create value for society overall. Automation can transform and create jobs (as when taxi cabs created new opportunities for cab drivers) as well as destroy them. Even as we pick a point on this spectrum, let’s take others’ livelihoods into account and create value that is widely and fairly shared.</em></p><p><em>Keep learning!</em></p><p><em>Andrew</em></p><p></p><h2 id=\"news\">News</h2><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1.gif\" class=\"kg-image\" alt=\"Dozens of different faces shown in a series of images\" loading=\"lazy\"></figure><h2 id=\"cutting-corners-to-recognize-faces\">Cutting Corners to Recognize Faces</h2><p>Datasets for training face recognition models have ballooned in size — while slipping in quality and respect for privacy.</p><p><strong>What’s new:</strong> In a <a href=\"https://arxiv.org/pdf/2102.00813.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">survey</a> of 130 datasets compiled over the last four decades, Mozilla fellow Inioluwa Deborah Raji and AI consultant Genevieve Fried traced how the need for increasing quantities of data led researchers to relax their standards. The result: datasets riddled with blurred photos, biased labels, and images of minors, collected and used without permission, the authors told <em><a href=\"https://www.technologyreview.com/2021/02/05/1017388/ai-deep-learning-facial-recognition-data-history/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">MIT Technology Review</a></em>.</p><p><strong>What they found:</strong> The study divides the history of face datasets into four periods.</p><ul><li>Starting in 1964, face images were captured in photo shoots using paid models and controlled lighting. Gathering these datasets was expensive and time-consuming; the biggest comprised 7,900 images.</li><li>The U.S. Department of Defense kicked off the second period in 1996 by spending $6.5 million to develop <a href=\"https://www.nist.gov/programs-projects/face-recognition-technology-feret?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">FERET</a>, which contained 14,126 images of 1,200 individuals. Like most other datasets of this era, it was compiled from photo shoots with consenting subjects. Models trained on these datasets faltered in the real world partly due to their relatively homogenous lighting and poses.</li><li>Released in 2007, <a href=\"http://vis-www.cs.umass.edu/lfw/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Labeled Faces in the Wild</a> was the first face dataset scraped from the web. LFW’s 13,000 images included varied lighting conditions, poses, and facial expressions. Other large datasets were gathered from Google, Flickr, and Yahoo as well as mugshots and surveillance footage.</li><li>In 2014, Facebook introduced <a href=\"https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">DeepFace</a>, the first face recognition model that used deep learning, which identified people with unprecedented accuracy. Researchers collected tens of millions of images to take advantage of this data-intensive approach. Obtaining consent for every example became impossible, as did ensuring that each one’s label was accurate and unbiased.</li></ul><p><strong>Why it matters:</strong> People deserve to be treated fairly and respectfully by algorithms as well as other people. Moreover, datasets assembled without due attention to permission and data quality erode the public’s trust in machine learning. Companies like <a href=\"https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Clearview.ai</a> and <a href=\"https://nakedsecurity.sophos.com/2019/05/31/facial-recognition-used-to-strip-adult-industry-workers-of-anonymity/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">FindFace</a> stand accused of harvesting online images without consent and using them in ways that violate individuals’ privacy, while shaky algorithms have contributed to <a href=\"https://www.technologyreview.com/2020/12/29/1015563/why-2020-was-a-pivotal-contradictory-year-for-facial-recognition/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">biased policing</a>. In the <a href=\"https://www.theguardian.com/technology/2020/jun/23/uks-facial-recognition-technology-breaches-privacy-rights?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">U.K.</a>, <a href=\"https://www.cbc.ca/news/politics/technology-clearview-facial-recognition-1.5899008?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Canada</a>, and <a href=\"https://techcrunch.com/2020/07/15/facial-recognition-lawsuit-vance-janecyk-bipa/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">certain U.S. jurisdictions</a>, lawmakers and lawsuits are calling for restrictions on the use of face images without consent.</p><p><strong>We’re thinking:</strong> Andrew and his teams have worked on many face recognition systems over the years. Our practices have evolved — and continue to do so — as both society and AI practitioners have come to recognize the importance of privacy. As we gather data, we must also work toward fairer and more respectful standards governing its collection, documentation, and use.</p><p><strong>Fun fact:</strong> Andrew’s face appears (with permission!) in a Carnegie Mellon University face <a href=\"https://www.cs.cmu.edu/~tom/faces.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">dataset</a> collected by Tom Mitchell in 1996. Here’s <a href=\"https://twitter.com/i/status/1105594325344251904?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">what Andrew looked like</a> in those days. </p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1-1.gif\" class=\"kg-image\" alt=\"Different graphs showing switch transformer data \" loading=\"lazy\"></figure><h2 id=\"bigger-faster-transformers\">Bigger, Faster Transformers</h2><p>Performance in language tasks <a href=\"https://arxiv.org/abs/2001.0836?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">rises with the size of the model</a> — yet, as a model’s parameter count rises, so does the time it takes to render output. New work pumps up the number of parameters without slowing down the network.<br><br><strong>What’s new:</strong> William Fedus, Barret Zoph, and Noam Shazeer at Google Brain developed the <a href=\"https://arxiv.org/abs/2101.03961?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Switch Transformer</a>, a large-scale architecture (the authors built a version comprising 1.6 <em>trillion</em> parameters) that’s nearly as fast as a much smaller model.<br><br><strong>Key insight:</strong> The approach known as <a href=\"https://arxiv.org/abs/1701.06538?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">mixture-of-experts</a> uses only a subset of a model’s parameters per input example. Like mixture-of-experts, Switch Transformer chooses which of many layers would best process a given input.<br><br><strong>How it works:</strong> The authors trained Switch Transformer to predict words that had been removed at random from a <a href=\"https://arxiv.org/abs/1910.10683?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">large text dataset</a> scraped from the web. The dataset was preprocessed to remove offensive language, placeholder text, and other issues.</p><ul><li>A typical transformer extracts a representation from each input token, such as a word, and then uses self-attention to compare the representations before passing them to a fully connected layer. Switch Transformer replaces the fully connected layer with one of a number (determined by a hyperparameter) of fully connected layers.</li><li>A softmax layer calculates the probability that any particular fully connected layer is best for processing a given token. Then it uses the chosen layer in the usual manner.</li><li>The fully connected layers process tokens in parallel. The authors added a loss to encourage them to be equally active. On a hardware chip, a separate processor core handles each layer, so this loss encourages equal distribution of the load on each core.</li></ul><p><strong>Results:</strong> The authors compared Switch Transformer (7.4 billion parameters) to <a href=\"https://arxiv.org/abs/1910.10683?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">T5</a> (223 million parameters), a variant similar to the original transformer that was trained on the same dataset, using negative log perplexity, a measure of the model’s uncertainty (higher is better). The new model achieved -1.561 negative log perplexity compared to T5’s -1.731. Switch Transformer ran at two-thirds the speed of T5 — it executed 1,000 predictions per second compared to T5’s 1,600 — with 33 times the number of parameters. It beat a mixture-of-experts transformer, presumably of roughly the same size, on both counts.<br><br><strong>Why it matters:</strong> In deep learning, bigger is better — but so is a manageable computation budget.<br><br><strong>We’re thinking:</strong> Transformers come in an increasing variety of flavors. We hope this summary helps you remember which is switch. </p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/unnamed-2.gif\" class=\"kg-image\" alt=\"Person in wheelchair, person in side profile, person wearing a hoodie\" loading=\"lazy\"></figure><h2 id=\"human-disabilities-baffle-algorithms\">Human Disabilities Baffle Algorithms</h2><p>Facebook’s content moderation algorithms block many advertisements aimed at disabled people.</p><p><strong>What’s new:</strong> The social media platform’s automated systems regularly reject ads for clothing designed for people with physical disabilities. The algorithms have misread such messages as pornography or sales pitches for medical devices, <em><a href=\"https://www.nytimes.com/2021/02/11/style/disabled-fashion-facebook-discrimination.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">The New York Times</a></em> reported.</p><p><strong>How it works:</strong> Automated systems at Facebook and Instagram examines the images and words in ads that users try to place on the sites. They turn down ads they deem to violate their terms of service. The system tells would-be ad buyers when it rejects their messages, but not why, making it difficult for advertisers to bring rejected materials into compliance. Companies can appeal rejections, but appeals often are reviewed by another AI system, creating a frustrating loop.</p><ul><li>Facebook disallowed an ad for a sweatshirt from <a href=\"https://mighty-well.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Mighty Well</a> bearing the words “I am immunocompromised — please give me space.” The social network’s algorithm had flagged it as a medical product. Mighty Well successfully appealed the decision.</li><li>Facebook and Instagram rejected ads from <a href=\"https://slickchicksonline.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Slick Chicks</a>, which makes underwear that clasps on the side as a convenience for wheelchair users, saying the ads contained “adult content.” Slick Chicks’ founder appealed the decision in dozens of emails and launched an online petition before Facebook lifted the ban.</li><li>The social-networking giant routinely rejects ads from <a href=\"https://juniperunltd.com/blogs/style/yarrow?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Yarrow</a>, which sells pants specially fitted for people in wheelchairs. Facebook doesn’t allow ads for medical equipment, and apparently the algorithm concluded that the ads were for wheelchairs. Yarrow has successfully appealed the rejections, which takes an average of 10 days each.</li><li><a href=\"https://www.pattiandricky.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Patty + Ricky</a>, a marketplace that sells clothing for people with disabilities, has appealed Facebook’s rejection of 200 adaptive fashion products.</li></ul><p><strong>Behind the news:</strong> Other social media platforms have been tripped up by well-intentioned efforts to control harmful speech.</p><ul><li><a href=\"https://www.news18.com/news/buzz/youtube-ai-blocked-chess-channel-after-confusing-black-and-white-for-racist-slurs-3454316.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">YouTube</a> blocked a popular chess channel for promoting harmful and dangerous content. Apparently, its algorithm objected to words like “black,” “white,” “attack,” and “threat” in descriptions of chess matches.</li><li>In 2019, TikTok admitted to <a href=\"https://www.theverge.com/2019/12/2/20991843/tiktok-bytedance-platform-disabled-autism-lgbt-fat-user-algorithm-reach-limit?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">suppressing</a> videos made by users who were disabled, queer, or overweight, purportedly an effort to discourage bullying.</li></ul><p><strong>Why it matters:</strong> Millions of <a href=\"https://www.facebook.com/business/news/3-million-advertisers?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">small businesses</a> advertise on Facebook and Instagram, many of which serve niche communities. For such companies, being barred from promoting their wares on these platforms is a major blow.</p><p><strong>We’re thinking:</strong> Moderating content on platforms as big as Facebook would be impossible without AI. But these cases illustrate how far automated systems are from being able to handle the job by themselves. Humans in the loop are still required to mediate between online platforms and their users.</p><hr><h2 id=\"a-message-from-deeplearningai\">A MESSAGE FROM <a href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\">DEEPLEARNING.AI</a></h2><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/unnamed-2-1024x576.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Last day to register for “Optimizing BizOps with AI,” an Expert Panel presented in collaboration with FourthBrain. Technical leaders at Amazon, Samsung, and Uber will explain how they’re deploying AI to improve business efficiency. Join us on Feb. 25, 2021, at 4 p.m. Pacific Standard Time! <a href=\"https://aibusiness.eventbrite.com/?aff=batch&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">RSVP</a></p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/unnamed-3.gif\" class=\"kg-image\" alt=\"Model predicting ingredients in a recipe and woman cooking\" loading=\"lazy\"></figure><h2 id=\"cake-cookie-cakie\">Cake + Cookie = Cakie</h2><p>AI may help revolutionize the human diet – or dessert, at least.</p><p><strong>What’s new:</strong> Google applied AI engineer Dale Markowitz and developer advocate Sara Robinson <a href=\"https://cloud.google.com/blog/topics/developers-practitioners/baking-recipes-made-ai?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">trained</a> a model to predict whether a recipe is a bread, cake, or cookie. They brightened the recent holiday season by using it to develop novel hybrid recipes.</p><p><strong>How it works:</strong> The engineers conceived their project to demonstrate Google’s AutoML, a software suite for easy-bake machine learning.</p><ul><li>They compiled and labeled roughly 600 recipes for breads, cakes, or cookies and limited ingredient lists to 16 essentials, like eggs, flour, and milk.</li><li>They trained a model to classify recipes as bread, cake, or cookies with high accuracy.</li><li>For each recipe, AutoML’s explainability feature assigned the ingredients a percentage that described their importance to the classification. Butter, sugar, and yeast were most important overall.</li><li>The authors adjusted ingredient ratios until they developed a recipe that the model classified as equal parts cookie and cake, and another that was equal parts bread and cookie. They baked and tasted these creations: a cakie (“nasty”) and a breakie (“pretty good”).</li></ul><p><strong>Behind the news:</strong> Machine learning’s culinary education is coming along, though some of its creations are tastier than others.</p><ul><li>Sony AI’s <a href=\"https://venturebeat.com/2020/12/14/sony-ai-launches-the-gastronomy-flagship-project-to-apply-ai-to-cooking/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Flagship Gastronomy Project</a> recently launched an app to help professional chefs develop new recipes, food pairings, and menus.</li><li>In 2019, <a href=\"https://ai.facebook.com/blog/inverse-cooking/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Facebook</a> trained a vision model to recognize different types of food and generate recipes to produce them.</li><li>In 2016, IBM debuted <a href=\"https://www.newyorker.com/magazine/2016/11/28/cooking-with-chef-watson-ibms-artificial-intelligence-app?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Chef Watson</a>, an app trained partly on <em>Bon</em> <em>Appétit’s</em> recipe archive, which generates recipes based on ingredients specified by users.</li><li>Blogger Janelle Shane <a href=\"https://aiweirdness.com/post/190569291992/ai-recipes-are-bad-and-a-proposal-for-making-them?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">prompted</a> GPT-2 to generate recipes for dish names that were themselves generated by AI, producing gustatory horrors like Chocolate Chicken Chicken Cake.</li></ul><p><strong>Why it matters:</strong> Experimenting with new recipes isn’t just fun for home cooks. Commercial test kitchens are on the lookout for novel flavors, textures, and dishes. AI could help chefs invent smorgasbords of culinary delights.</p><p><strong>We’re thinking:</strong> These AI-powered recipes may seem half-baked, but suddenly we have a craving for Chocolate Chicken Chicken Cake.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://www.deeplearning.ai/wp-content/uploads/2021/02/unnamed-4.gif\" class=\"kg-image\" alt=\"System Oscar+ working\" loading=\"lazy\"></figure><h2 id=\"sharper-eyes-for-visionlanguage\">Sharper Eyes For Vision+Language</h2><p>Models that interpret the interplay of words and images tend to be trained on richer bodies of text than images. Recent research worked toward giving such models a more balanced knowledge of the two domains.<br><br><strong>What’s new:</strong> Pengchuan Zhang and Xiujun Li led a team at Microsoft and University of Washington raised the bar in several vision-and-language tasks. They call their system <a href=\"https://www.microsoft.com/en-us/research/publication/vinvl-making-visual-representations-matter-in-vision-language-models/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Oscar+</a>, building on earlier <a href=\"https://arxiv.org/abs/2004.06165?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">work </a>that used class names of objects in an image to improve matching of image and text representations.<br><br><strong>Key insight:</strong> Recent progress in vision-and-language models has come mostly by combining learned image and text representations more effectively rather than improving the representations themselves, the authors observed. Honing these representations through additional pretraining ought to boost their performance.<br><br><strong>How it works:</strong> The authors started with pretrained representations for images and text generated by separate models for vision (<a href=\"https://arxiv.org/abs/1611.05431?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">ResNeXt-152 C4</a>  pretrained on <a href=\"http://image-net.org/index?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">ImageNet-5k</a>) and language (pretrained <a href=\"https://arxiv.org/abs/1810.04805?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">BERT</a>). They honed the image representations by further pretraining the vision model on new data. Then they generated image-and-text representations as they pretrained Oscar+ as a whole. Finally, they fine-tuned the system on specific vision-and-language tasks.</p><ul><li>In the additional pretraining step, the authors pretrained the ResNeXt-152 C4 to detect 1,848 objects or attributes (such as labels describing colors or textures) in 2.49 million images in <a href=\"https://cocodataset.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">four </a><a href=\"https://github.com/openimages/dataset?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">object</a> <a href=\"https://www.objects365.org/overview.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">detection</a> <a href=\"http://visualgenome.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">datasets</a>.</li><li>A transformer fused image and text representations as the authors pretrained Oscar+ on 8.85 million examples from <a href=\"https://www.kaggle.com/hsankesara/flickr-image-dataset?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">four</a> <a href=\"https://cocodataset.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">image</a> <a href=\"https://ai.google.com/research/ConceptualCaptions/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">caption</a> <a href=\"https://www.cs.virginia.edu/~vicente/sbucaptions/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">datasets</a> with generated image tags, <a href=\"https://github.com/openimages/dataset?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">image tag datasets</a> with generated captions, and <a href=\"https://visualqa.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">visual question-and-answer</a> <a href=\"https://cs.stanford.edu/people/dorarad/gqa/about.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">datasets</a>. At this stage, the system optimized two loss terms. One term encouraged the system to predict randomly hidden words in a caption or an image’s tags. The other term encouraged the system to match an image and its tags, or an answer with its question and its image.</li><li>They fine-tuned the system to perform seven specific tasks.</li></ul><p><strong>Results:</strong> Oscar+ achieved state-of-the-art results in all seven tasks, from <a href=\"https://cocodataset.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">matching images with captions</a> (and vice-versa) to <a href=\"http://lil.nlp.cornell.edu/nlvr/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">determining the truth of a statement about two images</a>. The system boosted <a href=\"https://nocaps.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">NoCaps</a> accuracy (captioning images that contain objects not seen in training) to 92.5 percent from 86.6 percent — its biggest gain. To show that performance was substantially improved by separately pretraining the object detector on additional data, the authors compared performance with and without that step. That step boosted visual question-answering accuracy, for instance, to 74.90 percent from 71.34 percent.<br><br><strong>Why it matters:</strong> Performance in multimodal tasks can improve with additional learning in just one of the modes involved.<br><br><strong>We’re thinking:</strong> If <a href=\"https://www.youtube.com/watch?v=7JRoo4o9fF8&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=2&_hsenc=p2ANqtz--XRa7vIW8UYuvGD4sU9D8-a0ryBxFZA2N0M4bzWpMf8nD_LeeUPpkCl_TMXUSpylC7TuAKoSbzJOmNyBwPoTtYsNQRJQ\" rel=\"noreferrer noopener\">Oscar is a grouch</a>, is Oscar+ nicer — or even more grumpy?</p>","comment_id":"60bfc9a1274d5b003b1060c3","feature_image":"https://dl-staging-website.ghost.io/content/images/2021/06/unnamed-1-1.png","featured":false,"visibility":"public","created_at":"2021-06-08T12:48:49.000-07:00","updated_at":"2022-10-06T09:37:13.000-07:00","published_at":"2021-02-24T12:00:00.000-08:00","custom_excerpt":"AI-enabled automation is often portrayed as a binary on-or-off: A process is either automated or not. But in practice, automation is a spectrum, and AI teams have to choose where on this spectrum to operate. It’s important to...","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"60bfddad274d5b003b1062be","name":"issue-80","slug":"issue-80","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-80/"},{"id":"60cc1d2a70a082003e13dd0e","name":"Feb 24, 2021","slug":"feb-24-2021","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/feb-24-2021/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-80/","excerpt":"AI-enabled automation is often portrayed as a binary on-or-off: A process is either automated or not. But in practice, automation is a spectrum, and AI teams have to choose where on this spectrum to operate. It’s important to...","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Face Datasets Under Fire, Baking With AI, Human Disabilities","meta_description":"The Batch - AI News & Insights: Cutting Corners to Recognize Faces | AI may help revolutionize the human diet – or dessert, at least | Human Disabilities Baffle Algorithms","email_subject":null,"frontmatter":null,"feature_image_alt":"Process of diagnosing a medical patient slide","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/06/unnamed-1-1.png","dimensions":{"width":576,"height":324}},"banner":{"title":"Data Engineering","databaseId":35522,"id":"cG9zdDozNTUyMg==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/10/DE-Vertical-2.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/4eQA3NM","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}