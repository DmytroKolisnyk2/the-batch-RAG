{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.120","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-242","id":"66044d328f53d7000183f9c1","uuid":"f02290cc-7eda-494e-b3ef-b5dd40148964","title":"One Agent For Many Worlds, Cross-Species Cell Embeddings, U.S. Deploys AI Targeting, Robo Football Gets Real","html":"<p>Dear friends,</p><p>Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality output. In this letter, I'd like to discuss Reflection. For a design pattern that’s relatively quick to implement, I've seen it lead to surprising performance gains.&nbsp;<br><br>You may have had the experience of prompting ChatGPT/Claude/Gemini, receiving unsatisfactory output, delivering critical feedback to help the LLM improve its response, and then getting a better response. What if you automate the step of delivering critical feedback, so the model automatically criticizes its own output and improves its response? This is the crux of Reflection.&nbsp;<br><br>Take the task of asking an LLM to write code. We can prompt it to generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:</p><p><em>Here’s code intended for task X:&nbsp;[previously generated code]&nbsp; &nbsp;&nbsp;<br>Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.</em><br><br>Sometimes this causes the LLM to spot problems and come up with constructive suggestions. Next, we can prompt the LLM with context including (i) the previously generated code and the constructive feedback and (ii) ask it to use the feedback to rewrite the code. This can lead to a better response. Repeating the criticism/rewrite process might yield further improvements. This self-reflection process allows the LLM to spot gaps and improve its output on a variety of tasks including producing code, writing text, and answering questions.&nbsp;</p><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/03/AGENTS-REFLECTION-1.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/03/AGENTS-REFLECTION-1.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/03/AGENTS-REFLECTION-1.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/03/AGENTS-REFLECTION-1.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p>And we can go beyond self-reflection by giving the LLM tools that help evaluate its output; for example, running its code through a few unit tests to check whether it generates correct results on test cases or searching the web to double-check text output. Then it can reflect on any errors it found and come up with ideas for improvement.</p><p>Further, we can implement Reflection using a multi-agent framework. I've found it convenient to create two different agents, one prompted to generate good outputs and the other prompted to give constructive criticism of the first agent's output. The resulting discussion between the two agents leads to improved responses.</p><p>Reflection is a relatively basic type of agentic workflow, but I've been delighted by how much it improved my applications’ results in a few cases. I hope you will try it in your own work. If you’re interested in learning more about reflection, I recommend these papers:</p><ul><li>“<a href=\"https://arxiv.org/abs/2303.17651?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Self-Refine: Iterative Refinement with Self-Feedback</a>,” Madaan et al. (2023)</li><li>“<a href=\"https://arxiv.org/abs/2303.11366?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Reflexion: Language Agents with Verbal Reinforcement Learning</a>,” Shinn et al. (2023)</li><li>“<a href=\"https://arxiv.org/abs/2305.11738?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</a>,” Gou et al. (2024)</li></ul><p>I’ll discuss the other agentic design patterns in future letters.</p><p>Keep learning!</p><p>Andrew&nbsp;</p><p>P.S. New JavaScript short course! Learn to build full-stack web applications that use RAG in “JavaScript RAG Web Apps with LlamaIndex,” taught by Laurie Voss, VP of Developer Relations at LlamaIndex and co-founder of npm.&nbsp;</p><ul><li>Build a RAG application for querying your own data.</li><li>Develop tools that interact with multiple data sources and use an agent to autonomously select the right tool for a given query.</li><li>Create a full-stack web app step by step that lets you chat with your data.&nbsp;</li><li>Dig further into production-ready techniques like how to persist your data, so you don’t need to reindex constantly.</li></ul><p><a href=\"https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Sign up here</a>!</p><p></p><h1 id=\"news\">News</h1><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/03/SIMA.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/03/SIMA.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/03/SIMA.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/03/SIMA.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"one-agent-many-environments\">One Agent, Many Environments</h1><p>AI agents are typically designed to operate a particular software environment. Recent work enabled a single agent to take actions in a variety of three-dimensional virtual worlds.</p><p><strong>What's new:</strong>&nbsp;A team of 90 people at Google and University of British Columbia announced&nbsp;<a href=\"https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Scalable Instructable Multiworld Agent</a>&nbsp;(SIMA), a system that learned to follow text instructions (such as “make a pile of rocks to mark this spot” or “see if you can jump over this chasm”) in seven commercial video games and four research environments.&nbsp;</p><p><strong>How it works:</strong>&nbsp;SIMA’s architecture consists of several transformers and a vanilla neural network. The authors trained it to mimic human players using a dataset of gameplay broken into 10 second tasks, including onscreen images, text instructions, keyboard presses, and mouse motions. The video games included Goat Simulator 3 (a third-person game in which the player takes the form of a goat), No Man’s Sky (a first- or third-person game of exploration and survival in outer space), Hydroneer (a first-person game of mining and building), and others.&nbsp;&nbsp;</p><ul><li>Given a text instruction and a frame of onscreen imagery,&nbsp;<a href=\"https://arxiv.org/abs/2401.09865?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">SPARC</a>&nbsp;(a pair of transformers pretrained on text-image pairs to produce similar embeddings of similar text and images) produced text and image embeddings. Given recent frames,&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/googles-phenaki-generates-long-form-video-from-text/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Phenaki</a>&nbsp;(a transformer pretrained to predict future frames in a video) generated a video embedding.</li><li>Given the image, text, and video embeddings, a collection of transformers learned to produce a representation of the game. (The authors don’t fully describe this part of the architecture.)&nbsp;</li><li>Given the game representation, a vanilla neural network learned to produce the corresponding keyboard and mouse actions.</li></ul><p><strong>Results:</strong>&nbsp;Judges evaluated SIMA’s success or failure at completing nearly 1,500 instructions that spanned tasks in nine categories like action (“jump”), navigation (“go to your ship”), and gathering resources (“get raspberries”). In Goat Simulator 3, SIMA completed 40 percent of the tasks. In No Man’s Sky, the judges compared SIMA’s performance to that of the human players whose gameplay produced the training data. SIMA was successful 34 percent of the time, while the players were successful 60 percent of the time. Judges also compared SIMA to versions that were trained to be experts in a single game. SIMA was successful more than 1.5 times more often than the specialized agents.</p><p><strong>Behind the news:</strong>&nbsp;SIMA extends Google’s earlier successes building agents that rival or beat human players at individual games including&nbsp;<a href=\"https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Go</a>,&nbsp;<a href=\"https://deepmind.google/discover/blog/agent57-outperforming-the-human-atari-benchmark/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">classic Atari games</a>, and&nbsp;<a href=\"https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">StarCraft II</a>.&nbsp;</p><p><strong>Why it matters:</strong>&nbsp;Training agents to follow directions in various environments, seeing the same things humans would, is a step toward building instructable agents that can work in any situation. The authors point to potential applications in robotics, simulations, and gaming; wherever an agent might need to be guided through diverse challenges.&nbsp;</p><p><strong>We're thinking:</strong>&nbsp;This work shows that an agent trained on multiple games can perform better than an agent trained on just one, and that the richer the language inputs in a gameworld, the better the agent can perform. With only a handful of training environments under its belt, SIMA doesn’t demonstrate superhuman performance, but it gets the job done a surprising amount of the time!</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/03/CELLS.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1120\" height=\"630\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/03/CELLS.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/03/CELLS.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/03/CELLS.jpg 1120w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"cross-species-cell-embeddings\">Cross-Species Cell Embeddings</h1><p>Researchers used an AI system to identify animal cell types from gene sequences, including a cell type that conventional approaches had discovered only in the past year.&nbsp;</p><p><strong>What’s new:</strong>&nbsp;Biologists at Stanford trained a&nbsp;<a href=\"https://www.biorxiv.org/content/10.1101/2023.11.28.568918v1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">system</a>&nbsp;to produce embeddings that represent individual cells in an organism. This enabled them to find cell types that have common function in different animals; for instance, the Norn cell, a type of kidney cell that biologists had previously theorized but&nbsp;<a href=\"https://www.nature.com/articles/s41591-023-02314-7?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">discovered</a>&nbsp;only in 2023.</p><p><strong>How it works:</strong>&nbsp;Universal Cell Embedding (UCE) comprises two transformers that produce embeddings of genes and cells respectively, plus a classifier based on a vanilla neural network. The authors trained the classifier, given embeddings of a gene and cell, to classify whether or not the cell produces the protein coded by that gene. The training dataset included RNA sequences of 36.2 million cells from eight animal species (humans and mice accounted for 33.9 million) along with related protein structures.&nbsp;</p><ul><li>The authors represented each cell as a sequence of gene embeddings, laid out in the order in which they appear in the cell’s genome. Instead of including all of a cell’s genes, the authors sampled 1,024 genes known to encode proteins. A pretrained&nbsp;<a href=\"https://pubmed.ncbi.nlm.nih.gov/36927031/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">ESM-2</a>&nbsp;transformer computed each gene’s embedding based on the protein(s) — that is, amino acid sequence(s) — it produces.&nbsp;</li><li>The authors randomly masked 20 percent of the gene embeddings. Given the masked sequence, a vanilla transformer learned to compute an embedding of the cell.&nbsp;</li><li>For each gene in the cell, the authors concatenated its embedding with the cell embedding. Given the combined embeddings, the vanilla neural network learned to classify whether the genes encoded a protein.</li></ul><p><strong>Results:</strong>&nbsp;Cell embeddings produced by UCE enabled the authors to identify cell types in animal species that weren’t in the training set. For instance, the authors embedded a dataset of mouse cells and applied&nbsp;<a href=\"https://arxiv.org/abs/1802.03426?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">UMAP</a>&nbsp;clustering to differentiate the types. They labeled the clusters as specific cell types (including Norn cells, which biologists took more than a century to find) based on the presence of certain genes that distinguish one cell type from another. Using the labels, they trained a logistic classifier. They applied the classifier to their training dataset and found Norn cells, among other cell types, in species other than mice. They verified the findings by looking for genes that tend to show up only in Norn cells.</p><p><strong>Why it matters:</strong>&nbsp;UCE’s embeddings encode biologically meaningful information about individual cells, enabling a clustering algorithm to group them into recognized cell types. The fact that the recently discovered Norn cell was among those clusters suggests that UCE may yield further discoveries that accelerate development of new medicines, lab processes, and research methods. In fact, the model found Norn cells — which are known to occur in the kidney — in organs where they have not been seen before. If this result turns out to be valid, UCE will have made a discovery that has eluded biologists to date.</p><p><strong>We’re thinking:</strong>&nbsp;It’s a truism that a machine learning model is only as good as its data. That makes this work all the more impressive: Its training data included a handful of species, yet it generalized to others.</p><hr><h2 id=\"new-from-deeplearningai\">NEW FROM&nbsp;DEEPLEARNING.AI</h2><figure class=\"kg-card kg-image-card\"><a href=\"https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/?ref=dl-staging-website.ghost.io\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-25T154251.995.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1125\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-25T154251.995.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-25T154251.995.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-25T154251.995.png 1600w, https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-25T154251.995.png 2100w\" sizes=\"(min-width: 720px) 720px\"></a></figure><p>Join our short course on “JavaScript RAG Web Apps with LlamaIndex” to learn how to build full-stack JavaScript web applications that let you chat with your data. Harness the capabilities of large language models and retrieval augmented generation (RAG)!&nbsp;<a href=\"https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\">Enroll for free</a></p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/03/MAVEN.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/03/MAVEN.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/03/MAVEN.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/03/MAVEN.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><h1 id=\"us-deploys-ai-assisted-targeting\">U.S. Deploys AI-Assisted Targeting</h1><p>The United States military is using computer vision to target enemy positions in the Red Sea and elsewhere.<br><br><strong>What’s new:</strong>&nbsp;Maven, a system that analyzes satellite and geolocation data, has been used to identify targets in real-world conflicts,&nbsp;<em>Bloomberg</em>&nbsp;<a href=\"https://www.bloomberg.com/features/2024-ai-warfare-project-maven/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">reported</a>. The system was developed primarily by&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/a-military-chatbot-can-create-battle-plans/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Palantir</a>&nbsp;and integrates technology from Amazon, Microsoft, information technology firms ECS Federal and L3Harris, aerospace firms Maxar and Sierra Nevada, and other unnamed companies.<br><br><strong>How it works:</strong>&nbsp;The 18th Airborne Corps, a U.S. Army unit organized for rapid deployment around the world, used Maven in live-fire training exercises. The system helped locate surface vessels in the Red Sea, rocket launchers in Yemen, and potential airstrike targets in Iraq and Syria. The U.S. used it to help Ukraine’s armed forces to locate Russian equipment, anonymous sources said.&nbsp;</p><ul><li>Maven melds various data streams into a top-down image of a geographic area. Satellites provide still images and video, and radar and infrared observations enable the system to see through clouds and other obstructions. It can also integrate non-visual information such as location data from mobile devices and social media posts.</li><li>Computer vision models identify military equipment such as aircraft and tanks, highlighting significant changes to object locations. They can register a buildup of equipment that may indicate a new, or newly active, military base.&nbsp;</li><li>The system displays a map that outlines potential targets in yellow and friendly forces, schools, hospitals, and other no-strike zones outlined in blue. Human decision-makers review output and authorize responses.</li></ul><p><strong>Behind the news:</strong>&nbsp;Google initially developed Maven for the U.S. Defense Department around 2017. Palantir&nbsp;<a href=\"https://www.businessinsider.com/palantir-took-over-from-google-on-project-maven-2019-12?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">inherited</a>&nbsp;the project after Google, facing protests by employees who did not want to contribute to government intelligence systems,&nbsp;<a href=\"https://www.nytimes.com/2018/06/01/technology/google-pentagon-project-maven.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">declined</a>&nbsp;to renew its contract in 2018. The U.S. military now has more than 800 active AI projects with a wide range of technology partners and contractors. Other countries are deploying similar technology:&nbsp;<a href=\"https://www.972mag.com/mass-assassination-factory-israel-calculated-bombing-gaza/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Israel</a>&nbsp;and&nbsp;<a href=\"https://www.deeplearning.ai/the-batch/ukraines-drone-industry-takes-flight-amidst-conflict/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Ukraine</a>&nbsp;have used AI-assisted targeting in their ongoing conflicts.</p><p><strong>Yes, but:</strong>&nbsp;Some U.S. military experts worry about Maven’s accuracy. In tests, Maven successfully identified objects about 60 percent of the time, while human analysts working with the 18th Airborne Corps did so 84 percent of time. Moreover, the system’s training data emphasizes deserts, and its success rate drops in other types of environments.<br><br><strong>Why it matters:</strong>&nbsp;Maven and similar systems offer some advantages over human analysts. They can observe and integrate multiple data streams simultaneously, and they can identify potential targets much more quickly. It’s likely that more data will make these systems more accurate. On the other hand, they represent a further step toward automated warfare in which automated assistance could come to displace human decision-making.&nbsp;</p><p><strong>We’re thinking:</strong>&nbsp;Automated targeting is increasingly used in military applications, and less-sophisticated systems have been in&nbsp;<a href=\"https://www.jstor.org/stable/pdf/resrep32146.4.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">use</a>&nbsp;for decades. However, humans should always be in control of decisions to fire. We support a global&nbsp;<a href=\"https://news.un.org/en/story/2023/10/1141922?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">ban</a>&nbsp;on fully autonomous weapons.</p><hr><figure class=\"kg-card kg-image-card\"><img src=\"https://dl-staging-website.ghost.io/content/images/2024/03/SOCCER-ezgif.com-optimize.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"600\" height=\"335\" srcset=\"https://dl-staging-website.ghost.io/content/images/2024/03/SOCCER-ezgif.com-optimize.gif 600w\"></figure><h1 id=\"robo-football-from-simulation-to-reality\">Robo-Football From Simulation to Reality</h1><p>Humanoid robots can play football (known as soccer in the United States) in the real world, thanks to reinforcement learning.</p><p><strong>What’s new:&nbsp;</strong>Tuomas Haarnoja and colleagues at Google and University of Oxford trained an&nbsp;<a href=\"https://arxiv.org/abs/2304.13653?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">agent</a>&nbsp;to play one-on-one football in a simulated environment. They applied the agent to 20-inch hardware robots on a scaled-down field. You can see it in action&nbsp;<a href=\"https://sites.google.com/view/op3-soccer?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">here</a>.</p><p><strong>Key insight:</strong>&nbsp;In reinforcement learning, an agent improves as it explores various motions. However, such exploration risks damaging expensive hardware. By training in a simulation, the agent can attempt a diversity of motions without risking a physical robot. Once the agent is trained, it can make the leap from simulation to reality.</p><p><strong>How it works:</strong>&nbsp;The agent learned in a&nbsp;<a href=\"https://ieeexplore.ieee.org/document/6386109?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">virtual world</a>&nbsp;to control the robot’s motion given (i) a simulated robot’s state (including the position, velocity, and acceleration of each of 20 joints), (ii) the current game state (including the location and velocity of the ball and opponent), (iii) the game state at each of the last five time steps, and (iv) the agent’s five previous actions. Training proceeded via reinforcement learning in two stages.&nbsp;</p><ul><li>During the first stage of training, the authors trained two teachers, both of which were vanilla neural networks. (i) The first teacher learned to predict movements that help a simulated robot score goals against an untrained opponent that immediately fell over. The teacher earned rewards for scoring and was penalized for falling over or letting the opponent score, among other rewards and penalties. (ii) The second teacher learned to make a fallen simulated robot stand up. It received larger rewards for smaller differences, and smaller rewards for larger differences, between the robot’s joint positions and the joint positions for key robot poses&nbsp;<a href=\"https://github.com/ROBOTIS-GIT/ROBOTIS-OP3?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">recorded</a>&nbsp;during a manually designed process of standing up.</li><li>The second stage of training involved another agent, also a vanilla neural network. This agent played a match against a previous version of itself in which each agent controlled a simulated robot. It received rewards for moving the robot’s joints in ways that helped it win the match or resembled the two teachers’ movements; this encouraged the agent to score goals and stand up after falling. To better approximate real-world conditions, the authors randomly perturbed the simulation, adding noise to the sensors that measured the robot’s actions and delaying parts of the simulation. They also restricted the joints’ range of motion to prevent the simulated robot from acting in ways that would damage a hardware robot.</li><li>At inference, the trained agent controlled an off-the-shelf&nbsp;<a href=\"https://emanual.robotis.com/docs/en/platform/op3/introduction/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">Robotis OP3</a>&nbsp;humanoid robot, which costs around $14,000.</li></ul><p><strong>Results:</strong>&nbsp;The agent learned not only to turn and kick but also to anticipate the ball’s motion and block an opponent’s shots. It scored penalties against a stationary goalie with 90 percent success in simulation and 70 percent success in the physical world. It stood up in 0.9 seconds on average, while a manually designed agent stood up in 2.5 seconds. Its maximum walking speed of 0.69 meters per second beat the manually designed agent’s 0.27 meters per second. However, its kicks propelled the ball at 2.0 meters per second on average, slower than the manually designed agent’s 2.1 meters per second.</p><p><strong>Why it matters:</strong>&nbsp;Controlling humanoid robots is challenging, as they’re less stable than&nbsp;<a href=\"https://arxiv.org/abs/2304.01159?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">quadrupeds</a>. Just getting them to do one type of motion, such as&nbsp;<a href=\"https://arxiv.org/abs/2302.09450?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B\" rel=\"noopener\">jumping</a>, can require dedicated research. This work drives humanoid robots in complex motions by combining established training methods: training in a noisy simulation, self-play, and using teacher agents to reward particular actions.</p><p><strong>We’re thinking:</strong>&nbsp;This work demonstrates that robots get a kick out of machine learning.</p><hr><h1 id=\"data-points\"><a href=\"https://www.deeplearning.ai/the-batch/tag/data-points/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\">Data Points</a></h1><p>The latest AI updates of the week include: </p><p>👉 Stability AI’s Stable Video 3D<br>👉 Sakana’s evolution-inspired model merging technique<br>👉 The new Blackwell B200 GPU by Nvidia </p><p>And much more. </p><p>Read <a href=\"https://www.deeplearning.ai/the-batch/data-points-issue-242/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\">Data Points</a>, your weekly AI news digest.</p>","comment_id":"66044d328f53d7000183f9c1","feature_image":"https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-27T124456.072.png","featured":false,"visibility":"public","created_at":"2024-03-27T09:45:38.000-07:00","updated_at":"2024-04-02T14:17:39.000-07:00","published_at":"2024-03-27T10:36:33.000-07:00","custom_excerpt":"The Batch AI News and Insights: Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year...","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"6604533a8f53d7000183fa0c","name":"issue-242","slug":"issue-242","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-242/"},{"id":"660452b58f53d7000183f9f9","name":"Mar 27, 2024","slug":"mar-27-2024","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/mar-27-2024/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-242/","excerpt":"The Batch AI News and Insights: Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year...","reading_time":12,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"One Agent For Many Worlds, Cross-Species Cell Embeddings, and more","meta_description":"The Batch AI News and Insights: Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress...","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2024/03/The-Batch-ads-and-exclusive-banners---2024-03-27T124456.072.png","dimensions":{"width":1680,"height":945}},"banner":{"title":"Generative AI for Software Development","databaseId":35156,"id":"cG9zdDozNTE1Ng==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/08/Vertical-side-banner-ads-5.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3YTVMir","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"Hugging Face C5","date":"2025-04-23T07:45:01","databaseId":36454,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4lAbjwN","courseName":"Building Code Agents with Hugging Face smolagents","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(77, 59, 189, 1) 0%, rgba(21, 94, 252, 1) 100%)","isOpenInNewTab":true}}]}}},"__N_SSG":true}